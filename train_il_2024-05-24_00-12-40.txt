



~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Config:
network:
  emb_size: 64
  num_rounds: 1
  cons_nfeats: 5
  edge_nfeats: 1
  var_nfeats: 19
  aggregator: add
  activation: null
  num_heads: 1
  head_depth: 2
  head_aggreagtor: add
  linear_weight_init: normal
  linear_bias_init: zeros
  layernorm_weight_init: null
  layernorm_bias_init: null
  include_edge_features: true
  name: gnn
instances:
  co_class: set_covering
  co_class_kwargs:
    n_rows: 500
    n_cols: 1000
experiment:
  seed: 0
  device: cuda:0
  path_to_load_imitation_data: /home/liutf/datas/
  path_to_save: ../scratch/datasets/retro_branching
  branching: pure_strong_branch
  max_steps: null
  num_samples: 120000
  num_epochs: 1000
learner:
  imitation_target: expert_actions
  loss_function: cross_entropy
  lr: 0.0001
  epoch_log_frequency: 1
  checkpoint_frequency: 1
  name: supervised_learner

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
_____________________None
Initialised imitation agent.
Loading imitation data from /home/liutf/datas//pure_strong_branch/set_covering/max_steps_None/set_covering_n_rows_500_n_cols_1000/samples/samples_1/...
Initialised training and validation data loaders.
init over
Initialised learner with params defaultdict(<class 'list'>, {'agent_name': 'gnn', 'agent_device': 'cuda:0', 'lr': 0.0001, 'learner_name': 'supervised_learner'}). Will save to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/
Training imitation agent...
Training imitation2 agent...
train_begin
run_epoch
step: 50 loss_mean: 4.265729885101319
step: 100 loss_mean: 4.247364168167114
step: 150 loss_mean: 4.073068342208862
step: 200 loss_mean: 3.7623358249664305
step: 250 loss_mean: 3.5393276262283324
step: 300 loss_mean: 3.4982853507995606
step: 350 loss_mean: 3.457269353866577
step: 400 loss_mean: 3.4469344234466552
step: 450 loss_mean: 3.392528100013733
step: 500 loss_mean: 3.457559823989868
step: 550 loss_mean: 3.451129631996155
step: 600 loss_mean: 3.3734769868850707
step: 650 loss_mean: 3.4042251014709475
step: 700 loss_mean: 3.31717538356781
step: 750 loss_mean: 3.3938098669052126
step: 800 loss_mean: 3.376361050605774
step: 850 loss_mean: 3.3842356824874877
step: 900 loss_mean: 3.3820770883560183
step: 950 loss_mean: 3.371021237373352
step: 1000 loss_mean: 3.412274036407471
step: 1050 loss_mean: 3.3412860345840456
step: 1100 loss_mean: 3.432439126968384
step: 1150 loss_mean: 3.3912495231628417
step: 1200 loss_mean: 3.4066515302658082
step: 1250 loss_mean: 3.413596930503845
step: 1300 loss_mean: 3.3717513275146485
step: 1350 loss_mean: 3.3896604490280153
step: 1400 loss_mean: 3.4573737573623657
step: 1450 loss_mean: 3.346344714164734
step: 1500 loss_mean: 3.3784685230255125
step: 1550 loss_mean: 3.373337960243225
step: 1600 loss_mean: 3.3808040523529055
step: 1650 loss_mean: 3.4004565382003786
step: 1700 loss_mean: 3.3497723007202147
step: 1750 loss_mean: 3.409318389892578
step: 1800 loss_mean: 3.34657012462616
step: 1850 loss_mean: 3.3560725021362305
step: 1900 loss_mean: 3.334590697288513
step: 1950 loss_mean: 3.365555176734924
step: 2000 loss_mean: 3.385605854988098
step: 2050 loss_mean: 3.336088275909424
step: 2100 loss_mean: 3.369254455566406
step: 2150 loss_mean: 3.348736524581909
step: 2200 loss_mean: 3.3715345573425295
step: 2250 loss_mean: 3.3504131126403807
step: 2300 loss_mean: 3.341088333129883
step: 2350 loss_mean: 3.3487816047668457
step: 2400 loss_mean: 3.3174562311172484
step: 2450 loss_mean: 3.326588177680969
step: 2500 loss_mean: 3.283926181793213
step: 2550 loss_mean: 3.2424481964111327
step: 2600 loss_mean: 3.2560090160369874
step: 2650 loss_mean: 3.323487153053284
step: 2700 loss_mean: 3.2844759941101076
step: 2750 loss_mean: 3.2803281784057616
step: 2800 loss_mean: 3.2803000831604003
step: 2850 loss_mean: 3.278232340812683
step: 2900 loss_mean: 3.291967830657959
step: 2950 loss_mean: 3.2377361536026
step: 3000 loss_mean: 3.250014777183533
step: 3050 loss_mean: 3.278442397117615
step: 3100 loss_mean: 3.262310767173767
step: 50 loss_mean: 3.2249797439575194
step: 100 loss_mean: 3.351395773887634
step: 150 loss_mean: 3.1989578723907472
step: 200 loss_mean: 3.2705470371246337
step: 250 loss_mean: 3.2266840600967406
step: 300 loss_mean: 3.304988327026367
step: 350 loss_mean: 3.231219506263733
step: 400 loss_mean: 3.294302053451538
step: 450 loss_mean: 3.219352388381958
step: 500 loss_mean: 3.3136786031723022
step: 550 loss_mean: 3.2862813663482666
step: 600 loss_mean: 3.212603530883789
Epoch: 1 | Run time: 620.0 s | Train loss: 3.41 | Valid loss: 3.26
Saved checkpoint 1 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0076 s.
run_epoch
step: 50 loss_mean: 3.2364257621765136
step: 100 loss_mean: 3.227175946235657
step: 150 loss_mean: 3.236698279380798
step: 200 loss_mean: 3.2647659969329834
step: 250 loss_mean: 3.215072441101074
step: 300 loss_mean: 3.260410737991333
step: 350 loss_mean: 3.2058296251296996
step: 400 loss_mean: 3.2778718996047975
step: 450 loss_mean: 3.2709269237518313
step: 500 loss_mean: 3.2506345224380495
step: 550 loss_mean: 3.2357424783706663
step: 600 loss_mean: 3.228202781677246
step: 650 loss_mean: 3.232460284233093
step: 700 loss_mean: 3.1572360801696777
step: 750 loss_mean: 3.2182040643692016
step: 800 loss_mean: 3.1812454986572267
step: 850 loss_mean: 3.1318028497695924
step: 900 loss_mean: 3.206856713294983
step: 950 loss_mean: 3.258654823303223
step: 1000 loss_mean: 3.186913056373596
step: 1050 loss_mean: 3.26383638381958
step: 1100 loss_mean: 3.142655987739563
step: 1150 loss_mean: 3.157539477348328
step: 1200 loss_mean: 3.2056139945983886
step: 1250 loss_mean: 3.253623156547546
step: 1300 loss_mean: 3.1557453155517576
step: 1350 loss_mean: 3.1844556522369385
step: 1400 loss_mean: 3.1779367828369143
step: 1450 loss_mean: 3.248159284591675
step: 1500 loss_mean: 3.199838652610779
step: 1550 loss_mean: 3.187365326881409
step: 1600 loss_mean: 3.1807437419891356
step: 1650 loss_mean: 3.1974160194396974
step: 1700 loss_mean: 3.1777937841415405
step: 1750 loss_mean: 3.174321660995483
step: 1800 loss_mean: 3.15289635181427
step: 1850 loss_mean: 3.1605361032485964
step: 1900 loss_mean: 3.155066032409668
step: 1950 loss_mean: 3.226894726753235
step: 2000 loss_mean: 3.1677665948867797
step: 2050 loss_mean: 3.124229850769043
step: 2100 loss_mean: 3.1551686239242556
step: 2150 loss_mean: 3.163150053024292
step: 2200 loss_mean: 3.1348650360107424
step: 2250 loss_mean: 3.086021819114685
step: 2300 loss_mean: 3.1552676010131835
step: 2350 loss_mean: 3.1476752853393553
step: 2400 loss_mean: 3.171679425239563
step: 2450 loss_mean: 3.0971003723144532
step: 2500 loss_mean: 3.1503346967697143
step: 2550 loss_mean: 3.142460722923279
step: 2600 loss_mean: 3.1683558893203734
step: 2650 loss_mean: 3.145975890159607
step: 2700 loss_mean: 3.1139777851104737
step: 2750 loss_mean: 3.153006682395935
step: 2800 loss_mean: 3.0934662437438964
step: 2850 loss_mean: 3.1678145265579225
step: 2900 loss_mean: 3.137563180923462
step: 2950 loss_mean: 3.1035937786102297
step: 3000 loss_mean: 3.0821020793914795
step: 3050 loss_mean: 3.1252116107940675
step: 3100 loss_mean: 3.102954902648926
step: 50 loss_mean: 3.0802546882629396
step: 100 loss_mean: 3.180754098892212
step: 150 loss_mean: 3.030700511932373
step: 200 loss_mean: 3.1591547870635988
step: 250 loss_mean: 3.1247524356842042
step: 300 loss_mean: 3.1804520654678345
step: 350 loss_mean: 3.0718103885650634
step: 400 loss_mean: 3.1394705247879027
step: 450 loss_mean: 3.0459137201309203
step: 500 loss_mean: 3.1551660966873167
step: 550 loss_mean: 3.1337933349609375
step: 600 loss_mean: 3.0599278926849367
Epoch: 2 | Run time: 621.0 s | Train loss: 3.18 | Valid loss: 3.11
Saved checkpoint 2 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0080 s.
run_epoch
step: 50 loss_mean: 3.132033905982971
step: 100 loss_mean: 3.1381522607803345
step: 150 loss_mean: 3.058015751838684
step: 200 loss_mean: 3.1072201681137086
step: 250 loss_mean: 3.0621650791168213
step: 300 loss_mean: 3.07283166885376
step: 350 loss_mean: 3.1035042428970336
step: 400 loss_mean: 3.1407173442840577
step: 450 loss_mean: 3.0910615015029905
step: 500 loss_mean: 3.0691449737548826
step: 550 loss_mean: 3.1358789920806887
step: 600 loss_mean: 3.1047531366348267
step: 650 loss_mean: 3.0939199352264404
step: 700 loss_mean: 3.066498188972473
step: 750 loss_mean: 3.040675411224365
step: 800 loss_mean: 3.1212536478042603
step: 850 loss_mean: 3.056902861595154
step: 900 loss_mean: 3.0436111736297606
step: 950 loss_mean: 3.1237322664260865
step: 1000 loss_mean: 3.1194203805923464
step: 1050 loss_mean: 3.0385338258743286
step: 1100 loss_mean: 3.0107890796661376
step: 1150 loss_mean: 3.0553625631332397
step: 1200 loss_mean: 3.109169282913208
step: 1250 loss_mean: 3.061958227157593
step: 1300 loss_mean: 3.00020010471344
step: 1350 loss_mean: 3.060012602806091
step: 1400 loss_mean: 3.098795385360718
step: 1450 loss_mean: 3.0188315105438233
step: 1500 loss_mean: 3.070860114097595
step: 1550 loss_mean: 3.0696476888656616
step: 1600 loss_mean: 3.0713396501541137
step: 1650 loss_mean: 3.078770623207092
step: 1700 loss_mean: 3.0870466136932375
step: 1750 loss_mean: 3.055354232788086
step: 1800 loss_mean: 3.0914160871505736
step: 1850 loss_mean: 3.0770266914367674
step: 1900 loss_mean: 3.0211240243911743
step: 1950 loss_mean: 3.0217428731918337
step: 2000 loss_mean: 3.0934953355789183
step: 2050 loss_mean: 2.9754387950897216
step: 2100 loss_mean: 3.0077583360672
step: 2150 loss_mean: 3.0474574422836302
step: 2200 loss_mean: 2.9938740968704223
step: 2250 loss_mean: 3.02483416557312
step: 2300 loss_mean: 2.99114378452301
step: 2350 loss_mean: 3.010966877937317
step: 2400 loss_mean: 3.0569377946853638
step: 2450 loss_mean: 3.0306493997573853
step: 2500 loss_mean: 3.096052255630493
step: 2550 loss_mean: 3.015863046646118
step: 2600 loss_mean: 3.0393624448776246
step: 2650 loss_mean: 3.020439581871033
step: 2700 loss_mean: 3.006527419090271
step: 2750 loss_mean: 2.9879305028915404
step: 2800 loss_mean: 3.0055851793289183
step: 2850 loss_mean: 3.0392703771591187
step: 2900 loss_mean: 3.020160608291626
step: 2950 loss_mean: 3.054467363357544
step: 3000 loss_mean: 3.0569965696334838
step: 3050 loss_mean: 3.0490799283981325
step: 3100 loss_mean: 3.0437890481948853
step: 50 loss_mean: 3.045644931793213
step: 100 loss_mean: 3.1086579322814942
step: 150 loss_mean: 2.9983922624588013
step: 200 loss_mean: 3.1302280807495118
step: 250 loss_mean: 3.0767799758911134
step: 300 loss_mean: 3.126495862007141
step: 350 loss_mean: 3.0035089111328124
step: 400 loss_mean: 3.0588102912902833
step: 450 loss_mean: 3.006976799964905
step: 500 loss_mean: 3.079511227607727
step: 550 loss_mean: 3.062467794418335
step: 600 loss_mean: 3.016259722709656
Epoch: 3 | Run time: 619.0 s | Train loss: 3.06 | Valid loss: 3.06
Saved checkpoint 3 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0090 s.
run_epoch
step: 50 loss_mean: 3.012089948654175
step: 100 loss_mean: 3.034348611831665
step: 150 loss_mean: 3.042704463005066
step: 200 loss_mean: 3.03063533782959
step: 250 loss_mean: 3.067138361930847
step: 300 loss_mean: 3.0314551305770876
step: 350 loss_mean: 3.0257654285430906
step: 400 loss_mean: 3.002226414680481
step: 450 loss_mean: 3.0670330142974853
step: 500 loss_mean: 3.0037710952758787
step: 550 loss_mean: 3.0162503337860107
step: 600 loss_mean: 2.978425645828247
step: 650 loss_mean: 3.0002045106887816
step: 700 loss_mean: 3.053127198219299
step: 750 loss_mean: 3.0733329820632935
step: 800 loss_mean: 2.9973131656646728
step: 850 loss_mean: 3.011295838356018
step: 900 loss_mean: 3.0255314683914185
step: 950 loss_mean: 3.025782279968262
step: 1000 loss_mean: 3.046583151817322
step: 1050 loss_mean: 2.9380273628234863
step: 1100 loss_mean: 3.0015448570251464
step: 1150 loss_mean: 3.035252962112427
step: 1200 loss_mean: 2.9941544675827028
step: 1250 loss_mean: 3.0153628969192505
step: 1300 loss_mean: 3.0488818883895874
step: 1350 loss_mean: 3.0193907976150514
step: 1400 loss_mean: 3.0223401498794558
step: 1450 loss_mean: 2.9823405838012693
step: 1500 loss_mean: 3.013903946876526
step: 1550 loss_mean: 3.0248448562622072
step: 1600 loss_mean: 3.0420040607452394
step: 1650 loss_mean: 3.0162487411499024
step: 1700 loss_mean: 2.9867042446136476
step: 1750 loss_mean: 2.979725799560547
step: 1800 loss_mean: 3.016122522354126
step: 1850 loss_mean: 3.046162052154541
step: 1900 loss_mean: 2.9658508825302126
step: 1950 loss_mean: 3.0306036901474
step: 2000 loss_mean: 2.9956517601013184
step: 2050 loss_mean: 3.1246837902069093
step: 2100 loss_mean: 3.0486237049102782
step: 2150 loss_mean: 3.0202822875976563
step: 2200 loss_mean: 3.0074765729904174
step: 2250 loss_mean: 3.015877709388733
step: 2300 loss_mean: 3.030479474067688
step: 2350 loss_mean: 3.0425015592575075
step: 2400 loss_mean: 3.0407904243469237
step: 2450 loss_mean: 3.003414101600647
step: 2500 loss_mean: 3.0505355644226073
step: 2550 loss_mean: 3.0209970664978028
step: 2600 loss_mean: 3.0795985984802248
step: 2650 loss_mean: 2.9676637840270996
step: 2700 loss_mean: 3.036929225921631
step: 2750 loss_mean: 2.9846231269836427
step: 2800 loss_mean: 3.005232529640198
step: 2850 loss_mean: 3.021398811340332
step: 2900 loss_mean: 3.0201813173294068
step: 2950 loss_mean: 3.0107248306274412
step: 3000 loss_mean: 2.96898428440094
step: 3050 loss_mean: 3.0213636589050292
step: 3100 loss_mean: 2.952898983955383
step: 50 loss_mean: 3.008596453666687
step: 100 loss_mean: 3.0790850925445556
step: 150 loss_mean: 2.9865742444992067
step: 200 loss_mean: 3.0710619497299194
step: 250 loss_mean: 3.050274200439453
step: 300 loss_mean: 3.099658155441284
step: 350 loss_mean: 2.9931244611740113
step: 400 loss_mean: 3.038063669204712
step: 450 loss_mean: 2.9994625806808473
step: 500 loss_mean: 3.04282262802124
step: 550 loss_mean: 3.028369860649109
step: 600 loss_mean: 2.9888551902770994
Epoch: 4 | Run time: 621.0 s | Train loss: 3.02 | Valid loss: 3.03
Saved checkpoint 4 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0102 s.
run_epoch
step: 50 loss_mean: 3.0385812091827393
step: 100 loss_mean: 3.031937952041626
step: 150 loss_mean: 3.0984145832061767
step: 200 loss_mean: 2.9954287815093994
step: 250 loss_mean: 3.0274676942825316
step: 300 loss_mean: 2.967085418701172
step: 350 loss_mean: 2.9987102508544923
step: 400 loss_mean: 3.039286050796509
step: 450 loss_mean: 3.0049291133880613
step: 500 loss_mean: 3.0113885307312014
step: 550 loss_mean: 2.954493179321289
step: 600 loss_mean: 3.032740259170532
step: 650 loss_mean: 3.0126431941986085
step: 700 loss_mean: 2.967586340904236
step: 750 loss_mean: 3.0472281694412233
step: 800 loss_mean: 3.0166215610504152
step: 850 loss_mean: 3.019348645210266
step: 900 loss_mean: 3.0267079067230225
step: 950 loss_mean: 2.9749727773666383
step: 1000 loss_mean: 2.9524373197555542
step: 1050 loss_mean: 3.032203531265259
step: 1100 loss_mean: 3.038485150337219
step: 1150 loss_mean: 2.928648672103882
step: 1200 loss_mean: 3.006629285812378
step: 1250 loss_mean: 3.0191898202896117
step: 1300 loss_mean: 3.048211178779602
step: 1350 loss_mean: 2.9593048810958864
step: 1400 loss_mean: 2.972287168502808
step: 1450 loss_mean: 3.0053865766525267
step: 1500 loss_mean: 2.948801670074463
step: 1550 loss_mean: 3.006589913368225
step: 1600 loss_mean: 3.0204692935943602
step: 1650 loss_mean: 2.9878933811187744
step: 1700 loss_mean: 2.9888073444366454
step: 1750 loss_mean: 3.061908288002014
step: 1800 loss_mean: 3.045997543334961
step: 1850 loss_mean: 2.9801154804229735
step: 1900 loss_mean: 2.9463001251220704
step: 1950 loss_mean: 2.9814685201644897
step: 2000 loss_mean: 3.011003942489624
step: 2050 loss_mean: 2.9598794651031493
step: 2100 loss_mean: 3.0135174655914305
step: 2150 loss_mean: 2.9984191703796386
step: 2200 loss_mean: 2.9816690349578856
step: 2250 loss_mean: 3.020670132637024
step: 2300 loss_mean: 3.0476295137405396
step: 2350 loss_mean: 2.973903570175171
step: 2400 loss_mean: 2.9910385179519654
step: 2450 loss_mean: 3.0180719089508057
step: 2500 loss_mean: 2.982861204147339
step: 2550 loss_mean: 2.9954871940612793
step: 2600 loss_mean: 3.047017774581909
step: 2650 loss_mean: 2.9960775136947633
step: 2700 loss_mean: 3.010727925300598
step: 2750 loss_mean: 3.021751432418823
step: 2800 loss_mean: 3.0186553764343262
step: 2850 loss_mean: 2.9314471006393434
step: 2900 loss_mean: 3.003210644721985
step: 2950 loss_mean: 2.9698692178726196
step: 3000 loss_mean: 3.0053317403793334
step: 3050 loss_mean: 2.954867663383484
step: 3100 loss_mean: 3.023931612968445
step: 50 loss_mean: 2.9824953269958496
step: 100 loss_mean: 3.059679479598999
step: 150 loss_mean: 2.9708644723892212
step: 200 loss_mean: 3.0621395111083984
step: 250 loss_mean: 3.0184039783477785
step: 300 loss_mean: 3.0872286701202394
step: 350 loss_mean: 2.9484882164001465
step: 400 loss_mean: 3.006536293029785
step: 450 loss_mean: 2.9734288454055786
step: 500 loss_mean: 3.0392287731170655
step: 550 loss_mean: 3.018507537841797
step: 600 loss_mean: 2.957281999588013
Epoch: 5 | Run time: 623.0 s | Train loss: 3.0 | Valid loss: 3.01
Saved checkpoint 5 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0115 s.
run_epoch
step: 50 loss_mean: 3.0017129468917845
step: 100 loss_mean: 3.0059502124786377
step: 150 loss_mean: 2.9735812950134277
step: 200 loss_mean: 2.963046851158142
step: 250 loss_mean: 3.003904643058777
step: 300 loss_mean: 2.9958446264266967
step: 350 loss_mean: 3.012300386428833
step: 400 loss_mean: 3.025586986541748
step: 450 loss_mean: 3.011077914237976
step: 500 loss_mean: 3.001056489944458
step: 550 loss_mean: 3.0347551584243773
step: 600 loss_mean: 2.9784066677093506
step: 650 loss_mean: 2.9936730575561525
step: 700 loss_mean: 2.98830855846405
step: 750 loss_mean: 3.017709970474243
step: 800 loss_mean: 2.986875033378601
step: 850 loss_mean: 3.057400827407837
step: 900 loss_mean: 2.9855004262924196
step: 950 loss_mean: 3.0222962474823
step: 1000 loss_mean: 2.9602651023864746
step: 1050 loss_mean: 2.9756158351898194
step: 1100 loss_mean: 3.030173673629761
step: 1150 loss_mean: 2.9959155988693236
step: 1200 loss_mean: 3.0138654470443726
step: 1250 loss_mean: 2.9375912141799927
step: 1300 loss_mean: 3.021376166343689
step: 1350 loss_mean: 3.0196801376342775
step: 1400 loss_mean: 2.9780867767333983
step: 1450 loss_mean: 3.0416703748703005
step: 1500 loss_mean: 2.9962458992004395
step: 1550 loss_mean: 2.9953881311416626
step: 1600 loss_mean: 2.9851026153564453
step: 1650 loss_mean: 3.009054284095764
step: 1700 loss_mean: 2.952121863365173
step: 1750 loss_mean: 2.996639413833618
step: 1800 loss_mean: 3.012839045524597
step: 1850 loss_mean: 2.9975961589813234
step: 1900 loss_mean: 2.9564980268478394
step: 1950 loss_mean: 2.9961501502990724
step: 2000 loss_mean: 2.9619664049148557
step: 2050 loss_mean: 2.993762183189392
step: 2100 loss_mean: 2.978245053291321
step: 2150 loss_mean: 2.998818588256836
step: 2200 loss_mean: 2.9643851709365845
step: 2250 loss_mean: 2.9243976640701295
step: 2300 loss_mean: 3.0232145309448244
step: 2350 loss_mean: 2.983895492553711
step: 2400 loss_mean: 2.969983386993408
step: 2450 loss_mean: 2.9390950536727907
step: 2500 loss_mean: 2.9968559885025026
step: 2550 loss_mean: 3.002063012123108
step: 2600 loss_mean: 2.93539972782135
step: 2650 loss_mean: 2.976669292449951
step: 2700 loss_mean: 3.0166418075561525
step: 2750 loss_mean: 2.9372677755355836
step: 2800 loss_mean: 2.965094318389893
step: 2850 loss_mean: 3.027180857658386
step: 2900 loss_mean: 2.974504828453064
step: 2950 loss_mean: 2.967971863746643
step: 3000 loss_mean: 2.992378659248352
step: 3050 loss_mean: 3.0146137952804564
step: 3100 loss_mean: 2.9657703971862794
step: 50 loss_mean: 2.965960817337036
step: 100 loss_mean: 3.0385064268112183
step: 150 loss_mean: 2.9532415103912353
step: 200 loss_mean: 3.0560969352722167
step: 250 loss_mean: 3.0151364517211916
step: 300 loss_mean: 3.0752794408798216
step: 350 loss_mean: 2.946518325805664
step: 400 loss_mean: 2.9999479627609253
step: 450 loss_mean: 2.9563484621047973
step: 500 loss_mean: 3.026794686317444
step: 550 loss_mean: 3.0074882888793946
step: 600 loss_mean: 2.948777189254761
Epoch: 6 | Run time: 618.0 s | Train loss: 2.99 | Valid loss: 3.0
Saved checkpoint 6 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0127 s.
run_epoch
step: 50 loss_mean: 2.9956187629699706
step: 100 loss_mean: 3.008753924369812
step: 150 loss_mean: 2.9702076435089113
step: 200 loss_mean: 2.9875544834136964
step: 250 loss_mean: 2.956454563140869
step: 300 loss_mean: 2.94726731300354
step: 350 loss_mean: 2.8911739683151243
step: 400 loss_mean: 3.0011491107940675
step: 450 loss_mean: 2.996514329910278
step: 500 loss_mean: 2.9915271329879762
step: 550 loss_mean: 2.9108295249938965
step: 600 loss_mean: 2.9644181156158447
step: 650 loss_mean: 3.0065835237503054
step: 700 loss_mean: 2.9569163513183594
step: 750 loss_mean: 2.958413758277893
step: 800 loss_mean: 2.984583601951599
step: 850 loss_mean: 2.9669301080703736
step: 900 loss_mean: 2.993591513633728
step: 950 loss_mean: 3.0206276750564576
step: 1000 loss_mean: 3.0276073265075683
step: 1050 loss_mean: 2.9835448837280274
step: 1100 loss_mean: 2.961156268119812
step: 1150 loss_mean: 2.971796164512634
step: 1200 loss_mean: 2.959482355117798
step: 1250 loss_mean: 2.9835027837753296
step: 1300 loss_mean: 3.0042060232162475
step: 1350 loss_mean: 2.9785720109939575
step: 1400 loss_mean: 2.980973720550537
step: 1450 loss_mean: 2.9776287364959715
step: 1500 loss_mean: 2.9321539211273193
step: 1550 loss_mean: 2.9663845539093017
step: 1600 loss_mean: 3.006355948448181
step: 1650 loss_mean: 2.962954864501953
step: 1700 loss_mean: 2.9854443883895874
step: 1750 loss_mean: 2.971652054786682
step: 1800 loss_mean: 3.0002257299423216
step: 1850 loss_mean: 2.9284478998184205
step: 1900 loss_mean: 2.963791136741638
step: 1950 loss_mean: 2.9977702617645265
step: 2000 loss_mean: 2.9319971323013307
step: 2050 loss_mean: 3.0112084436416624
step: 2100 loss_mean: 2.947958664894104
step: 2150 loss_mean: 3.0011855125427247
step: 2200 loss_mean: 2.982699294090271
step: 2250 loss_mean: 2.9804939460754394
step: 2300 loss_mean: 3.0410680866241453
step: 2350 loss_mean: 2.9985276222229005
step: 2400 loss_mean: 2.998852405548096
step: 2450 loss_mean: 3.0282716226577757
step: 2500 loss_mean: 2.962452321052551
step: 2550 loss_mean: 2.99562460899353
step: 2600 loss_mean: 3.0534436225891115
step: 2650 loss_mean: 2.982892870903015
step: 2700 loss_mean: 2.9666820859909055
step: 2750 loss_mean: 2.9450506496429445
step: 2800 loss_mean: 3.018239459991455
step: 2850 loss_mean: 2.9769143009185792
step: 2900 loss_mean: 3.0453973960876466
step: 2950 loss_mean: 3.002504529953003
step: 3000 loss_mean: 2.970168261528015
step: 3050 loss_mean: 2.9365654802322387
step: 3100 loss_mean: 2.9876666259765625
step: 50 loss_mean: 2.9602480363845824
step: 100 loss_mean: 3.01267786026001
step: 150 loss_mean: 2.9234316682815553
step: 200 loss_mean: 3.044825067520142
step: 250 loss_mean: 2.9958077478408813
step: 300 loss_mean: 3.0564567470550537
step: 350 loss_mean: 2.922016830444336
step: 400 loss_mean: 2.9825116395950317
step: 450 loss_mean: 2.9437333583831786
step: 500 loss_mean: 3.0094714736938477
step: 550 loss_mean: 2.9877548217773438
step: 600 loss_mean: 2.922286024093628
Epoch: 7 | Run time: 619.0 s | Train loss: 2.98 | Valid loss: 2.98
Saved checkpoint 7 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0140 s.
run_epoch
step: 50 loss_mean: 2.926028995513916
step: 100 loss_mean: 2.9670421838760377
step: 150 loss_mean: 2.971604447364807
step: 200 loss_mean: 3.0145046710968018
step: 250 loss_mean: 2.9665156269073485
step: 300 loss_mean: 2.968713393211365
step: 350 loss_mean: 3.000085110664368
step: 400 loss_mean: 2.967341160774231
step: 450 loss_mean: 2.974562282562256
step: 500 loss_mean: 2.9522775506973264
step: 550 loss_mean: 2.966352033615112
step: 600 loss_mean: 2.9790639448165894
step: 650 loss_mean: 3.0751451015472413
step: 700 loss_mean: 3.0182589435577394
step: 750 loss_mean: 2.9789320707321165
step: 800 loss_mean: 2.92458655834198
step: 850 loss_mean: 2.95932635307312
step: 900 loss_mean: 2.928559603691101
step: 950 loss_mean: 3.0469401979446413
step: 1000 loss_mean: 2.97800142288208
step: 1050 loss_mean: 2.976034002304077
step: 1100 loss_mean: 3.0052412271499636
step: 1150 loss_mean: 2.9220031309127807
step: 1200 loss_mean: 2.9590333032608034
step: 1250 loss_mean: 3.013779392242432
step: 1300 loss_mean: 2.917833914756775
step: 1350 loss_mean: 2.931517033576965
step: 1400 loss_mean: 2.9749470138549805
step: 1450 loss_mean: 2.9791105794906616
step: 1500 loss_mean: 2.974123306274414
step: 1550 loss_mean: 2.969785885810852
step: 1600 loss_mean: 2.9953830671310424
step: 1650 loss_mean: 3.0090763711929323
step: 1700 loss_mean: 2.938878870010376
step: 1750 loss_mean: 3.026048002243042
step: 1800 loss_mean: 2.963830885887146
step: 1850 loss_mean: 2.998298168182373
step: 1900 loss_mean: 2.9804469203948973
step: 1950 loss_mean: 2.9870911836624146
step: 2000 loss_mean: 3.0208299446105955
step: 2050 loss_mean: 2.9423348903656006
step: 2100 loss_mean: 2.952896919250488
step: 2150 loss_mean: 2.952513241767883
step: 2200 loss_mean: 2.9446427392959595
step: 2250 loss_mean: 2.96146842956543
step: 2300 loss_mean: 3.0150733757019044
step: 2350 loss_mean: 2.9967648792266846
step: 2400 loss_mean: 2.996352481842041
step: 2450 loss_mean: 2.9588899707794187
step: 2500 loss_mean: 3.0146087884902952
step: 2550 loss_mean: 2.9676734828948974
step: 2600 loss_mean: 2.9155381441116335
step: 2650 loss_mean: 2.9570993852615355
step: 2700 loss_mean: 2.967867527008057
step: 2750 loss_mean: 2.967568802833557
step: 2800 loss_mean: 2.906234788894653
step: 2850 loss_mean: 2.976573100090027
step: 2900 loss_mean: 2.9713253736495973
step: 2950 loss_mean: 2.99941921710968
step: 3000 loss_mean: 2.980228476524353
step: 3050 loss_mean: 2.9401031494140626
step: 3100 loss_mean: 2.944056191444397
step: 50 loss_mean: 2.9793547439575194
step: 100 loss_mean: 3.0357875394821168
step: 150 loss_mean: 2.944797329902649
step: 200 loss_mean: 3.039005832672119
step: 250 loss_mean: 3.0262667465209963
step: 300 loss_mean: 3.0601754331588746
step: 350 loss_mean: 2.9396309804916383
step: 400 loss_mean: 3.017302327156067
step: 450 loss_mean: 2.9662610340118407
step: 500 loss_mean: 3.0230077934265136
step: 550 loss_mean: 2.9946499824523927
step: 600 loss_mean: 2.948093638420105
Epoch: 8 | Run time: 616.0 s | Train loss: 2.97 | Valid loss: 3.0
Saved checkpoint 8 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0152 s.
run_epoch
step: 50 loss_mean: 2.994450716972351
step: 100 loss_mean: 2.9751195096969605
step: 150 loss_mean: 3.000862603187561
step: 200 loss_mean: 2.9641438579559325
step: 250 loss_mean: 2.981251368522644
step: 300 loss_mean: 2.9923155021667482
step: 350 loss_mean: 2.9532265138626097
step: 400 loss_mean: 2.9289790201187134
step: 450 loss_mean: 2.9987607860565184
step: 500 loss_mean: 2.958669800758362
step: 550 loss_mean: 2.9473318004608156
step: 600 loss_mean: 2.9577222633361817
step: 650 loss_mean: 2.9359624719619752
step: 700 loss_mean: 2.967508063316345
step: 750 loss_mean: 3.02413999080658
step: 800 loss_mean: 2.9986420106887817
step: 850 loss_mean: 2.947258324623108
step: 900 loss_mean: 2.948509111404419
step: 950 loss_mean: 2.982082552909851
step: 1000 loss_mean: 3.0041043376922607
step: 1050 loss_mean: 2.9846393489837646
step: 1100 loss_mean: 2.9706481885910034
step: 1150 loss_mean: 3.0174208784103396
step: 1200 loss_mean: 2.9404087018966676
step: 1250 loss_mean: 2.923946108818054
step: 1300 loss_mean: 2.9614875078201295
step: 1350 loss_mean: 2.9478941440582274
step: 1400 loss_mean: 2.9288562536239624
step: 1450 loss_mean: 3.011214647293091
step: 1500 loss_mean: 2.9514326524734495
step: 1550 loss_mean: 2.9544866037368775
step: 1600 loss_mean: 2.9622765493392946
step: 1650 loss_mean: 2.9396778345108032
step: 1700 loss_mean: 2.9929829025268555
step: 1750 loss_mean: 2.971885013580322
step: 1800 loss_mean: 2.9576519298553468
step: 1850 loss_mean: 2.949200096130371
step: 1900 loss_mean: 2.9991344833374023
step: 1950 loss_mean: 2.9660038852691653
step: 2000 loss_mean: 2.938475179672241
step: 2050 loss_mean: 2.9735959911346437
step: 2100 loss_mean: 2.968909749984741
step: 2150 loss_mean: 2.909370741844177
step: 2200 loss_mean: 3.0123527240753174
step: 2250 loss_mean: 2.9843152236938475
step: 2300 loss_mean: 2.9648696613311767
step: 2350 loss_mean: 2.955064353942871
step: 2400 loss_mean: 2.945685906410217
step: 2450 loss_mean: 2.9618183088302614
step: 2500 loss_mean: 3.0034937334060667
step: 2550 loss_mean: 2.96130539894104
step: 2600 loss_mean: 2.988439955711365
step: 2650 loss_mean: 2.938249068260193
step: 2700 loss_mean: 2.9482874488830566
step: 2750 loss_mean: 2.9547730350494383
step: 2800 loss_mean: 2.99156192779541
step: 2850 loss_mean: 2.9535009241104127
step: 2900 loss_mean: 2.930893840789795
step: 2950 loss_mean: 2.9853386926651
step: 3000 loss_mean: 3.0177002000808715
step: 3050 loss_mean: 2.9865750598907472
step: 3100 loss_mean: 2.921448950767517
step: 50 loss_mean: 2.968384652137756
step: 100 loss_mean: 3.0297424030303954
step: 150 loss_mean: 2.9292030048370363
step: 200 loss_mean: 3.055273385047913
step: 250 loss_mean: 3.0197808361053466
step: 300 loss_mean: 3.0680944776535033
step: 350 loss_mean: 2.925523386001587
step: 400 loss_mean: 3.0191825008392335
step: 450 loss_mean: 2.961222310066223
step: 500 loss_mean: 3.0406214952468873
step: 550 loss_mean: 3.03056519985199
step: 600 loss_mean: 2.937718920707703
Epoch: 9 | Run time: 622.0 s | Train loss: 2.97 | Valid loss: 3.0
Saved checkpoint 9 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0166 s.
run_epoch
step: 50 loss_mean: 2.969982175827026
step: 100 loss_mean: 2.973400435447693
step: 150 loss_mean: 3.0177846908569337
step: 200 loss_mean: 2.9514849615097045
step: 250 loss_mean: 2.951074562072754
step: 300 loss_mean: 2.9235079669952393
step: 350 loss_mean: 2.9420764017105103
step: 400 loss_mean: 2.930798416137695
step: 450 loss_mean: 2.889608826637268
step: 500 loss_mean: 3.0281559562683107
step: 550 loss_mean: 3.013404231071472
step: 600 loss_mean: 3.016376118659973
step: 650 loss_mean: 2.9607715511322024
step: 700 loss_mean: 2.924651389122009
step: 750 loss_mean: 2.9660045099258423
step: 800 loss_mean: 3.012287244796753
step: 850 loss_mean: 2.949678587913513
step: 900 loss_mean: 2.9701942110061648
step: 950 loss_mean: 2.9992939233779907
step: 1000 loss_mean: 2.9655700302124024
step: 1050 loss_mean: 2.9577660846710203
step: 1100 loss_mean: 2.957394700050354
step: 1150 loss_mean: 2.9511139726638795
step: 1200 loss_mean: 2.985684161186218
step: 1250 loss_mean: 2.9531952953338623
step: 1300 loss_mean: 2.936439847946167
step: 1350 loss_mean: 2.976792435646057
step: 1400 loss_mean: 2.919036908149719
step: 1450 loss_mean: 2.9335696268081666
step: 1500 loss_mean: 2.9259545040130615
step: 1550 loss_mean: 2.9808598279953005
step: 1600 loss_mean: 2.935499505996704
step: 1650 loss_mean: 2.9681829690933226
step: 1700 loss_mean: 2.988185396194458
step: 1750 loss_mean: 2.9815126419067384
step: 1800 loss_mean: 2.918254590034485
step: 1850 loss_mean: 3.003285913467407
step: 1900 loss_mean: 2.941458373069763
step: 1950 loss_mean: 2.9569767570495604
step: 2000 loss_mean: 2.971736717224121
step: 2050 loss_mean: 2.939319734573364
step: 2100 loss_mean: 2.9268279504776
step: 2150 loss_mean: 3.0113374853134154
step: 2200 loss_mean: 2.9249990129470826
step: 2250 loss_mean: 2.9425371170043944
step: 2300 loss_mean: 2.949549617767334
step: 2350 loss_mean: 2.930704116821289
step: 2400 loss_mean: 2.9221039390563965
step: 2450 loss_mean: 2.9934861135482786
step: 2500 loss_mean: 2.98042085647583
step: 2550 loss_mean: 2.928464775085449
step: 2600 loss_mean: 2.971100997924805
step: 2650 loss_mean: 2.9665174102783203
step: 2700 loss_mean: 2.9651548862457275
step: 2750 loss_mean: 2.997368264198303
step: 2800 loss_mean: 2.946369833946228
step: 2850 loss_mean: 2.898369326591492
step: 2900 loss_mean: 2.971151237487793
step: 2950 loss_mean: 3.004017667770386
step: 3000 loss_mean: 2.8821951007843016
step: 3050 loss_mean: 2.972530279159546
step: 3100 loss_mean: 2.980692467689514
step: 50 loss_mean: 2.9525300788879396
step: 100 loss_mean: 3.0303378438949586
step: 150 loss_mean: 2.9351321506500243
step: 200 loss_mean: 3.0348889875411986
step: 250 loss_mean: 2.9991716623306273
step: 300 loss_mean: 3.050446796417236
step: 350 loss_mean: 2.919180850982666
step: 400 loss_mean: 3.0039333724975585
step: 450 loss_mean: 2.941069903373718
step: 500 loss_mean: 3.013388977050781
step: 550 loss_mean: 2.9953932046890257
step: 600 loss_mean: 2.948850769996643
Epoch: 10 | Run time: 622.0 s | Train loss: 2.96 | Valid loss: 2.99
Saved checkpoint 10 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0185 s.
run_epoch
step: 50 loss_mean: 2.9726631593704225
step: 100 loss_mean: 2.992889790534973
step: 150 loss_mean: 2.9200404262542725
step: 200 loss_mean: 2.9764082050323486
step: 250 loss_mean: 2.9510297632217406
step: 300 loss_mean: 2.9821231889724733
step: 350 loss_mean: 2.9385317087173464
step: 400 loss_mean: 2.9569306707382204
step: 450 loss_mean: 2.981138162612915
step: 500 loss_mean: 2.9613185501098633
step: 550 loss_mean: 2.9393243646621703
step: 600 loss_mean: 2.9770138311386107
step: 650 loss_mean: 2.928771147727966
step: 700 loss_mean: 2.9573946046829223
step: 750 loss_mean: 2.928614101409912
step: 800 loss_mean: 2.8544721889495848
step: 850 loss_mean: 2.923703875541687
step: 900 loss_mean: 2.958389706611633
step: 950 loss_mean: 2.955324063301086
step: 1000 loss_mean: 2.9678492593765258
step: 1050 loss_mean: 2.9532091283798216
step: 1100 loss_mean: 2.9692876958847045
step: 1150 loss_mean: 2.970272741317749
step: 1200 loss_mean: 2.9526491498947145
step: 1250 loss_mean: 2.9504998302459717
step: 1300 loss_mean: 2.9486939525604248
step: 1350 loss_mean: 2.933691167831421
step: 1400 loss_mean: 2.9773072814941406
step: 1450 loss_mean: 2.9339197158813475
step: 1500 loss_mean: 2.9272438144683837
step: 1550 loss_mean: 2.947646336555481
step: 1600 loss_mean: 2.922512664794922
step: 1650 loss_mean: 3.010769100189209
step: 1700 loss_mean: 2.934538555145264
step: 1750 loss_mean: 3.0170606422424315
step: 1800 loss_mean: 2.9697460889816285
step: 1850 loss_mean: 3.026201868057251
step: 1900 loss_mean: 2.94748685836792
step: 1950 loss_mean: 2.926618800163269
step: 2000 loss_mean: 2.941432614326477
step: 2050 loss_mean: 2.8928740072250365
step: 2100 loss_mean: 2.975351905822754
step: 2150 loss_mean: 2.973212628364563
step: 2200 loss_mean: 2.9538605070114134
step: 2250 loss_mean: 2.91899552822113
step: 2300 loss_mean: 2.9472432565689086
step: 2350 loss_mean: 2.96482647895813
step: 2400 loss_mean: 2.9237885093688964
step: 2450 loss_mean: 2.9546155309677125
step: 2500 loss_mean: 2.969165940284729
step: 2550 loss_mean: 2.9653605079650878
step: 2600 loss_mean: 2.930579118728638
step: 2650 loss_mean: 2.9177499198913575
step: 2700 loss_mean: 2.9511954832077025
step: 2750 loss_mean: 2.9582294034957886
step: 2800 loss_mean: 2.9561838245391847
step: 2850 loss_mean: 2.91595995426178
step: 2900 loss_mean: 2.9138252925872803
step: 2950 loss_mean: 2.96269540309906
step: 3000 loss_mean: 2.9626936435699465
step: 3050 loss_mean: 2.9944930267333985
step: 3100 loss_mean: 2.9604553174972534
step: 50 loss_mean: 2.964509868621826
step: 100 loss_mean: 2.9981186294555666
step: 150 loss_mean: 2.951204228401184
step: 200 loss_mean: 3.0545338010787964
step: 250 loss_mean: 3.007104501724243
step: 300 loss_mean: 3.033221750259399
step: 350 loss_mean: 2.909778218269348
step: 400 loss_mean: 3.0124112367630005
step: 450 loss_mean: 2.934525909423828
step: 500 loss_mean: 3.0048081350326536
step: 550 loss_mean: 2.967794213294983
step: 600 loss_mean: 2.9279805040359497
Epoch: 11 | Run time: 621.0 s | Train loss: 2.95 | Valid loss: 2.98
Saved checkpoint 11 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0195 s.
run_epoch
step: 50 loss_mean: 2.9580192136764527
step: 100 loss_mean: 3.030628023147583
step: 150 loss_mean: 2.9711665868759156
step: 200 loss_mean: 2.980193829536438
step: 250 loss_mean: 2.9408194065093993
step: 300 loss_mean: 2.9540772151947023
step: 350 loss_mean: 2.9431850862503053
step: 400 loss_mean: 2.957649335861206
step: 450 loss_mean: 2.942037749290466
step: 500 loss_mean: 2.9445599842071535
step: 550 loss_mean: 2.9330707693099978
step: 600 loss_mean: 2.980109896659851
step: 650 loss_mean: 2.9692804288864134
step: 700 loss_mean: 2.9731647300720216
step: 750 loss_mean: 2.868028745651245
step: 800 loss_mean: 2.9500943756103517
step: 850 loss_mean: 2.9337355518341064
step: 900 loss_mean: 2.988740797042847
step: 950 loss_mean: 2.8967523622512816
step: 1000 loss_mean: 2.961058611869812
step: 1050 loss_mean: 2.9562412309646606
step: 1100 loss_mean: 2.9176117515563966
step: 1150 loss_mean: 2.9397017431259154
step: 1200 loss_mean: 2.9365285205841065
step: 1250 loss_mean: 2.9346559715270994
step: 1300 loss_mean: 2.9482053995132445
step: 1350 loss_mean: 2.9165193319320677
step: 1400 loss_mean: 2.944822063446045
step: 1450 loss_mean: 3.020296640396118
step: 1500 loss_mean: 2.9338101053237917
step: 1550 loss_mean: 2.937933688163757
step: 1600 loss_mean: 2.9847291994094847
step: 1650 loss_mean: 2.92607843875885
step: 1700 loss_mean: 2.970760312080383
step: 1750 loss_mean: 2.938250560760498
step: 1800 loss_mean: 2.9769185161590577
step: 1850 loss_mean: 2.9479870319366457
step: 1900 loss_mean: 2.9441415452957154
step: 1950 loss_mean: 2.9503231954574587
step: 2000 loss_mean: 2.938700308799744
step: 2050 loss_mean: 2.899099931716919
step: 2100 loss_mean: 2.9079867935180665
step: 2150 loss_mean: 2.9674011421203614
step: 2200 loss_mean: 2.8849955224990844
step: 2250 loss_mean: 2.9682201242446897
step: 2300 loss_mean: 2.957980613708496
step: 2350 loss_mean: 2.904141149520874
step: 2400 loss_mean: 2.9184144926071167
step: 2450 loss_mean: 2.922038617134094
step: 2500 loss_mean: 2.888435273170471
step: 2550 loss_mean: 2.925918278694153
step: 2600 loss_mean: 2.991037063598633
step: 2650 loss_mean: 2.946289324760437
step: 2700 loss_mean: 2.9261977243423463
step: 2750 loss_mean: 2.9232029914855957
step: 2800 loss_mean: 2.8987008237838747
step: 2850 loss_mean: 2.917193832397461
step: 2900 loss_mean: 2.928839797973633
step: 2950 loss_mean: 2.9768805170059203
step: 3000 loss_mean: 2.9488481855392457
step: 3050 loss_mean: 2.9078267097473143
step: 3100 loss_mean: 2.898851456642151
step: 50 loss_mean: 2.959643769264221
step: 100 loss_mean: 2.9936835050582884
step: 150 loss_mean: 2.9389561891555784
step: 200 loss_mean: 3.0315259504318237
step: 250 loss_mean: 2.985110630989075
step: 300 loss_mean: 3.035917935371399
step: 350 loss_mean: 2.8871047258377076
step: 400 loss_mean: 2.9914833164215087
step: 450 loss_mean: 2.920211796760559
step: 500 loss_mean: 2.9869296741485596
step: 550 loss_mean: 2.9602466106414793
step: 600 loss_mean: 2.919877915382385
Epoch: 12 | Run time: 620.0 s | Train loss: 2.94 | Valid loss: 2.97
Saved checkpoint 12 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0210 s.
run_epoch
step: 50 loss_mean: 3.0115452241897582
step: 100 loss_mean: 2.9347057819366453
step: 150 loss_mean: 2.9670674180984498
step: 200 loss_mean: 2.9087855291366576
step: 250 loss_mean: 2.9958304882049562
step: 300 loss_mean: 2.948944802284241
step: 350 loss_mean: 2.932404613494873
step: 400 loss_mean: 2.953507413864136
step: 450 loss_mean: 2.9270348930358887
step: 500 loss_mean: 2.961221385002136
step: 550 loss_mean: 2.909378185272217
step: 600 loss_mean: 2.92811252117157
step: 650 loss_mean: 2.968489365577698
step: 700 loss_mean: 2.909074611663818
step: 750 loss_mean: 2.9241570091247557
step: 800 loss_mean: 2.908395919799805
step: 850 loss_mean: 2.9217049074172974
step: 900 loss_mean: 2.9509443044662476
step: 950 loss_mean: 2.9562542963027956
step: 1000 loss_mean: 2.902866687774658
step: 1050 loss_mean: 2.9225407648086548
step: 1100 loss_mean: 2.9587793064117434
step: 1150 loss_mean: 2.941340141296387
step: 1200 loss_mean: 2.908417706489563
step: 1250 loss_mean: 2.924963345527649
step: 1300 loss_mean: 2.9469639253616333
step: 1350 loss_mean: 2.9092512798309325
step: 1400 loss_mean: 2.9068935775756835
step: 1450 loss_mean: 2.9366419649124147
step: 1500 loss_mean: 2.951492438316345
step: 1550 loss_mean: 2.9839592456817625
step: 1600 loss_mean: 2.917022957801819
step: 1650 loss_mean: 2.8824489974975585
step: 1700 loss_mean: 2.9123073482513426
step: 1750 loss_mean: 2.9334918642044068
step: 1800 loss_mean: 2.9273785018920897
step: 1850 loss_mean: 2.9263450622558596
step: 1900 loss_mean: 2.931083126068115
step: 1950 loss_mean: 2.9813110780715943
step: 2000 loss_mean: 2.975174307823181
step: 2050 loss_mean: 2.932690644264221
step: 2100 loss_mean: 2.952977180480957
step: 2150 loss_mean: 2.95980251789093
step: 2200 loss_mean: 2.8936168146133423
step: 2250 loss_mean: 2.928915162086487
step: 2300 loss_mean: 2.935612049102783
step: 2350 loss_mean: 2.923520874977112
step: 2400 loss_mean: 2.9038482332229614
step: 2450 loss_mean: 2.912257318496704
step: 2500 loss_mean: 2.8980776929855345
step: 2550 loss_mean: 2.906319718360901
step: 2600 loss_mean: 2.944772572517395
step: 2650 loss_mean: 2.9365273189544676
step: 2700 loss_mean: 2.9082218742370607
step: 2750 loss_mean: 2.9376406908035277
step: 2800 loss_mean: 2.852101345062256
step: 2850 loss_mean: 2.888310604095459
step: 2900 loss_mean: 2.921512289047241
step: 2950 loss_mean: 2.8939337301254273
step: 3000 loss_mean: 2.918293628692627
step: 3050 loss_mean: 2.891320834159851
step: 3100 loss_mean: 2.9502846336364748
step: 50 loss_mean: 2.922513418197632
step: 100 loss_mean: 2.9611778593063356
step: 150 loss_mean: 2.883638277053833
step: 200 loss_mean: 2.998388924598694
step: 250 loss_mean: 2.949469385147095
step: 300 loss_mean: 3.015372486114502
step: 350 loss_mean: 2.8519947814941404
step: 400 loss_mean: 2.9516862344741823
step: 450 loss_mean: 2.8940747737884522
step: 500 loss_mean: 2.9613571834564207
step: 550 loss_mean: 2.9486586618423463
step: 600 loss_mean: 2.872174129486084
Epoch: 13 | Run time: 624.0 s | Train loss: 2.93 | Valid loss: 2.94
Saved checkpoint 13 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0222 s.
run_epoch
step: 50 loss_mean: 2.8937940645217894
step: 100 loss_mean: 2.8842595195770264
step: 150 loss_mean: 2.8890244436264036
step: 200 loss_mean: 2.9527601051330565
step: 250 loss_mean: 2.881615171432495
step: 300 loss_mean: 2.946473774909973
step: 350 loss_mean: 2.9169373655319215
step: 400 loss_mean: 2.938742594718933
step: 450 loss_mean: 2.9437866020202637
step: 500 loss_mean: 2.941681971549988
step: 550 loss_mean: 2.9225726652145387
step: 600 loss_mean: 2.941999230384827
step: 650 loss_mean: 2.940817108154297
step: 700 loss_mean: 2.9010890769958495
step: 750 loss_mean: 2.9099087524414062
step: 800 loss_mean: 2.867589135169983
step: 850 loss_mean: 2.9204040718078614
step: 900 loss_mean: 2.901123595237732
step: 950 loss_mean: 2.897177228927612
step: 1000 loss_mean: 2.933060674667358
step: 1050 loss_mean: 2.9298359727859498
step: 1100 loss_mean: 2.897981948852539
step: 1150 loss_mean: 2.913220276832581
step: 1200 loss_mean: 2.8923911809921266
step: 1250 loss_mean: 2.963100938796997
step: 1300 loss_mean: 2.8990035915374754
step: 1350 loss_mean: 2.8934174251556395
step: 1400 loss_mean: 2.902590584754944
step: 1450 loss_mean: 2.954026947021484
step: 1500 loss_mean: 2.9712778759002685
step: 1550 loss_mean: 2.910953550338745
step: 1600 loss_mean: 2.9411649751663207
step: 1650 loss_mean: 2.874043469429016
step: 1700 loss_mean: 2.9506007766723634
step: 1750 loss_mean: 2.9209272956848142
step: 1800 loss_mean: 2.9558308506011963
step: 1850 loss_mean: 2.9392133855819704
step: 1900 loss_mean: 2.954505376815796
step: 1950 loss_mean: 2.9517565107345582
step: 2000 loss_mean: 2.9535819578170774
step: 2050 loss_mean: 2.898641290664673
step: 2100 loss_mean: 2.9552927207946778
step: 2150 loss_mean: 2.934454016685486
step: 2200 loss_mean: 2.8909741544723513
step: 2250 loss_mean: 2.8942498350143433
step: 2300 loss_mean: 2.8811080694198608
step: 2350 loss_mean: 2.889509677886963
step: 2400 loss_mean: 2.8962072944641113
step: 2450 loss_mean: 2.861608362197876
step: 2500 loss_mean: 2.8939462995529173
step: 2550 loss_mean: 2.9076014614105223
step: 2600 loss_mean: 2.9148159885406493
step: 2650 loss_mean: 2.868706111907959
step: 2700 loss_mean: 2.904157238006592
step: 2750 loss_mean: 2.9315108346939085
step: 2800 loss_mean: 2.8741785144805907
step: 2850 loss_mean: 2.893425850868225
step: 2900 loss_mean: 2.935475482940674
step: 2950 loss_mean: 2.9281397438049317
step: 3000 loss_mean: 2.939863338470459
step: 3050 loss_mean: 2.951875376701355
step: 3100 loss_mean: 2.9127525424957277
step: 50 loss_mean: 2.9052670621871948
step: 100 loss_mean: 2.9523646879196166
step: 150 loss_mean: 2.879698076248169
step: 200 loss_mean: 2.993142123222351
step: 250 loss_mean: 2.959764151573181
step: 300 loss_mean: 2.9977172946929933
step: 350 loss_mean: 2.8555353927612304
step: 400 loss_mean: 2.944936265945435
step: 450 loss_mean: 2.881515135765076
step: 500 loss_mean: 2.9482800197601318
step: 550 loss_mean: 2.9328241109848023
step: 600 loss_mean: 2.865252733230591
Epoch: 14 | Run time: 622.0 s | Train loss: 2.92 | Valid loss: 2.93
Saved checkpoint 14 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0240 s.
run_epoch
step: 50 loss_mean: 2.8814762830734253
step: 100 loss_mean: 2.918393530845642
step: 150 loss_mean: 2.880918869972229
step: 200 loss_mean: 2.8933295583724976
step: 250 loss_mean: 2.8693657064437867
step: 300 loss_mean: 2.9018181800842284
step: 350 loss_mean: 2.9133777141571047
step: 400 loss_mean: 2.9749577045440674
step: 450 loss_mean: 2.9144917488098145
step: 500 loss_mean: 2.908063974380493
step: 550 loss_mean: 2.8917908239364625
step: 600 loss_mean: 2.915757622718811
step: 650 loss_mean: 2.8947648096084593
step: 700 loss_mean: 2.854899444580078
step: 750 loss_mean: 2.906970248222351
step: 800 loss_mean: 2.9234815073013305
step: 850 loss_mean: 2.8834644556045532
step: 900 loss_mean: 2.895664191246033
step: 950 loss_mean: 2.8903121948242188
step: 1000 loss_mean: 2.926343812942505
step: 1050 loss_mean: 2.877731556892395
step: 1100 loss_mean: 2.9161269903182983
step: 1150 loss_mean: 2.851864137649536
step: 1200 loss_mean: 2.936453685760498
step: 1250 loss_mean: 2.8495380878448486
step: 1300 loss_mean: 2.908694653511047
step: 1350 loss_mean: 2.942990083694458
step: 1400 loss_mean: 2.882105097770691
step: 1450 loss_mean: 2.9583526992797853
step: 1500 loss_mean: 2.8713242053985595
step: 1550 loss_mean: 2.8375733852386475
step: 1600 loss_mean: 2.8855699634552003
step: 1650 loss_mean: 2.9093399953842165
step: 1700 loss_mean: 2.919979410171509
step: 1750 loss_mean: 2.9165939140319823
step: 1800 loss_mean: 2.871517400741577
step: 1850 loss_mean: 2.8266402006149294
step: 1900 loss_mean: 2.9413929653167723
step: 1950 loss_mean: 2.939036936759949
step: 2000 loss_mean: 2.8943617725372315
step: 2050 loss_mean: 2.925523977279663
step: 2100 loss_mean: 2.902176995277405
step: 2150 loss_mean: 2.9595168733596804
step: 2200 loss_mean: 2.9632008600234987
step: 2250 loss_mean: 2.8324616813659667
step: 2300 loss_mean: 2.919974765777588
step: 2350 loss_mean: 2.891655535697937
step: 2400 loss_mean: 2.938327841758728
step: 2450 loss_mean: 2.857750291824341
step: 2500 loss_mean: 2.881857590675354
step: 2550 loss_mean: 2.906751708984375
step: 2600 loss_mean: 2.9173638010025025
step: 2650 loss_mean: 2.8884583806991575
step: 2700 loss_mean: 2.865714921951294
step: 2750 loss_mean: 2.8863238286972046
step: 2800 loss_mean: 2.955626268386841
step: 2850 loss_mean: 2.8761544227600098
step: 2900 loss_mean: 2.9755065965652467
step: 2950 loss_mean: 2.914874029159546
step: 3000 loss_mean: 2.8949859142303467
step: 3050 loss_mean: 2.9033202075958253
step: 3100 loss_mean: 2.9007622385025025
step: 50 loss_mean: 2.902611632347107
step: 100 loss_mean: 2.9733529615402223
step: 150 loss_mean: 2.8670257711410523
step: 200 loss_mean: 2.9816768407821654
step: 250 loss_mean: 2.9388353395462037
step: 300 loss_mean: 2.9831971073150636
step: 350 loss_mean: 2.842088243961334
step: 400 loss_mean: 2.9244270181655883
step: 450 loss_mean: 2.888601837158203
step: 500 loss_mean: 2.9281593132019044
step: 550 loss_mean: 2.911273784637451
step: 600 loss_mean: 2.8504764890670775
Epoch: 15 | Run time: 622.0 s | Train loss: 2.9 | Valid loss: 2.92
Saved checkpoint 15 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0255 s.
run_epoch
step: 50 loss_mean: 2.862108836174011
step: 100 loss_mean: 2.9223630952835085
step: 150 loss_mean: 2.8829045152664183
step: 200 loss_mean: 2.886018238067627
step: 250 loss_mean: 2.939026689529419
step: 300 loss_mean: 2.8165029525756835
step: 350 loss_mean: 2.9367860078811647
step: 400 loss_mean: 2.8877159357070923
step: 450 loss_mean: 2.9485899353027345
step: 500 loss_mean: 2.9035465002059935
step: 550 loss_mean: 2.866276755332947
step: 600 loss_mean: 2.8885526418685914
step: 650 loss_mean: 2.8964719867706297
step: 700 loss_mean: 2.87872718334198
step: 750 loss_mean: 2.8839393043518067
step: 800 loss_mean: 2.8944339084625246
step: 850 loss_mean: 2.8289263010025025
step: 900 loss_mean: 2.9417633056640624
step: 950 loss_mean: 2.894337258338928
step: 1000 loss_mean: 2.8523870754241942
step: 1050 loss_mean: 2.8538639736175537
step: 1100 loss_mean: 2.909806628227234
step: 1150 loss_mean: 2.917743992805481
step: 1200 loss_mean: 2.9458867120742798
step: 1250 loss_mean: 2.8800342178344724
step: 1300 loss_mean: 2.8920614290237427
step: 1350 loss_mean: 2.890338478088379
step: 1400 loss_mean: 2.8672816610336302
step: 1450 loss_mean: 2.864421157836914
step: 1500 loss_mean: 2.8833072900772097
step: 1550 loss_mean: 2.8164960384368896
step: 1600 loss_mean: 2.9106069946289064
step: 1650 loss_mean: 2.8449374628067017
step: 1700 loss_mean: 2.8820444345474243
step: 1750 loss_mean: 2.8757361650466917
step: 1800 loss_mean: 2.8715799283981323
step: 1850 loss_mean: 2.9065983152389525
step: 1900 loss_mean: 2.9384247303009032
step: 1950 loss_mean: 2.924967575073242
step: 2000 loss_mean: 2.8520547056198122
step: 2050 loss_mean: 2.9115953159332277
step: 2100 loss_mean: 2.921480402946472
step: 2150 loss_mean: 2.8734695434570314
step: 2200 loss_mean: 2.8797584199905395
step: 2250 loss_mean: 2.877113962173462
step: 2300 loss_mean: 2.897323627471924
step: 2350 loss_mean: 2.8617622661590576
step: 2400 loss_mean: 2.9748459339141844
step: 2450 loss_mean: 2.8604313278198243
step: 2500 loss_mean: 2.8590567684173585
step: 2550 loss_mean: 2.8997275495529173
step: 2600 loss_mean: 2.9252506065368653
step: 2650 loss_mean: 2.902614884376526
step: 2700 loss_mean: 2.9234245920181277
step: 2750 loss_mean: 2.9455508899688723
step: 2800 loss_mean: 2.9077120447158813
step: 2850 loss_mean: 2.9394189643859865
step: 2900 loss_mean: 2.8848636865615847
step: 2950 loss_mean: 2.87276029586792
step: 3000 loss_mean: 2.8761577796936035
step: 3050 loss_mean: 2.9098760175704954
step: 3100 loss_mean: 2.893985366821289
step: 50 loss_mean: 2.8861625957489014
step: 100 loss_mean: 2.9451673364639284
step: 150 loss_mean: 2.872425608634949
step: 200 loss_mean: 2.9842712974548338
step: 250 loss_mean: 2.9144258165359496
step: 300 loss_mean: 2.978357348442078
step: 350 loss_mean: 2.831825199127197
step: 400 loss_mean: 2.924377450942993
step: 450 loss_mean: 2.8846784973144532
step: 500 loss_mean: 2.9395471906661985
step: 550 loss_mean: 2.92350040435791
step: 600 loss_mean: 2.8469634103775023
Epoch: 16 | Run time: 622.0 s | Train loss: 2.89 | Valid loss: 2.91
Saved checkpoint 16 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0270 s.
run_epoch
step: 50 loss_mean: 2.908194508552551
step: 100 loss_mean: 2.8844133758544923
step: 150 loss_mean: 2.873902087211609
step: 200 loss_mean: 2.9233732843399047
step: 250 loss_mean: 2.8879907178878783
step: 300 loss_mean: 2.917848677635193
step: 350 loss_mean: 2.906001443862915
step: 400 loss_mean: 2.905191502571106
step: 450 loss_mean: 2.8346404504776
step: 500 loss_mean: 2.853343586921692
step: 550 loss_mean: 2.8794280672073365
step: 600 loss_mean: 2.8966757249832153
step: 650 loss_mean: 2.824240651130676
step: 700 loss_mean: 2.872954707145691
step: 750 loss_mean: 2.939352760314941
step: 800 loss_mean: 2.8554124879837035
step: 850 loss_mean: 2.8555214977264405
step: 900 loss_mean: 2.895625185966492
step: 950 loss_mean: 2.9152784729003907
step: 1000 loss_mean: 2.970007338523865
step: 1050 loss_mean: 2.886901044845581
step: 1100 loss_mean: 2.9179281997680664
step: 1150 loss_mean: 2.888062205314636
step: 1200 loss_mean: 2.8989119005203245
step: 1250 loss_mean: 2.900588812828064
step: 1300 loss_mean: 2.9194645643234254
step: 1350 loss_mean: 2.890677881240845
step: 1400 loss_mean: 2.8709183025360105
step: 1450 loss_mean: 2.869658360481262
step: 1500 loss_mean: 2.910989465713501
step: 1550 loss_mean: 2.8527465057373047
step: 1600 loss_mean: 2.842895197868347
step: 1650 loss_mean: 2.861560425758362
step: 1700 loss_mean: 2.903721685409546
step: 1750 loss_mean: 2.8912344455718992
step: 1800 loss_mean: 2.873976259231567
step: 1850 loss_mean: 2.8590043640136718
step: 1900 loss_mean: 2.8778550481796263
step: 1950 loss_mean: 2.9217083406448365
step: 2000 loss_mean: 2.896408643722534
step: 2050 loss_mean: 2.86409873008728
step: 2100 loss_mean: 2.850840826034546
step: 2150 loss_mean: 2.909922103881836
step: 2200 loss_mean: 2.84345263004303
step: 2250 loss_mean: 2.8479657793045043
step: 2300 loss_mean: 2.8585083580017088
step: 2350 loss_mean: 2.8949401330947877
step: 2400 loss_mean: 2.826135935783386
step: 2450 loss_mean: 2.8565152645111085
step: 2500 loss_mean: 2.901315197944641
step: 2550 loss_mean: 2.8754603242874146
step: 2600 loss_mean: 2.8875379753112793
step: 2650 loss_mean: 2.871713047027588
step: 2700 loss_mean: 2.9006805324554445
step: 2750 loss_mean: 2.926517615318298
step: 2800 loss_mean: 2.8706278562545777
step: 2850 loss_mean: 2.8591514682769774
step: 2900 loss_mean: 2.865943202972412
step: 2950 loss_mean: 2.903905487060547
step: 3000 loss_mean: 2.893206238746643
step: 3050 loss_mean: 2.8200537252426146
step: 3100 loss_mean: 2.9091862344741823
step: 50 loss_mean: 2.930778636932373
step: 100 loss_mean: 3.004300889968872
step: 150 loss_mean: 2.8717264127731323
step: 200 loss_mean: 3.0136533403396606
step: 250 loss_mean: 2.9580737209320067
step: 300 loss_mean: 3.0398523950576783
step: 350 loss_mean: 2.8828394293785093
step: 400 loss_mean: 2.9170283269882202
step: 450 loss_mean: 2.8945406532287596
step: 500 loss_mean: 2.9887616634368896
step: 550 loss_mean: 2.9607437229156495
step: 600 loss_mean: 2.8682282257080076
Epoch: 17 | Run time: 619.0 s | Train loss: 2.88 | Valid loss: 2.94
Saved checkpoint 17 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0283 s.
run_epoch
step: 50 loss_mean: 2.855808777809143
step: 100 loss_mean: 2.8435735273361207
step: 150 loss_mean: 2.9382288551330564
step: 200 loss_mean: 2.929672417640686
step: 250 loss_mean: 2.840863437652588
step: 300 loss_mean: 2.866277747154236
step: 350 loss_mean: 2.895764527320862
step: 400 loss_mean: 2.8674380493164064
step: 450 loss_mean: 2.871677975654602
step: 500 loss_mean: 2.8468058824539186
step: 550 loss_mean: 2.8501933717727663
step: 600 loss_mean: 2.855312213897705
step: 650 loss_mean: 2.8622725915908815
step: 700 loss_mean: 2.787813763618469
step: 750 loss_mean: 2.9186525535583496
step: 800 loss_mean: 2.845311369895935
step: 850 loss_mean: 2.9329571676254274
step: 900 loss_mean: 2.7903018188476563
step: 950 loss_mean: 2.861639099121094
step: 1000 loss_mean: 2.8334736490249632
step: 1050 loss_mean: 2.8856274604797365
step: 1100 loss_mean: 2.852305574417114
step: 1150 loss_mean: 2.83662766456604
step: 1200 loss_mean: 2.8867753076553346
step: 1250 loss_mean: 2.861258478164673
step: 1300 loss_mean: 2.8936601066589356
step: 1350 loss_mean: 2.928669853210449
step: 1400 loss_mean: 2.867441167831421
step: 1450 loss_mean: 2.9220721912384033
step: 1500 loss_mean: 2.9230972480773927
step: 1550 loss_mean: 2.883927206993103
step: 1600 loss_mean: 2.8970294857025145
step: 1650 loss_mean: 2.842378897666931
step: 1700 loss_mean: 2.8674681568145752
step: 1750 loss_mean: 2.9252853441238402
step: 1800 loss_mean: 2.86503879070282
step: 1850 loss_mean: 2.93163800239563
step: 1900 loss_mean: 2.8957236051559447
step: 1950 loss_mean: 2.8588680171966554
step: 2000 loss_mean: 2.865301513671875
step: 2050 loss_mean: 2.9230840587615967
step: 2100 loss_mean: 2.9034491634368895
step: 2150 loss_mean: 2.8423715448379516
step: 2200 loss_mean: 2.854566798210144
step: 2250 loss_mean: 2.874794158935547
step: 2300 loss_mean: 2.907347626686096
step: 2350 loss_mean: 2.87326367855072
step: 2400 loss_mean: 2.904525203704834
step: 2450 loss_mean: 2.8685973358154295
step: 2500 loss_mean: 2.88346453666687
step: 2550 loss_mean: 2.8888459730148317
step: 2600 loss_mean: 2.899074935913086
step: 2650 loss_mean: 2.890802516937256
step: 2700 loss_mean: 2.8434880495071413
step: 2750 loss_mean: 2.8745765781402586
step: 2800 loss_mean: 2.849676947593689
step: 2850 loss_mean: 2.876005859375
step: 2900 loss_mean: 2.8701504373550417
step: 2950 loss_mean: 2.873733720779419
step: 3000 loss_mean: 2.8292564868927004
step: 3050 loss_mean: 2.9496724462509154
step: 3100 loss_mean: 2.878117551803589
step: 50 loss_mean: 2.8706455564498903
step: 100 loss_mean: 2.924736289978027
step: 150 loss_mean: 2.8337587547302245
step: 200 loss_mean: 2.9440427732467653
step: 250 loss_mean: 2.890457172393799
step: 300 loss_mean: 2.952445216178894
step: 350 loss_mean: 2.8212599420547484
step: 400 loss_mean: 2.893790955543518
step: 450 loss_mean: 2.8515311002731325
step: 500 loss_mean: 2.915061798095703
step: 550 loss_mean: 2.8816153812408447
step: 600 loss_mean: 2.8119688749313356
Epoch: 18 | Run time: 622.0 s | Train loss: 2.88 | Valid loss: 2.88
Saved checkpoint 18 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0295 s.
run_epoch
step: 50 loss_mean: 2.816366682052612
step: 100 loss_mean: 2.90230908870697
step: 150 loss_mean: 2.8632685422897337
step: 200 loss_mean: 2.8876006698608396
step: 250 loss_mean: 2.89491126537323
step: 300 loss_mean: 2.8555261421203615
step: 350 loss_mean: 2.8532750511169436
step: 400 loss_mean: 2.885744161605835
step: 450 loss_mean: 2.9046596002578737
step: 500 loss_mean: 2.792366280555725
step: 550 loss_mean: 2.867150297164917
step: 600 loss_mean: 2.8402442502975465
step: 650 loss_mean: 2.8963791131973267
step: 700 loss_mean: 2.7985749769210817
step: 750 loss_mean: 2.8813437271118163
step: 800 loss_mean: 2.8991344118118287
step: 850 loss_mean: 2.8745109462738037
step: 900 loss_mean: 2.871280164718628
step: 950 loss_mean: 2.9123655223846434
step: 1000 loss_mean: 2.8283930683135985
step: 1050 loss_mean: 2.856551809310913
step: 1100 loss_mean: 2.8945578050613405
step: 1150 loss_mean: 2.8361431312561036
step: 1200 loss_mean: 2.8675835132598877
step: 1250 loss_mean: 2.8971621084213255
step: 1300 loss_mean: 2.8939144134521486
step: 1350 loss_mean: 2.881243224143982
step: 1400 loss_mean: 2.889642186164856
step: 1450 loss_mean: 2.817726240158081
step: 1500 loss_mean: 2.878850026130676
step: 1550 loss_mean: 2.8781304264068606
step: 1600 loss_mean: 2.8859942150115967
step: 1650 loss_mean: 2.8984175729751587
step: 1700 loss_mean: 2.8581013965606687
step: 1750 loss_mean: 2.8751711082458495
step: 1800 loss_mean: 2.8726274061203
step: 1850 loss_mean: 2.854488673210144
step: 1900 loss_mean: 2.908657774925232
step: 1950 loss_mean: 2.8959533882141115
step: 2000 loss_mean: 2.8638730478286742
step: 2050 loss_mean: 2.8724102354049683
step: 2100 loss_mean: 2.847237229347229
step: 2150 loss_mean: 2.9030379390716554
step: 2200 loss_mean: 2.866663308143616
step: 2250 loss_mean: 2.9055476999282837
step: 2300 loss_mean: 2.827353625297546
step: 2350 loss_mean: 2.896291947364807
step: 2400 loss_mean: 2.8771752166748046
step: 2450 loss_mean: 2.8123650312423707
step: 2500 loss_mean: 2.784828577041626
step: 2550 loss_mean: 2.9073536872863768
step: 2600 loss_mean: 2.876068911552429
step: 2650 loss_mean: 2.8765037775039675
step: 2700 loss_mean: 2.8525175642967224
step: 2750 loss_mean: 2.8887732458114623
step: 2800 loss_mean: 2.8548019409179686
step: 2850 loss_mean: 2.8593987941741945
step: 2900 loss_mean: 2.8901174688339233
step: 2950 loss_mean: 2.8155736923217773
step: 3000 loss_mean: 2.901186580657959
step: 3050 loss_mean: 2.90464702129364
step: 3100 loss_mean: 2.8584125661849975
step: 50 loss_mean: 2.8890638732910157
step: 100 loss_mean: 2.95051438331604
step: 150 loss_mean: 2.8273030042648317
step: 200 loss_mean: 2.9672809934616087
step: 250 loss_mean: 2.9168905782699586
step: 300 loss_mean: 2.976578350067139
step: 350 loss_mean: 2.822766799926758
step: 400 loss_mean: 2.8755499601364134
step: 450 loss_mean: 2.859981670379639
step: 500 loss_mean: 2.922844533920288
step: 550 loss_mean: 2.893193917274475
step: 600 loss_mean: 2.825555763244629
Epoch: 19 | Run time: 621.0 s | Train loss: 2.87 | Valid loss: 2.9
Saved checkpoint 19 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0306 s.
run_epoch
step: 50 loss_mean: 2.861803970336914
step: 100 loss_mean: 2.864224991798401
step: 150 loss_mean: 2.8490972328186035
step: 200 loss_mean: 2.8857043409347534
step: 250 loss_mean: 2.84920578956604
step: 300 loss_mean: 2.8211705112457275
step: 350 loss_mean: 2.81455406665802
step: 400 loss_mean: 2.8478061962127685
step: 450 loss_mean: 2.8306598329544066
step: 500 loss_mean: 2.895210614204407
step: 550 loss_mean: 2.7795327949523925
step: 600 loss_mean: 2.8387815856933596
step: 650 loss_mean: 2.886876258850098
step: 700 loss_mean: 2.9121661520004274
step: 750 loss_mean: 2.842571597099304
step: 800 loss_mean: 2.8517949104309084
step: 850 loss_mean: 2.9178236961364745
step: 900 loss_mean: 2.8625721311569214
step: 950 loss_mean: 2.848931746482849
step: 1000 loss_mean: 2.827383270263672
step: 1050 loss_mean: 2.86073260307312
step: 1100 loss_mean: 2.8840635299682615
step: 1150 loss_mean: 2.8384867191314695
step: 1200 loss_mean: 2.79992883682251
step: 1250 loss_mean: 2.8545430803298952
step: 1300 loss_mean: 2.7962397384643554
step: 1350 loss_mean: 2.8624035835266115
step: 1400 loss_mean: 2.865981822013855
step: 1450 loss_mean: 2.9469546127319335
step: 1500 loss_mean: 2.8895121383666993
step: 1550 loss_mean: 2.801569366455078
step: 1600 loss_mean: 2.829771203994751
step: 1650 loss_mean: 2.8893512868881226
step: 1700 loss_mean: 2.844511399269104
step: 1750 loss_mean: 2.8525994062423705
step: 1800 loss_mean: 2.9163565540313723
step: 1850 loss_mean: 2.835818843841553
step: 1900 loss_mean: 2.8900776958465575
step: 1950 loss_mean: 2.876668634414673
step: 2000 loss_mean: 2.8648362588882446
step: 2050 loss_mean: 2.865883436203003
step: 2100 loss_mean: 2.8450039434432983
step: 2150 loss_mean: 2.8242914485931396
step: 2200 loss_mean: 2.8989496755599977
step: 2250 loss_mean: 2.881593041419983
step: 2300 loss_mean: 2.8545347690582275
step: 2350 loss_mean: 2.8803489112854006
step: 2400 loss_mean: 2.8797408056259157
step: 2450 loss_mean: 2.8925512075424193
step: 2500 loss_mean: 2.821550326347351
step: 2550 loss_mean: 2.8390459775924684
step: 2600 loss_mean: 2.8675264358520507
step: 2650 loss_mean: 2.8139879179000853
step: 2700 loss_mean: 2.873539505004883
step: 2750 loss_mean: 2.848043694496155
step: 2800 loss_mean: 2.841894955635071
step: 2850 loss_mean: 2.872689628601074
step: 2900 loss_mean: 2.882723431587219
step: 2950 loss_mean: 2.822620348930359
step: 3000 loss_mean: 2.8837170362472535
step: 3050 loss_mean: 2.8995892238616943
step: 3100 loss_mean: 2.875978422164917
step: 50 loss_mean: 2.8621759271621703
step: 100 loss_mean: 2.9247879219055175
step: 150 loss_mean: 2.821664042472839
step: 200 loss_mean: 2.9406695318222047
step: 250 loss_mean: 2.8906118154525755
step: 300 loss_mean: 2.946074199676514
step: 350 loss_mean: 2.8064047384262083
step: 400 loss_mean: 2.882839336395264
step: 450 loss_mean: 2.8449810934066773
step: 500 loss_mean: 2.9031545972824095
step: 550 loss_mean: 2.875974473953247
step: 600 loss_mean: 2.8062639570236207
Epoch: 20 | Run time: 621.0 s | Train loss: 2.86 | Valid loss: 2.88
Saved checkpoint 20 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0322 s.
run_epoch
step: 50 loss_mean: 2.869307279586792
step: 100 loss_mean: 2.810500292778015
step: 150 loss_mean: 2.877166666984558
step: 200 loss_mean: 2.855077304840088
step: 250 loss_mean: 2.8446477699279784
step: 300 loss_mean: 2.8343258905410766
step: 350 loss_mean: 2.819831590652466
step: 400 loss_mean: 2.8704471683502195
step: 450 loss_mean: 2.8785824584960937
step: 500 loss_mean: 2.887016043663025
step: 550 loss_mean: 2.8225262784957885
step: 600 loss_mean: 2.8535817766189577
step: 650 loss_mean: 2.8449719381332397
step: 700 loss_mean: 2.848244833946228
step: 750 loss_mean: 2.807756185531616
step: 800 loss_mean: 2.8944535636901856
step: 850 loss_mean: 2.9468988084793093
step: 900 loss_mean: 2.8588353395462036
step: 950 loss_mean: 2.8578827476501463
step: 1000 loss_mean: 2.8578455018997193
step: 1050 loss_mean: 2.8406213665008546
step: 1100 loss_mean: 2.835818877220154
step: 1150 loss_mean: 2.763201379776001
step: 1200 loss_mean: 2.831423854827881
step: 1250 loss_mean: 2.8792245817184448
step: 1300 loss_mean: 2.8891879510879517
step: 1350 loss_mean: 2.8517584466934203
step: 1400 loss_mean: 2.8995358657836916
step: 1450 loss_mean: 2.858575711250305
step: 1500 loss_mean: 2.8258378219604494
step: 1550 loss_mean: 2.8607838010787963
step: 1600 loss_mean: 2.859888401031494
step: 1650 loss_mean: 2.7831362295150757
step: 1700 loss_mean: 2.8103313064575195
step: 1750 loss_mean: 2.8439326667785645
step: 1800 loss_mean: 2.869470725059509
step: 1850 loss_mean: 2.906239695549011
step: 1900 loss_mean: 2.7892798376083374
step: 1950 loss_mean: 2.7998691749572755
step: 2000 loss_mean: 2.7928518199920656
step: 2050 loss_mean: 2.8638605880737305
step: 2100 loss_mean: 2.8667981815338135
step: 2150 loss_mean: 2.8604067611694335
step: 2200 loss_mean: 2.8594485759735107
step: 2250 loss_mean: 2.8826861715316774
step: 2300 loss_mean: 2.8604330682754515
step: 2350 loss_mean: 2.8186113166809084
step: 2400 loss_mean: 2.8510635566711424
step: 2450 loss_mean: 2.864274253845215
step: 2500 loss_mean: 2.854774761199951
step: 2550 loss_mean: 2.8231434488296507
step: 2600 loss_mean: 2.928512225151062
step: 2650 loss_mean: 2.8742396211624146
step: 2700 loss_mean: 2.8729382610321044
step: 2750 loss_mean: 2.8249004650115968
step: 2800 loss_mean: 2.827600541114807
step: 2850 loss_mean: 2.903884882926941
step: 2900 loss_mean: 2.8549507188797
step: 2950 loss_mean: 2.9058825016021728
step: 3000 loss_mean: 2.8898726081848145
step: 3050 loss_mean: 2.865289635658264
step: 3100 loss_mean: 2.8357910823822023
step: 50 loss_mean: 2.8622843980789185
step: 100 loss_mean: 2.9206496238708497
step: 150 loss_mean: 2.8288976192474364
step: 200 loss_mean: 2.9394882011413572
step: 250 loss_mean: 2.879509563446045
step: 300 loss_mean: 2.9477515745162965
step: 350 loss_mean: 2.8092103242874145
step: 400 loss_mean: 2.8753457975387575
step: 450 loss_mean: 2.8482094287872313
step: 500 loss_mean: 2.8883269929885866
step: 550 loss_mean: 2.85029420375824
step: 600 loss_mean: 2.7974833106994628
Epoch: 21 | Run time: 621.0 s | Train loss: 2.85 | Valid loss: 2.87
Saved checkpoint 21 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0334 s.
run_epoch
step: 50 loss_mean: 2.849661736488342
step: 100 loss_mean: 2.9198177671432495
step: 150 loss_mean: 2.783647165298462
step: 200 loss_mean: 2.8250356769561766
step: 250 loss_mean: 2.778685812950134
step: 300 loss_mean: 2.8467284727096556
step: 350 loss_mean: 2.81444363117218
step: 400 loss_mean: 2.8637460565567014
step: 450 loss_mean: 2.86010169506073
step: 500 loss_mean: 2.8959224891662596
step: 550 loss_mean: 2.9173969173431398
step: 600 loss_mean: 2.9064231872558595
step: 650 loss_mean: 2.7971761989593507
step: 700 loss_mean: 2.8745542907714845
step: 750 loss_mean: 2.854742178916931
step: 800 loss_mean: 2.8533676195144655
step: 850 loss_mean: 2.836895384788513
step: 900 loss_mean: 2.8842484283447267
step: 950 loss_mean: 2.8847265100479125
step: 1000 loss_mean: 2.8391599369049074
step: 1050 loss_mean: 2.834452691078186
step: 1100 loss_mean: 2.866650605201721
step: 1150 loss_mean: 2.842325949668884
step: 1200 loss_mean: 2.8313900136947634
step: 1250 loss_mean: 2.8056980848312376
step: 1300 loss_mean: 2.879688229560852
step: 1350 loss_mean: 2.83943217754364
step: 1400 loss_mean: 2.8507898235321045
step: 1450 loss_mean: 2.8332170629501343
step: 1500 loss_mean: 2.889674916267395
step: 1550 loss_mean: 2.8812608337402343
step: 1600 loss_mean: 2.7657625007629396
step: 1650 loss_mean: 2.811717138290405
step: 1700 loss_mean: 2.8506506395339968
step: 1750 loss_mean: 2.8885457849502565
step: 1800 loss_mean: 2.8052680015563967
step: 1850 loss_mean: 2.890788769721985
step: 1900 loss_mean: 2.8697700977325438
step: 1950 loss_mean: 2.8175414323806764
step: 2000 loss_mean: 2.8803859663009646
step: 2050 loss_mean: 2.831492123603821
step: 2100 loss_mean: 2.901631164550781
step: 2150 loss_mean: 2.869087996482849
step: 2200 loss_mean: 2.839152703285217
step: 2250 loss_mean: 2.8263042306900026
step: 2300 loss_mean: 2.839383020401001
step: 2350 loss_mean: 2.88348934173584
step: 2400 loss_mean: 2.8076900959014894
step: 2450 loss_mean: 2.8223380517959593
step: 2500 loss_mean: 2.8646724700927733
step: 2550 loss_mean: 2.8605654430389404
step: 2600 loss_mean: 2.8854797077178955
step: 2650 loss_mean: 2.846995749473572
step: 2700 loss_mean: 2.7897449493408204
step: 2750 loss_mean: 2.81603657245636
step: 2800 loss_mean: 2.841229453086853
step: 2850 loss_mean: 2.87273223400116
step: 2900 loss_mean: 2.8222680997848513
step: 2950 loss_mean: 2.8801097679138183
step: 3000 loss_mean: 2.895947003364563
step: 3050 loss_mean: 2.8245934534072874
step: 3100 loss_mean: 2.7931374216079714
step: 50 loss_mean: 2.8576541709899903
step: 100 loss_mean: 2.9001227045059204
step: 150 loss_mean: 2.809922857284546
step: 200 loss_mean: 2.9288848543167116
step: 250 loss_mean: 2.8746309804916383
step: 300 loss_mean: 2.940546932220459
step: 350 loss_mean: 2.8013069152832033
step: 400 loss_mean: 2.859553294181824
step: 450 loss_mean: 2.827583088874817
step: 500 loss_mean: 2.892731533050537
step: 550 loss_mean: 2.860243458747864
step: 600 loss_mean: 2.785670909881592
Epoch: 22 | Run time: 621.0 s | Train loss: 2.85 | Valid loss: 2.86
Saved checkpoint 22 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0348 s.
run_epoch
step: 50 loss_mean: 2.8428275394439697
step: 100 loss_mean: 2.8411985492706298
step: 150 loss_mean: 2.802150249481201
step: 200 loss_mean: 2.8753647661209105
step: 250 loss_mean: 2.8476742935180663
step: 300 loss_mean: 2.8165531301498414
step: 350 loss_mean: 2.904029679298401
step: 400 loss_mean: 2.8483520555496216
step: 450 loss_mean: 2.8748571395874025
step: 500 loss_mean: 2.838167848587036
step: 550 loss_mean: 2.8349097204208373
step: 600 loss_mean: 2.8809461402893066
step: 650 loss_mean: 2.7569686698913576
step: 700 loss_mean: 2.8297130727767943
step: 750 loss_mean: 2.874519052505493
step: 800 loss_mean: 2.8074809169769286
step: 850 loss_mean: 2.854150357246399
step: 900 loss_mean: 2.8384952402114867
step: 950 loss_mean: 2.8315916061401367
step: 1000 loss_mean: 2.7983633613586427
step: 1050 loss_mean: 2.835611038208008
step: 1100 loss_mean: 2.8355091619491577
step: 1150 loss_mean: 2.8270248985290527
step: 1200 loss_mean: 2.8448898553848267
step: 1250 loss_mean: 2.8837834119796755
step: 1300 loss_mean: 2.8422799205780027
step: 1350 loss_mean: 2.8701401996612548
step: 1400 loss_mean: 2.8927807998657227
step: 1450 loss_mean: 2.855957431793213
step: 1500 loss_mean: 2.8509886837005616
step: 1550 loss_mean: 2.848922462463379
step: 1600 loss_mean: 2.8604624843597413
step: 1650 loss_mean: 2.8172415590286253
step: 1700 loss_mean: 2.9240417766571043
step: 1750 loss_mean: 2.8298617601394653
step: 1800 loss_mean: 2.769081406593323
step: 1850 loss_mean: 2.8200770139694216
step: 1900 loss_mean: 2.899047441482544
step: 1950 loss_mean: 2.8493292808532713
step: 2000 loss_mean: 2.8234162664413454
step: 2050 loss_mean: 2.856017804145813
step: 2100 loss_mean: 2.771270079612732
step: 2150 loss_mean: 2.810601887702942
step: 2200 loss_mean: 2.85050283908844
step: 2250 loss_mean: 2.919148020744324
step: 2300 loss_mean: 2.8812561941146853
step: 2350 loss_mean: 2.813594923019409
step: 2400 loss_mean: 2.793403172492981
step: 2450 loss_mean: 2.855076994895935
step: 2500 loss_mean: 2.819518141746521
step: 2550 loss_mean: 2.880773119926453
step: 2600 loss_mean: 2.778819212913513
step: 2650 loss_mean: 2.8612375593185426
step: 2700 loss_mean: 2.859840784072876
step: 2750 loss_mean: 2.845345892906189
step: 2800 loss_mean: 2.8178835582733153
step: 2850 loss_mean: 2.8408064460754394
step: 2900 loss_mean: 2.838835234642029
step: 2950 loss_mean: 2.8919914674758913
step: 3000 loss_mean: 2.8687216949462893
step: 3050 loss_mean: 2.8597619581222533
step: 3100 loss_mean: 2.8136716222763063
step: 50 loss_mean: 2.8519361639022827
step: 100 loss_mean: 2.914177074432373
step: 150 loss_mean: 2.808538455963135
step: 200 loss_mean: 2.9662532234191894
step: 250 loss_mean: 2.8690734338760375
step: 300 loss_mean: 2.9588617134094237
step: 350 loss_mean: 2.7851046442985536
step: 400 loss_mean: 2.8529955339431763
step: 450 loss_mean: 2.829820909500122
step: 500 loss_mean: 2.8882404375076294
step: 550 loss_mean: 2.867782864570618
step: 600 loss_mean: 2.7732424211502074
Epoch: 23 | Run time: 623.0 s | Train loss: 2.84 | Valid loss: 2.87
Saved checkpoint 23 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0369 s.
run_epoch
step: 50 loss_mean: 2.8208098220825195
step: 100 loss_mean: 2.889881634712219
step: 150 loss_mean: 2.900245952606201
step: 200 loss_mean: 2.7760196352005004
step: 250 loss_mean: 2.8303160095214843
step: 300 loss_mean: 2.779134163856506
step: 350 loss_mean: 2.8210580730438233
step: 400 loss_mean: 2.8823322057724
step: 450 loss_mean: 2.8123430156707765
step: 500 loss_mean: 2.8190357875823975
step: 550 loss_mean: 2.862635407447815
step: 600 loss_mean: 2.853745222091675
step: 650 loss_mean: 2.845106506347656
step: 700 loss_mean: 2.822749557495117
step: 750 loss_mean: 2.87344934463501
step: 800 loss_mean: 2.775655355453491
step: 850 loss_mean: 2.8197556829452513
step: 900 loss_mean: 2.8204289627075196
step: 950 loss_mean: 2.8208633041381836
step: 1000 loss_mean: 2.8253415918350218
step: 1050 loss_mean: 2.836020050048828
step: 1100 loss_mean: 2.80455153465271
step: 1150 loss_mean: 2.8742195415496825
step: 1200 loss_mean: 2.7827197217941286
step: 1250 loss_mean: 2.8214725637435913
step: 1300 loss_mean: 2.81380886554718
step: 1350 loss_mean: 2.8689651012420656
step: 1400 loss_mean: 2.809488501548767
step: 1450 loss_mean: 2.850079131126404
step: 1500 loss_mean: 2.8728020191192627
step: 1550 loss_mean: 2.8448430919647216
step: 1600 loss_mean: 2.820683436393738
step: 1650 loss_mean: 2.808606286048889
step: 1700 loss_mean: 2.8158130121231078
step: 1750 loss_mean: 2.850583200454712
step: 1800 loss_mean: 2.8171130657196044
step: 1850 loss_mean: 2.8517322731018067
step: 1900 loss_mean: 2.8646263122558593
step: 1950 loss_mean: 2.8688899803161623
step: 2000 loss_mean: 2.8079724311828613
step: 2050 loss_mean: 2.902919359207153
step: 2100 loss_mean: 2.8869141864776613
step: 2150 loss_mean: 2.872087206840515
step: 2200 loss_mean: 2.8357988834381103
step: 2250 loss_mean: 2.8209130001068115
step: 2300 loss_mean: 2.826288437843323
step: 2350 loss_mean: 2.8622403144836426
step: 2400 loss_mean: 2.8132232284545897
step: 2450 loss_mean: 2.84653359413147
step: 2500 loss_mean: 2.865716691017151
step: 2550 loss_mean: 2.8415404558181763
step: 2600 loss_mean: 2.729257273674011
step: 2650 loss_mean: 2.8042590761184694
step: 2700 loss_mean: 2.881953682899475
step: 2750 loss_mean: 2.834381923675537
step: 2800 loss_mean: 2.7962586879730225
step: 2850 loss_mean: 2.853912901878357
step: 2900 loss_mean: 2.8120627212524414
step: 2950 loss_mean: 2.8822103357315063
step: 3000 loss_mean: 2.8396910667419433
step: 3050 loss_mean: 2.8770545530319214
step: 3100 loss_mean: 2.8838370180130006
step: 50 loss_mean: 2.8583714151382447
step: 100 loss_mean: 2.9186229133605956
step: 150 loss_mean: 2.8535443639755247
step: 200 loss_mean: 2.9397883892059324
step: 250 loss_mean: 2.892320499420166
step: 300 loss_mean: 2.9491897296905516
step: 350 loss_mean: 2.844442563056946
step: 400 loss_mean: 2.8993083906173704
step: 450 loss_mean: 2.8500012588500976
step: 500 loss_mean: 2.8825536489486696
step: 550 loss_mean: 2.861374092102051
step: 600 loss_mean: 2.8087281608581542
Epoch: 24 | Run time: 623.0 s | Train loss: 2.84 | Valid loss: 2.88
Saved checkpoint 24 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0381 s.
run_epoch
step: 50 loss_mean: 2.8482444190979006
step: 100 loss_mean: 2.8144531631469727
step: 150 loss_mean: 2.8172668552398683
step: 200 loss_mean: 2.7658968448638914
step: 250 loss_mean: 2.803725700378418
step: 300 loss_mean: 2.846173281669617
step: 350 loss_mean: 2.8069499588012694
step: 400 loss_mean: 2.8093142032623293
step: 450 loss_mean: 2.836085276603699
step: 500 loss_mean: 2.8415266704559325
step: 550 loss_mean: 2.849959235191345
step: 600 loss_mean: 2.8750586223602297
step: 650 loss_mean: 2.8166146230697633
step: 700 loss_mean: 2.8170116472244264
step: 750 loss_mean: 2.8883334302902224
step: 800 loss_mean: 2.870262608528137
step: 850 loss_mean: 2.814025421142578
step: 900 loss_mean: 2.8294966173172
step: 950 loss_mean: 2.8564881229400636
step: 1000 loss_mean: 2.8641004371643066
step: 1050 loss_mean: 2.8288200378417967
step: 1100 loss_mean: 2.8635987186431886
step: 1150 loss_mean: 2.8697978687286376
step: 1200 loss_mean: 2.83657594203949
step: 1250 loss_mean: 2.7985203742980955
step: 1300 loss_mean: 2.840589919090271
step: 1350 loss_mean: 2.8421435260772707
step: 1400 loss_mean: 2.8826393461227418
step: 1450 loss_mean: 2.7952844953536986
step: 1500 loss_mean: 2.8617960691452025
step: 1550 loss_mean: 2.837976350784302
step: 1600 loss_mean: 2.817493076324463
step: 1650 loss_mean: 2.7844911766052247
step: 1700 loss_mean: 2.8702931118011477
step: 1750 loss_mean: 2.838463587760925
step: 1800 loss_mean: 2.796822385787964
step: 1850 loss_mean: 2.818695611953735
step: 1900 loss_mean: 2.852529821395874
step: 1950 loss_mean: 2.7986507654190063
step: 2000 loss_mean: 2.903692812919617
step: 2050 loss_mean: 2.796172308921814
step: 2100 loss_mean: 2.8794437217712403
step: 2150 loss_mean: 2.8383134460449218
step: 2200 loss_mean: 2.831662073135376
step: 2250 loss_mean: 2.855099949836731
step: 2300 loss_mean: 2.813575978279114
step: 2350 loss_mean: 2.8331147241592407
step: 2400 loss_mean: 2.8135487747192385
step: 2450 loss_mean: 2.830543260574341
step: 2500 loss_mean: 2.8423022174835206
step: 2550 loss_mean: 2.9084668731689454
step: 2600 loss_mean: 2.838015899658203
step: 2650 loss_mean: 2.759857721328735
step: 2700 loss_mean: 2.796514711380005
step: 2750 loss_mean: 2.864918336868286
step: 2800 loss_mean: 2.8298270225524904
step: 2850 loss_mean: 2.799954881668091
step: 2900 loss_mean: 2.8258646631240847
step: 2950 loss_mean: 2.8439582872390745
step: 3000 loss_mean: 2.8333900928497315
step: 3050 loss_mean: 2.879794940948486
step: 3100 loss_mean: 2.7783884477615355
step: 50 loss_mean: 2.851896743774414
step: 100 loss_mean: 2.90939181804657
step: 150 loss_mean: 2.828204770088196
step: 200 loss_mean: 2.9216298484802246
step: 250 loss_mean: 2.8847948598861692
step: 300 loss_mean: 2.9208611726760862
step: 350 loss_mean: 2.817755494117737
step: 400 loss_mean: 2.886287531852722
step: 450 loss_mean: 2.836858115196228
step: 500 loss_mean: 2.8871414184570314
step: 550 loss_mean: 2.83926296710968
step: 600 loss_mean: 2.788269019126892
Epoch: 25 | Run time: 623.0 s | Train loss: 2.83 | Valid loss: 2.87
Saved checkpoint 25 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0392 s.
run_epoch
step: 50 loss_mean: 2.8416844844818114
step: 100 loss_mean: 2.810577087402344
step: 150 loss_mean: 2.8003885078430177
step: 200 loss_mean: 2.8100383138656615
step: 250 loss_mean: 2.8452361822128296
step: 300 loss_mean: 2.870190110206604
step: 350 loss_mean: 2.8218936014175413
step: 400 loss_mean: 2.8446469926834106
step: 450 loss_mean: 2.8857067108154295
step: 500 loss_mean: 2.8248382663726805
step: 550 loss_mean: 2.801338863372803
step: 600 loss_mean: 2.839137306213379
step: 650 loss_mean: 2.835728235244751
step: 700 loss_mean: 2.8520793437957765
step: 750 loss_mean: 2.8157026672363283
step: 800 loss_mean: 2.86000602722168
step: 850 loss_mean: 2.822755217552185
step: 900 loss_mean: 2.8053231334686277
step: 950 loss_mean: 2.7921326541900635
step: 1000 loss_mean: 2.799903326034546
step: 1050 loss_mean: 2.8626934337615966
step: 1100 loss_mean: 2.8538226461410523
step: 1150 loss_mean: 2.8280166149139405
step: 1200 loss_mean: 2.895099468231201
step: 1250 loss_mean: 2.818026232719421
step: 1300 loss_mean: 2.8356394529342652
step: 1350 loss_mean: 2.844011821746826
step: 1400 loss_mean: 2.8530718755722044
step: 1450 loss_mean: 2.8447131156921386
step: 1500 loss_mean: 2.814361000061035
step: 1550 loss_mean: 2.823998966217041
step: 1600 loss_mean: 2.8056161260604857
step: 1650 loss_mean: 2.8373985195159914
step: 1700 loss_mean: 2.799257507324219
step: 1750 loss_mean: 2.843638024330139
step: 1800 loss_mean: 2.819589238166809
step: 1850 loss_mean: 2.8354640102386472
step: 1900 loss_mean: 2.849338631629944
step: 1950 loss_mean: 2.8530067825317382
step: 2000 loss_mean: 2.7940687322616578
step: 2050 loss_mean: 2.8382902479171754
step: 2100 loss_mean: 2.8395443391799926
step: 2150 loss_mean: 2.7763979768753053
step: 2200 loss_mean: 2.8348899841308595
step: 2250 loss_mean: 2.827654175758362
step: 2300 loss_mean: 2.8532485866546633
step: 2350 loss_mean: 2.8508074283599854
step: 2400 loss_mean: 2.8702765130996704
step: 2450 loss_mean: 2.811766185760498
step: 2500 loss_mean: 2.8367181539535524
step: 2550 loss_mean: 2.742334098815918
step: 2600 loss_mean: 2.858752422332764
step: 2650 loss_mean: 2.878120846748352
step: 2700 loss_mean: 2.811240005493164
step: 2750 loss_mean: 2.7689308500289918
step: 2800 loss_mean: 2.8842833614349366
step: 2850 loss_mean: 2.8608749532699584
step: 2900 loss_mean: 2.865650987625122
step: 2950 loss_mean: 2.791604561805725
step: 3000 loss_mean: 2.758967571258545
step: 3050 loss_mean: 2.789529004096985
step: 3100 loss_mean: 2.830337142944336
step: 50 loss_mean: 2.8437580060958862
step: 100 loss_mean: 2.8920112562179567
step: 150 loss_mean: 2.8083791589736937
step: 200 loss_mean: 2.9252716970443724
step: 250 loss_mean: 2.854694004058838
step: 300 loss_mean: 2.919119162559509
step: 350 loss_mean: 2.7860096096992493
step: 400 loss_mean: 2.848329954147339
step: 450 loss_mean: 2.8234061098098753
step: 500 loss_mean: 2.870345907211304
step: 550 loss_mean: 2.834392423629761
step: 600 loss_mean: 2.7813239192962644
Epoch: 26 | Run time: 622.0 s | Train loss: 2.83 | Valid loss: 2.85
Saved checkpoint 26 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0409 s.
run_epoch
step: 50 loss_mean: 2.856956377029419
step: 100 loss_mean: 2.8498451709747314
step: 150 loss_mean: 2.826344380378723
step: 200 loss_mean: 2.8784609413146973
step: 250 loss_mean: 2.8216905117034914
step: 300 loss_mean: 2.785198783874512
step: 350 loss_mean: 2.829383282661438
step: 400 loss_mean: 2.8140797805786133
step: 450 loss_mean: 2.846455683708191
step: 500 loss_mean: 2.8281933736801146
step: 550 loss_mean: 2.806980137825012
step: 600 loss_mean: 2.8317844581604006
step: 650 loss_mean: 2.8653261423110963
step: 700 loss_mean: 2.842622747421265
step: 750 loss_mean: 2.814423441886902
step: 800 loss_mean: 2.794967284202576
step: 850 loss_mean: 2.7855945348739626
step: 900 loss_mean: 2.8207008028030396
step: 950 loss_mean: 2.824846935272217
step: 1000 loss_mean: 2.846111936569214
step: 1050 loss_mean: 2.8461344909667967
step: 1100 loss_mean: 2.820168972015381
step: 1150 loss_mean: 2.8806621837615967
step: 1200 loss_mean: 2.817026972770691
step: 1250 loss_mean: 2.8050457239151
step: 1300 loss_mean: 2.8551853990554807
step: 1350 loss_mean: 2.835125741958618
step: 1400 loss_mean: 2.8467388772964477
step: 1450 loss_mean: 2.818729724884033
step: 1500 loss_mean: 2.8722377824783325
step: 1550 loss_mean: 2.8500479221343995
step: 1600 loss_mean: 2.8481312704086306
step: 1650 loss_mean: 2.8919826459884646
step: 1700 loss_mean: 2.7781068086624146
step: 1750 loss_mean: 2.8096353101730345
step: 1800 loss_mean: 2.787328209877014
step: 1850 loss_mean: 2.8405283308029174
step: 1900 loss_mean: 2.8478465557098387
step: 1950 loss_mean: 2.8682660913467406
step: 2000 loss_mean: 2.7795210886001587
step: 2050 loss_mean: 2.7798691892623903
step: 2100 loss_mean: 2.7935357236862184
step: 2150 loss_mean: 2.809121494293213
step: 2200 loss_mean: 2.8515214681625367
step: 2250 loss_mean: 2.7677303552627563
step: 2300 loss_mean: 2.789530510902405
step: 2350 loss_mean: 2.8239803409576414
step: 2400 loss_mean: 2.8075143098831177
step: 2450 loss_mean: 2.85948260307312
step: 2500 loss_mean: 2.8087410974502562
step: 2550 loss_mean: 2.8036331510543824
step: 2600 loss_mean: 2.8398303985595703
step: 2650 loss_mean: 2.8165538263320924
step: 2700 loss_mean: 2.820928678512573
step: 2750 loss_mean: 2.825925250053406
step: 2800 loss_mean: 2.7999462604522707
step: 2850 loss_mean: 2.767302255630493
step: 2900 loss_mean: 2.7710141229629515
step: 2950 loss_mean: 2.837070932388306
step: 3000 loss_mean: 2.818533544540405
step: 3050 loss_mean: 2.8619579696655273
step: 3100 loss_mean: 2.809875178337097
step: 50 loss_mean: 2.8228716468811035
step: 100 loss_mean: 2.87018536567688
step: 150 loss_mean: 2.794750633239746
step: 200 loss_mean: 2.9016978311538697
step: 250 loss_mean: 2.825889730453491
step: 300 loss_mean: 2.900530481338501
step: 350 loss_mean: 2.768626484870911
step: 400 loss_mean: 2.8235728406906127
step: 450 loss_mean: 2.8041631174087525
step: 500 loss_mean: 2.843571271896362
step: 550 loss_mean: 2.8312676477432253
step: 600 loss_mean: 2.7682674074172975
Epoch: 27 | Run time: 623.0 s | Train loss: 2.82 | Valid loss: 2.83
Saved checkpoint 27 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0421 s.
run_epoch
step: 50 loss_mean: 2.7961732578277587
step: 100 loss_mean: 2.7972761011123657
step: 150 loss_mean: 2.803574213981628
step: 200 loss_mean: 2.8530565309524536
step: 250 loss_mean: 2.8202379941940308
step: 300 loss_mean: 2.8304808855056764
step: 350 loss_mean: 2.7751479864120485
step: 400 loss_mean: 2.828931531906128
step: 450 loss_mean: 2.8022783184051514
step: 500 loss_mean: 2.8612408590316774
step: 550 loss_mean: 2.851696367263794
step: 600 loss_mean: 2.832820734977722
step: 650 loss_mean: 2.8368828678131104
step: 700 loss_mean: 2.823982858657837
step: 750 loss_mean: 2.8060122394561766
step: 800 loss_mean: 2.797190260887146
step: 850 loss_mean: 2.796698799133301
step: 900 loss_mean: 2.8076771306991577
step: 950 loss_mean: 2.781715126037598
step: 1000 loss_mean: 2.7976345729827883
step: 1050 loss_mean: 2.8225058460235597
step: 1100 loss_mean: 2.7789838027954104
step: 1150 loss_mean: 2.837697215080261
step: 1200 loss_mean: 2.8390842628479005
step: 1250 loss_mean: 2.7769376611709595
step: 1300 loss_mean: 2.7980734729766845
step: 1350 loss_mean: 2.788738112449646
step: 1400 loss_mean: 2.84904664516449
step: 1450 loss_mean: 2.835973496437073
step: 1500 loss_mean: 2.852757248878479
step: 1550 loss_mean: 2.8191036224365233
step: 1600 loss_mean: 2.7983737802505493
step: 1650 loss_mean: 2.824837117195129
step: 1700 loss_mean: 2.8105564641952516
step: 1750 loss_mean: 2.8098983716964723
step: 1800 loss_mean: 2.838637599945068
step: 1850 loss_mean: 2.8610565662384033
step: 1900 loss_mean: 2.8363399076461793
step: 1950 loss_mean: 2.8520386743545534
step: 2000 loss_mean: 2.8343756103515627
step: 2050 loss_mean: 2.754855585098267
step: 2100 loss_mean: 2.855764379501343
step: 2150 loss_mean: 2.875951018333435
step: 2200 loss_mean: 2.837514572143555
step: 2250 loss_mean: 2.7831827926635744
step: 2300 loss_mean: 2.7705825042724608
step: 2350 loss_mean: 2.7990330696105956
step: 2400 loss_mean: 2.82594162940979
step: 2450 loss_mean: 2.815313105583191
step: 2500 loss_mean: 2.8006874418258665
step: 2550 loss_mean: 2.820758104324341
step: 2600 loss_mean: 2.7911211538314817
step: 2650 loss_mean: 2.8459877967834473
step: 2700 loss_mean: 2.8344243431091307
step: 2750 loss_mean: 2.869805393218994
step: 2800 loss_mean: 2.773377637863159
step: 2850 loss_mean: 2.825943260192871
step: 2900 loss_mean: 2.8125377464294434
step: 2950 loss_mean: 2.8042207956314087
step: 3000 loss_mean: 2.7930426359176637
step: 3050 loss_mean: 2.8258873128890993
step: 3100 loss_mean: 2.8666452836990355
step: 50 loss_mean: 2.817410264015198
step: 100 loss_mean: 2.859056091308594
step: 150 loss_mean: 2.7766509008407594
step: 200 loss_mean: 2.91810950756073
step: 250 loss_mean: 2.840360288619995
step: 300 loss_mean: 2.9137136363983154
step: 350 loss_mean: 2.7464161849021913
step: 400 loss_mean: 2.8109789419174196
step: 450 loss_mean: 2.804918999671936
step: 500 loss_mean: 2.836034321784973
step: 550 loss_mean: 2.815777792930603
step: 600 loss_mean: 2.751595163345337
Epoch: 28 | Run time: 624.0 s | Train loss: 2.82 | Valid loss: 2.83
Saved checkpoint 28 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0438 s.
run_epoch
step: 50 loss_mean: 2.83239248752594
step: 100 loss_mean: 2.7906590604782107
step: 150 loss_mean: 2.7877671003341673
step: 200 loss_mean: 2.81193000793457
step: 250 loss_mean: 2.8365952014923095
step: 300 loss_mean: 2.7574473094940184
step: 350 loss_mean: 2.860613670349121
step: 400 loss_mean: 2.843797917366028
step: 450 loss_mean: 2.8677875423431396
step: 500 loss_mean: 2.797531485557556
step: 550 loss_mean: 2.8295213985443115
step: 600 loss_mean: 2.8324919986724852
step: 650 loss_mean: 2.780109791755676
step: 700 loss_mean: 2.8253835821151734
step: 750 loss_mean: 2.8193457984924315
step: 800 loss_mean: 2.787207407951355
step: 850 loss_mean: 2.857875723838806
step: 900 loss_mean: 2.7726444244384765
step: 950 loss_mean: 2.7907349395751955
step: 1000 loss_mean: 2.8218328762054443
step: 1050 loss_mean: 2.827706456184387
step: 1100 loss_mean: 2.7881675863265993
step: 1150 loss_mean: 2.7693516635894775
step: 1200 loss_mean: 2.835455741882324
step: 1250 loss_mean: 2.763889808654785
step: 1300 loss_mean: 2.816060471534729
step: 1350 loss_mean: 2.840664291381836
step: 1400 loss_mean: 2.810591154098511
step: 1450 loss_mean: 2.831429600715637
step: 1500 loss_mean: 2.811444730758667
step: 1550 loss_mean: 2.8481275987625123
step: 1600 loss_mean: 2.8099440431594847
step: 1650 loss_mean: 2.8068670654296874
step: 1700 loss_mean: 2.809201021194458
step: 1750 loss_mean: 2.819413213729858
step: 1800 loss_mean: 2.82895516872406
step: 1850 loss_mean: 2.776463313102722
step: 1900 loss_mean: 2.7959472513198853
step: 1950 loss_mean: 2.8410041999816893
step: 2000 loss_mean: 2.864993758201599
step: 2050 loss_mean: 2.795705347061157
step: 2100 loss_mean: 2.81740686416626
step: 2150 loss_mean: 2.8240565204620363
step: 2200 loss_mean: 2.7775813817977903
step: 2250 loss_mean: 2.8637420034408567
step: 2300 loss_mean: 2.7466636800765993
step: 2350 loss_mean: 2.8392992639541625
step: 2400 loss_mean: 2.874990701675415
step: 2450 loss_mean: 2.805170526504517
step: 2500 loss_mean: 2.8822809743881224
step: 2550 loss_mean: 2.8142544984817506
step: 2600 loss_mean: 2.8544058847427367
step: 2650 loss_mean: 2.804763202667236
step: 2700 loss_mean: 2.805186977386475
step: 2750 loss_mean: 2.810760736465454
step: 2800 loss_mean: 2.835793070793152
step: 2850 loss_mean: 2.8112099075317385
step: 2900 loss_mean: 2.87018488407135
step: 2950 loss_mean: 2.8398268365859987
step: 3000 loss_mean: 2.8190603971481325
step: 3050 loss_mean: 2.8418539905548097
step: 3100 loss_mean: 2.7695614194869993
step: 50 loss_mean: 2.817453870773315
step: 100 loss_mean: 2.8645139741897583
step: 150 loss_mean: 2.7784474515914916
step: 200 loss_mean: 2.8938488006591796
step: 250 loss_mean: 2.8458320331573486
step: 300 loss_mean: 2.9105551767349245
step: 350 loss_mean: 2.76143100976944
step: 400 loss_mean: 2.814519500732422
step: 450 loss_mean: 2.7954142618179323
step: 500 loss_mean: 2.850863814353943
step: 550 loss_mean: 2.8304430198669435
step: 600 loss_mean: 2.7495167779922487
Epoch: 29 | Run time: 623.0 s | Train loss: 2.82 | Valid loss: 2.83
Saved checkpoint 29 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0452 s.
run_epoch
step: 50 loss_mean: 2.8735409021377563
step: 100 loss_mean: 2.780349645614624
step: 150 loss_mean: 2.835390739440918
step: 200 loss_mean: 2.8029523038864137
step: 250 loss_mean: 2.7646529197692873
step: 300 loss_mean: 2.8060221529006957
step: 350 loss_mean: 2.80649197101593
step: 400 loss_mean: 2.8275353717803955
step: 450 loss_mean: 2.714485092163086
step: 500 loss_mean: 2.7946485805511476
step: 550 loss_mean: 2.810066442489624
step: 600 loss_mean: 2.797587504386902
step: 650 loss_mean: 2.793267455101013
step: 700 loss_mean: 2.7968819999694823
step: 750 loss_mean: 2.81933874130249
step: 800 loss_mean: 2.8242812538146973
step: 850 loss_mean: 2.836295084953308
step: 900 loss_mean: 2.8529895782470702
step: 950 loss_mean: 2.78731960773468
step: 1000 loss_mean: 2.842536072731018
step: 1050 loss_mean: 2.8217863416671753
step: 1100 loss_mean: 2.830630412101746
step: 1150 loss_mean: 2.8464147233963013
step: 1200 loss_mean: 2.793851628303528
step: 1250 loss_mean: 2.7902179622650145
step: 1300 loss_mean: 2.8028608179092407
step: 1350 loss_mean: 2.770963363647461
step: 1400 loss_mean: 2.804587278366089
step: 1450 loss_mean: 2.8067047595977783
step: 1500 loss_mean: 2.804606308937073
step: 1550 loss_mean: 2.799094476699829
step: 1600 loss_mean: 2.8024066925048827
step: 1650 loss_mean: 2.848805923461914
step: 1700 loss_mean: 2.8135073709487917
step: 1750 loss_mean: 2.8566042137145997
step: 1800 loss_mean: 2.8086557626724242
step: 1850 loss_mean: 2.794432563781738
step: 1900 loss_mean: 2.7980916595458982
step: 1950 loss_mean: 2.8032856607437133
step: 2000 loss_mean: 2.789690775871277
step: 2050 loss_mean: 2.860841836929321
step: 2100 loss_mean: 2.7702605438232424
step: 2150 loss_mean: 2.8113755655288695
step: 2200 loss_mean: 2.811528525352478
step: 2250 loss_mean: 2.8155810403823853
step: 2300 loss_mean: 2.8322759532928465
step: 2350 loss_mean: 2.789385757446289
step: 2400 loss_mean: 2.8091470289230345
step: 2450 loss_mean: 2.8377429962158205
step: 2500 loss_mean: 2.8597721958160403
step: 2550 loss_mean: 2.8639932823181153
step: 2600 loss_mean: 2.8355879163742066
step: 2650 loss_mean: 2.7927885818481446
step: 2700 loss_mean: 2.7416673040390016
step: 2750 loss_mean: 2.8261241626739504
step: 2800 loss_mean: 2.8572347831726073
step: 2850 loss_mean: 2.798683567047119
step: 2900 loss_mean: 2.790886015892029
step: 2950 loss_mean: 2.805733742713928
step: 3000 loss_mean: 2.835734734535217
step: 3050 loss_mean: 2.842707386016846
step: 3100 loss_mean: 2.782626223564148
step: 50 loss_mean: 2.812009472846985
step: 100 loss_mean: 2.875219883918762
step: 150 loss_mean: 2.7947118854522706
step: 200 loss_mean: 2.916656537055969
step: 250 loss_mean: 2.843306064605713
step: 300 loss_mean: 2.91090841293335
step: 350 loss_mean: 2.7632645535469056
step: 400 loss_mean: 2.8093436336517335
step: 450 loss_mean: 2.8075129699707033
step: 500 loss_mean: 2.82851749420166
step: 550 loss_mean: 2.8340987300872804
step: 600 loss_mean: 2.75971803188324
Epoch: 30 | Run time: 623.0 s | Train loss: 2.81 | Valid loss: 2.83
Saved checkpoint 30 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0465 s.
run_epoch
step: 50 loss_mean: 2.829849696159363
step: 100 loss_mean: 2.812822017669678
step: 150 loss_mean: 2.8033386039733887
step: 200 loss_mean: 2.8136408281326295
step: 250 loss_mean: 2.8210205602645875
step: 300 loss_mean: 2.863866958618164
step: 350 loss_mean: 2.785691990852356
step: 400 loss_mean: 2.8199917221069337
step: 450 loss_mean: 2.783894352912903
step: 500 loss_mean: 2.843815760612488
step: 550 loss_mean: 2.80413330078125
step: 600 loss_mean: 2.8199814224243163
step: 650 loss_mean: 2.8156488800048827
step: 700 loss_mean: 2.849456491470337
step: 750 loss_mean: 2.847215943336487
step: 800 loss_mean: 2.8326866006851197
step: 850 loss_mean: 2.80856264591217
step: 900 loss_mean: 2.7560525846481325
step: 950 loss_mean: 2.777822422981262
step: 1000 loss_mean: 2.795109443664551
step: 1050 loss_mean: 2.8241660594940186
step: 1100 loss_mean: 2.772543306350708
step: 1150 loss_mean: 2.845949821472168
step: 1200 loss_mean: 2.8222535133361815
step: 1250 loss_mean: 2.8446167373657225
step: 1300 loss_mean: 2.78639554977417
step: 1350 loss_mean: 2.817814049720764
step: 1400 loss_mean: 2.842530245780945
step: 1450 loss_mean: 2.7932012939453124
step: 1500 loss_mean: 2.775431628227234
step: 1550 loss_mean: 2.829853067398071
step: 1600 loss_mean: 2.784980001449585
step: 1650 loss_mean: 2.793816237449646
step: 1700 loss_mean: 2.7840433263778688
step: 1750 loss_mean: 2.798440999984741
step: 1800 loss_mean: 2.839672050476074
step: 1850 loss_mean: 2.8291217851638795
step: 1900 loss_mean: 2.82859335899353
step: 1950 loss_mean: 2.79376624584198
step: 2000 loss_mean: 2.7902923679351805
step: 2050 loss_mean: 2.770947675704956
step: 2100 loss_mean: 2.8396297311782837
step: 2150 loss_mean: 2.774549903869629
step: 2200 loss_mean: 2.800584034919739
step: 2250 loss_mean: 2.783577356338501
step: 2300 loss_mean: 2.794702763557434
step: 2350 loss_mean: 2.795484085083008
step: 2400 loss_mean: 2.789456539154053
step: 2450 loss_mean: 2.773072853088379
step: 2500 loss_mean: 2.820365490913391
step: 2550 loss_mean: 2.8671911573410034
step: 2600 loss_mean: 2.8009018421173097
step: 2650 loss_mean: 2.753657078742981
step: 2700 loss_mean: 2.8351323556900025
step: 2750 loss_mean: 2.7786132955551146
step: 2800 loss_mean: 2.830269751548767
step: 2850 loss_mean: 2.7387810707092286
step: 2900 loss_mean: 2.810515341758728
step: 2950 loss_mean: 2.8596652030944822
step: 3000 loss_mean: 2.7991647863388063
step: 3050 loss_mean: 2.7489455270767214
step: 3100 loss_mean: 2.818071360588074
step: 50 loss_mean: 2.851387410163879
step: 100 loss_mean: 2.9073595666885375
step: 150 loss_mean: 2.8151370096206665
step: 200 loss_mean: 2.901960892677307
step: 250 loss_mean: 2.8775933980941772
step: 300 loss_mean: 2.9282020044326784
step: 350 loss_mean: 2.816684446334839
step: 400 loss_mean: 2.872757983207703
step: 450 loss_mean: 2.8131202173233034
step: 500 loss_mean: 2.893466062545776
step: 550 loss_mean: 2.8318137884140016
step: 600 loss_mean: 2.791539306640625
Epoch: 31 | Run time: 625.0 s | Train loss: 2.81 | Valid loss: 2.86
Saved checkpoint 31 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0481 s.
run_epoch
step: 50 loss_mean: 2.759637393951416
step: 100 loss_mean: 2.763359503746033
step: 150 loss_mean: 2.792761244773865
step: 200 loss_mean: 2.83621337890625
step: 250 loss_mean: 2.7983380556106567
step: 300 loss_mean: 2.842338342666626
step: 350 loss_mean: 2.8964863872528075
step: 400 loss_mean: 2.743614010810852
step: 450 loss_mean: 2.7852890300750732
step: 500 loss_mean: 2.7913964414596557
step: 550 loss_mean: 2.836993455886841
step: 600 loss_mean: 2.7929011821746825
step: 650 loss_mean: 2.808143057823181
step: 700 loss_mean: 2.857874450683594
step: 750 loss_mean: 2.7955986499786376
step: 800 loss_mean: 2.8495794105529786
step: 850 loss_mean: 2.781205835342407
step: 900 loss_mean: 2.829801015853882
step: 950 loss_mean: 2.7920057916641237
step: 1000 loss_mean: 2.8256938552856443
step: 1050 loss_mean: 2.817702441215515
step: 1100 loss_mean: 2.8449040460586548
step: 1150 loss_mean: 2.7961788272857664
step: 1200 loss_mean: 2.795991039276123
step: 1250 loss_mean: 2.754349274635315
step: 1300 loss_mean: 2.791818132400513
step: 1350 loss_mean: 2.766350116729736
step: 1400 loss_mean: 2.789592924118042
step: 1450 loss_mean: 2.7716883182525636
step: 1500 loss_mean: 2.872212738990784
step: 1550 loss_mean: 2.831166067123413
step: 1600 loss_mean: 2.7980683422088624
step: 1650 loss_mean: 2.8042828130722044
step: 1700 loss_mean: 2.811308889389038
step: 1750 loss_mean: 2.816592230796814
step: 1800 loss_mean: 2.8104792928695677
step: 1850 loss_mean: 2.8589365673065186
step: 1900 loss_mean: 2.801434235572815
step: 1950 loss_mean: 2.842187180519104
step: 2000 loss_mean: 2.843475465774536
step: 2050 loss_mean: 2.8027218914031984
step: 2100 loss_mean: 2.80619740486145
step: 2150 loss_mean: 2.787021803855896
step: 2200 loss_mean: 2.817064518928528
step: 2250 loss_mean: 2.8290063762664794
step: 2300 loss_mean: 2.8741139221191405
step: 2350 loss_mean: 2.7621204805374147
step: 2400 loss_mean: 2.80305145740509
step: 2450 loss_mean: 2.8184494304656984
step: 2500 loss_mean: 2.7169367408752443
step: 2550 loss_mean: 2.787651925086975
step: 2600 loss_mean: 2.8181482124328614
step: 2650 loss_mean: 2.778757963180542
step: 2700 loss_mean: 2.8028391551971437
step: 2750 loss_mean: 2.7799174785614014
step: 2800 loss_mean: 2.7961227321624755
step: 2850 loss_mean: 2.79032283782959
step: 2900 loss_mean: 2.8304799604415893
step: 2950 loss_mean: 2.813865900039673
step: 3000 loss_mean: 2.8022919845581056
step: 3050 loss_mean: 2.835923833847046
step: 3100 loss_mean: 2.8170343923568724
step: 50 loss_mean: 2.813134708404541
step: 100 loss_mean: 2.8723976469039916
step: 150 loss_mean: 2.764388542175293
step: 200 loss_mean: 2.9082235622406007
step: 250 loss_mean: 2.8291456651687623
step: 300 loss_mean: 2.901857714653015
step: 350 loss_mean: 2.762839226722717
step: 400 loss_mean: 2.8279825401306153
step: 450 loss_mean: 2.7898665952682493
step: 500 loss_mean: 2.8485003900527954
step: 550 loss_mean: 2.834269390106201
step: 600 loss_mean: 2.7510873174667356
Epoch: 32 | Run time: 623.0 s | Train loss: 2.81 | Valid loss: 2.83
Saved checkpoint 32 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0493 s.
run_epoch
step: 50 loss_mean: 2.8258799839019777
step: 100 loss_mean: 2.8258579778671264
step: 150 loss_mean: 2.8093971681594847
step: 200 loss_mean: 2.822169761657715
step: 250 loss_mean: 2.831960606575012
step: 300 loss_mean: 2.8033723926544187
step: 350 loss_mean: 2.7810827541351317
step: 400 loss_mean: 2.8199686098098753
step: 450 loss_mean: 2.807676424980164
step: 500 loss_mean: 2.815137195587158
step: 550 loss_mean: 2.7908882236480714
step: 600 loss_mean: 2.792568144798279
step: 650 loss_mean: 2.8133963108062745
step: 700 loss_mean: 2.778881378173828
step: 750 loss_mean: 2.8453476333618166
step: 800 loss_mean: 2.795142149925232
step: 850 loss_mean: 2.8392979288101197
step: 900 loss_mean: 2.859637932777405
step: 950 loss_mean: 2.81231427192688
step: 1000 loss_mean: 2.804975128173828
step: 1050 loss_mean: 2.872727174758911
step: 1100 loss_mean: 2.7120246648788453
step: 1150 loss_mean: 2.849323377609253
step: 1200 loss_mean: 2.8150988626480102
step: 1250 loss_mean: 2.8469764375686646
step: 1300 loss_mean: 2.766753258705139
step: 1350 loss_mean: 2.8219027757644652
step: 1400 loss_mean: 2.771712703704834
step: 1450 loss_mean: 2.8183870267868043
step: 1500 loss_mean: 2.795793614387512
step: 1550 loss_mean: 2.742146010398865
step: 1600 loss_mean: 2.8166238832473756
step: 1650 loss_mean: 2.810742440223694
step: 1700 loss_mean: 2.716433525085449
step: 1750 loss_mean: 2.7940300607681277
step: 1800 loss_mean: 2.808962106704712
step: 1850 loss_mean: 2.803345966339111
step: 1900 loss_mean: 2.849083924293518
step: 1950 loss_mean: 2.8647960138320925
step: 2000 loss_mean: 2.7401842308044433
step: 2050 loss_mean: 2.8184384870529176
step: 2100 loss_mean: 2.7664132118225098
step: 2150 loss_mean: 2.72880889415741
step: 2200 loss_mean: 2.83616632938385
step: 2250 loss_mean: 2.7629684734344484
step: 2300 loss_mean: 2.7767153072357176
step: 2350 loss_mean: 2.8955153608322144
step: 2400 loss_mean: 2.782444453239441
step: 2450 loss_mean: 2.8066776752471925
step: 2500 loss_mean: 2.752935976982117
step: 2550 loss_mean: 2.785252637863159
step: 2600 loss_mean: 2.8163952445983886
step: 2650 loss_mean: 2.770738248825073
step: 2700 loss_mean: 2.8198741722106933
step: 2750 loss_mean: 2.7756305646896364
step: 2800 loss_mean: 2.8028793144226074
step: 2850 loss_mean: 2.7994746732711793
step: 2900 loss_mean: 2.8576141834259032
step: 2950 loss_mean: 2.7661654806137084
step: 3000 loss_mean: 2.835035424232483
step: 3050 loss_mean: 2.7950221061706544
step: 3100 loss_mean: 2.7341192102432252
step: 50 loss_mean: 2.8360399293899534
step: 100 loss_mean: 2.8845477056503297
step: 150 loss_mean: 2.7889742279052734
step: 200 loss_mean: 2.9023169088363647
step: 250 loss_mean: 2.8511823081970213
step: 300 loss_mean: 2.90910062789917
step: 350 loss_mean: 2.7778336787223816
step: 400 loss_mean: 2.8254704809188844
step: 450 loss_mean: 2.810888605117798
step: 500 loss_mean: 2.855648384094238
step: 550 loss_mean: 2.8200236415863036
step: 600 loss_mean: 2.767358798980713
Epoch: 33 | Run time: 625.0 s | Train loss: 2.8 | Valid loss: 2.84
Saved checkpoint 33 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0508 s.
run_epoch
step: 50 loss_mean: 2.7780926609039307
step: 100 loss_mean: 2.819288282394409
step: 150 loss_mean: 2.78568904876709
step: 200 loss_mean: 2.815586595535278
step: 250 loss_mean: 2.775422606468201
step: 300 loss_mean: 2.7509911680221557
step: 350 loss_mean: 2.799391465187073
step: 400 loss_mean: 2.83170524597168
step: 450 loss_mean: 2.805467100143433
step: 500 loss_mean: 2.800568561553955
step: 550 loss_mean: 2.8500403547286988
step: 600 loss_mean: 2.797026205062866
step: 650 loss_mean: 2.7717934894561767
step: 700 loss_mean: 2.840040626525879
step: 750 loss_mean: 2.8321319341659548
step: 800 loss_mean: 2.81896897315979
step: 850 loss_mean: 2.7952657079696657
step: 900 loss_mean: 2.7841929149627687
step: 950 loss_mean: 2.8453106451034547
step: 1000 loss_mean: 2.8285538959503174
step: 1050 loss_mean: 2.7925075101852417
step: 1100 loss_mean: 2.834389805793762
step: 1150 loss_mean: 2.832541055679321
step: 1200 loss_mean: 2.780047492980957
step: 1250 loss_mean: 2.8053238821029662
step: 1300 loss_mean: 2.845603380203247
step: 1350 loss_mean: 2.88909565448761
step: 1400 loss_mean: 2.813674602508545
step: 1450 loss_mean: 2.7847694110870362
step: 1500 loss_mean: 2.79550434589386
step: 1550 loss_mean: 2.858936448097229
step: 1600 loss_mean: 2.785421619415283
step: 1650 loss_mean: 2.7790286779403686
step: 1700 loss_mean: 2.8216065740585328
step: 1750 loss_mean: 2.873989520072937
step: 1800 loss_mean: 2.8084050464630126
step: 1850 loss_mean: 2.807050223350525
step: 1900 loss_mean: 2.8267720890045167
step: 1950 loss_mean: 2.7773935556411744
step: 2000 loss_mean: 2.740006866455078
step: 2050 loss_mean: 2.7910405874252318
step: 2100 loss_mean: 2.7741207838058473
step: 2150 loss_mean: 2.7687719440460206
step: 2200 loss_mean: 2.8466070175170897
step: 2250 loss_mean: 2.7284483766555785
step: 2300 loss_mean: 2.673277997970581
step: 2350 loss_mean: 2.819775075912476
step: 2400 loss_mean: 2.7626086139678954
step: 2450 loss_mean: 2.778467016220093
step: 2500 loss_mean: 2.751790986061096
step: 2550 loss_mean: 2.795567111968994
step: 2600 loss_mean: 2.7530360412597656
step: 2650 loss_mean: 2.828342432975769
step: 2700 loss_mean: 2.8129034662246704
step: 2750 loss_mean: 2.822914695739746
step: 2800 loss_mean: 2.784139723777771
step: 2850 loss_mean: 2.824732794761658
step: 2900 loss_mean: 2.810750422477722
step: 2950 loss_mean: 2.811778202056885
step: 3000 loss_mean: 2.8068248462677
step: 3050 loss_mean: 2.7413364124298094
step: 3100 loss_mean: 2.741984276771545
step: 50 loss_mean: 2.8423005056381228
step: 100 loss_mean: 2.9044552135467527
step: 150 loss_mean: 2.8070995378494263
step: 200 loss_mean: 2.9081369972229005
step: 250 loss_mean: 2.835767903327942
step: 300 loss_mean: 2.927212357521057
step: 350 loss_mean: 2.799240732192993
step: 400 loss_mean: 2.847983732223511
step: 450 loss_mean: 2.823291940689087
step: 500 loss_mean: 2.8861420583724975
step: 550 loss_mean: 2.830819387435913
step: 600 loss_mean: 2.779330825805664
Epoch: 34 | Run time: 624.0 s | Train loss: 2.8 | Valid loss: 2.85
Saved checkpoint 34 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0523 s.
run_epoch
step: 50 loss_mean: 2.794813413619995
step: 100 loss_mean: 2.7545206642150877
step: 150 loss_mean: 2.8122911930084227
step: 200 loss_mean: 2.8247688817977905
step: 250 loss_mean: 2.7790112781524656
step: 300 loss_mean: 2.7501387214660644
step: 350 loss_mean: 2.717771887779236
step: 400 loss_mean: 2.7821975708007813
step: 450 loss_mean: 2.7221519899368287
step: 500 loss_mean: 2.76554181098938
step: 550 loss_mean: 2.871915283203125
step: 600 loss_mean: 2.7821053409576417
step: 650 loss_mean: 2.779004273414612
step: 700 loss_mean: 2.809195203781128
step: 750 loss_mean: 2.77659215927124
step: 800 loss_mean: 2.825591206550598
step: 850 loss_mean: 2.8308515882492067
step: 900 loss_mean: 2.7760509490966796
step: 950 loss_mean: 2.843177070617676
step: 1000 loss_mean: 2.7869451236724854
step: 1050 loss_mean: 2.8006548643112184
step: 1100 loss_mean: 2.7721205949783325
step: 1150 loss_mean: 2.826239814758301
step: 1200 loss_mean: 2.789111852645874
step: 1250 loss_mean: 2.842621169090271
step: 1300 loss_mean: 2.8210531949996946
step: 1350 loss_mean: 2.813424892425537
step: 1400 loss_mean: 2.7547096681594847
step: 1450 loss_mean: 2.8557411527633665
step: 1500 loss_mean: 2.8199774074554442
step: 1550 loss_mean: 2.778311023712158
step: 1600 loss_mean: 2.782722544670105
step: 1650 loss_mean: 2.735412712097168
step: 1700 loss_mean: 2.8270547151565553
step: 1750 loss_mean: 2.7787331533432007
step: 1800 loss_mean: 2.7307984256744384
step: 1850 loss_mean: 2.847285227775574
step: 1900 loss_mean: 2.8366943120956423
step: 1950 loss_mean: 2.8259767293930054
step: 2000 loss_mean: 2.735882453918457
step: 2050 loss_mean: 2.7850942373275758
step: 2100 loss_mean: 2.836300940513611
step: 2150 loss_mean: 2.832668128013611
step: 2200 loss_mean: 2.817078347206116
step: 2250 loss_mean: 2.780855655670166
step: 2300 loss_mean: 2.807950859069824
step: 2350 loss_mean: 2.805697212219238
step: 2400 loss_mean: 2.7813144016265867
step: 2450 loss_mean: 2.808748822212219
step: 2500 loss_mean: 2.7662109088897706
step: 2550 loss_mean: 2.7713420534133912
step: 2600 loss_mean: 2.809837293624878
step: 2650 loss_mean: 2.79616605758667
step: 2700 loss_mean: 2.722594909667969
step: 2750 loss_mean: 2.83896568775177
step: 2800 loss_mean: 2.8219706916809084
step: 2850 loss_mean: 2.8449467611312866
step: 2900 loss_mean: 2.792660045623779
step: 2950 loss_mean: 2.8068736171722413
step: 3000 loss_mean: 2.7480089902877807
step: 3050 loss_mean: 2.819732246398926
step: 3100 loss_mean: 2.785689468383789
step: 50 loss_mean: 2.7959154987335206
step: 100 loss_mean: 2.8376765966415407
step: 150 loss_mean: 2.758009281158447
step: 200 loss_mean: 2.8699691581726072
step: 250 loss_mean: 2.8045366382598877
step: 300 loss_mean: 2.888126378059387
step: 350 loss_mean: 2.739960889816284
step: 400 loss_mean: 2.786710820198059
step: 450 loss_mean: 2.7694522285461427
step: 500 loss_mean: 2.8274765634536743
step: 550 loss_mean: 2.809434762001038
step: 600 loss_mean: 2.720461812019348
Epoch: 35 | Run time: 616.0 s | Train loss: 2.8 | Valid loss: 2.8
Saved checkpoint 35 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0535 s.
run_epoch
step: 50 loss_mean: 2.791342272758484
step: 100 loss_mean: 2.753008918762207
step: 150 loss_mean: 2.8234842300415037
step: 200 loss_mean: 2.7554649114608765
step: 250 loss_mean: 2.7452553272247315
step: 300 loss_mean: 2.729984006881714
step: 350 loss_mean: 2.7221763849258425
step: 400 loss_mean: 2.8281780338287352
step: 450 loss_mean: 2.7899646520614625
step: 500 loss_mean: 2.8360997915267943
step: 550 loss_mean: 2.7771645259857176
step: 600 loss_mean: 2.775534782409668
step: 650 loss_mean: 2.87274631023407
step: 700 loss_mean: 2.740817971229553
step: 750 loss_mean: 2.7953527116775514
step: 800 loss_mean: 2.7455703973770142
step: 850 loss_mean: 2.7981207370758057
step: 900 loss_mean: 2.7906601858139037
step: 950 loss_mean: 2.819486985206604
step: 1000 loss_mean: 2.7911100244522093
step: 1050 loss_mean: 2.750667634010315
step: 1100 loss_mean: 2.818424344062805
step: 1150 loss_mean: 2.809827241897583
step: 1200 loss_mean: 2.794151439666748
step: 1250 loss_mean: 2.8792505931854246
step: 1300 loss_mean: 2.833019618988037
step: 1350 loss_mean: 2.78501202583313
step: 1400 loss_mean: 2.7967495107650757
step: 1450 loss_mean: 2.8290570116043092
step: 1500 loss_mean: 2.8018263149261475
step: 1550 loss_mean: 2.7989340877532958
step: 1600 loss_mean: 2.757256007194519
step: 1650 loss_mean: 2.713545632362366
step: 1700 loss_mean: 2.8171304416656495
step: 1750 loss_mean: 2.802146244049072
step: 1800 loss_mean: 2.809778342247009
step: 1850 loss_mean: 2.8015027713775633
step: 1900 loss_mean: 2.849605903625488
step: 1950 loss_mean: 2.797598247528076
step: 2000 loss_mean: 2.793510904312134
step: 2050 loss_mean: 2.748977918624878
step: 2100 loss_mean: 2.7949767780303953
step: 2150 loss_mean: 2.8192547750473023
step: 2200 loss_mean: 2.7803290987014773
step: 2250 loss_mean: 2.775884051322937
step: 2300 loss_mean: 2.805956244468689
step: 2350 loss_mean: 2.8045878553390504
step: 2400 loss_mean: 2.813397831916809
step: 2450 loss_mean: 2.8288228797912596
step: 2500 loss_mean: 2.8087910747528078
step: 2550 loss_mean: 2.8043548345565794
step: 2600 loss_mean: 2.708693017959595
step: 2650 loss_mean: 2.83096164226532
step: 2700 loss_mean: 2.7873014211654663
step: 2750 loss_mean: 2.80119393825531
step: 2800 loss_mean: 2.8367669200897216
step: 2850 loss_mean: 2.764818468093872
step: 2900 loss_mean: 2.7754981327056885
step: 2950 loss_mean: 2.8377134227752685
step: 3000 loss_mean: 2.746814641952515
step: 3050 loss_mean: 2.788177890777588
step: 3100 loss_mean: 2.8206582117080687
step: 50 loss_mean: 2.8100516605377197
step: 100 loss_mean: 2.8597022819519045
step: 150 loss_mean: 2.7735053491592407
step: 200 loss_mean: 2.875952501296997
step: 250 loss_mean: 2.8111008310317995
step: 300 loss_mean: 2.8799837923049925
step: 350 loss_mean: 2.7633749771118166
step: 400 loss_mean: 2.8311720418930055
step: 450 loss_mean: 2.7913365268707278
step: 500 loss_mean: 2.8369296264648436
step: 550 loss_mean: 2.78651171207428
step: 600 loss_mean: 2.756054744720459
Epoch: 36 | Run time: 620.0 s | Train loss: 2.79 | Valid loss: 2.82
Saved checkpoint 36 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0555 s.
run_epoch
step: 50 loss_mean: 2.7755810260772704
step: 100 loss_mean: 2.7899987077713013
step: 150 loss_mean: 2.7636839532852173
step: 200 loss_mean: 2.777324261665344
step: 250 loss_mean: 2.8100205135345457
step: 300 loss_mean: 2.7727032041549684
step: 350 loss_mean: 2.801800742149353
step: 400 loss_mean: 2.7261277627944946
step: 450 loss_mean: 2.7462424993515016
step: 500 loss_mean: 2.7660770654678344
step: 550 loss_mean: 2.8347828340530397
step: 600 loss_mean: 2.7879931783676146
step: 650 loss_mean: 2.7881778955459593
step: 700 loss_mean: 2.7808876276016234
step: 750 loss_mean: 2.7753110837936403
step: 800 loss_mean: 2.7958675241470337
step: 850 loss_mean: 2.778287796974182
step: 900 loss_mean: 2.80956045627594
step: 950 loss_mean: 2.8158723306655884
step: 1000 loss_mean: 2.771353492736816
step: 1050 loss_mean: 2.7966978931427002
step: 1100 loss_mean: 2.7440836572647096
step: 1150 loss_mean: 2.8163546180725096
step: 1200 loss_mean: 2.803488688468933
step: 1250 loss_mean: 2.8173101854324343
step: 1300 loss_mean: 2.7999981355667116
step: 1350 loss_mean: 2.7898933839797975
step: 1400 loss_mean: 2.7431951427459715
step: 1450 loss_mean: 2.780388412475586
step: 1500 loss_mean: 2.7733438348770143
step: 1550 loss_mean: 2.8405949592590334
step: 1600 loss_mean: 2.8057773447036745
step: 1650 loss_mean: 2.7807154178619387
step: 1700 loss_mean: 2.79409068107605
step: 1750 loss_mean: 2.785125560760498
step: 1800 loss_mean: 2.7963178873062136
step: 1850 loss_mean: 2.806765604019165
step: 1900 loss_mean: 2.780221128463745
step: 1950 loss_mean: 2.7472481107711793
step: 2000 loss_mean: 2.7410478019714355
step: 2050 loss_mean: 2.7550531959533693
step: 2100 loss_mean: 2.80310640335083
step: 2150 loss_mean: 2.7841643142700194
step: 2200 loss_mean: 2.870018997192383
step: 2250 loss_mean: 2.844961824417114
step: 2300 loss_mean: 2.8130193948745728
step: 2350 loss_mean: 2.8317660808563234
step: 2400 loss_mean: 2.8206922960281373
step: 2450 loss_mean: 2.820228018760681
step: 2500 loss_mean: 2.8044903469085694
step: 2550 loss_mean: 2.7952846002578737
step: 2600 loss_mean: 2.816793451309204
step: 2650 loss_mean: 2.8109075450897216
step: 2700 loss_mean: 2.771170401573181
step: 2750 loss_mean: 2.760103216171265
step: 2800 loss_mean: 2.767977924346924
step: 2850 loss_mean: 2.8155693292617796
step: 2900 loss_mean: 2.781155433654785
step: 2950 loss_mean: 2.8101896476745605
step: 3000 loss_mean: 2.742345952987671
step: 3050 loss_mean: 2.774062547683716
step: 3100 loss_mean: 2.805186667442322
step: 50 loss_mean: 2.7893960523605346
step: 100 loss_mean: 2.823091149330139
step: 150 loss_mean: 2.7497137880325315
step: 200 loss_mean: 2.86844286441803
step: 250 loss_mean: 2.8108702421188356
step: 300 loss_mean: 2.8693953561782837
step: 350 loss_mean: 2.728394432067871
step: 400 loss_mean: 2.7878902530670167
step: 450 loss_mean: 2.7540118408203127
step: 500 loss_mean: 2.8153773498535157
step: 550 loss_mean: 2.778401131629944
step: 600 loss_mean: 2.7252464485168457
Epoch: 37 | Run time: 623.0 s | Train loss: 2.79 | Valid loss: 2.79
Saved checkpoint 37 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0566 s.
run_epoch
step: 50 loss_mean: 2.7504121255874634
step: 100 loss_mean: 2.8980104684829713
step: 150 loss_mean: 2.806916742324829
step: 200 loss_mean: 2.8372683048248293
step: 250 loss_mean: 2.7886204385757445
step: 300 loss_mean: 2.7656682395935057
step: 350 loss_mean: 2.7897774314880373
step: 400 loss_mean: 2.81959753036499
step: 450 loss_mean: 2.762145128250122
step: 500 loss_mean: 2.8336943197250366
step: 550 loss_mean: 2.800139141082764
step: 600 loss_mean: 2.795680685043335
step: 650 loss_mean: 2.7784472417831423
step: 700 loss_mean: 2.765898427963257
step: 750 loss_mean: 2.799307150840759
step: 800 loss_mean: 2.735090050697327
step: 850 loss_mean: 2.7816233110427855
step: 900 loss_mean: 2.8063494968414306
step: 950 loss_mean: 2.7308047103881834
step: 1000 loss_mean: 2.7985340213775634
step: 1050 loss_mean: 2.76258665561676
step: 1100 loss_mean: 2.814399394989014
step: 1150 loss_mean: 2.7962674856185914
step: 1200 loss_mean: 2.780655469894409
step: 1250 loss_mean: 2.8240459060668943
step: 1300 loss_mean: 2.788125901222229
step: 1350 loss_mean: 2.7820969676971434
step: 1400 loss_mean: 2.7832427740097048
step: 1450 loss_mean: 2.7466618967056275
step: 1500 loss_mean: 2.773193664550781
step: 1550 loss_mean: 2.7888288259506226
step: 1600 loss_mean: 2.8262879467010498
step: 1650 loss_mean: 2.8331075716018677
step: 1700 loss_mean: 2.7464505577087404
step: 1750 loss_mean: 2.7954249429702758
step: 1800 loss_mean: 2.818076968193054
step: 1850 loss_mean: 2.7579450273513793
step: 1900 loss_mean: 2.795442123413086
step: 1950 loss_mean: 2.801742687225342
step: 2000 loss_mean: 2.76771532535553
step: 2050 loss_mean: 2.802313070297241
step: 2100 loss_mean: 2.826433606147766
step: 2150 loss_mean: 2.7928877162933348
step: 2200 loss_mean: 2.766347599029541
step: 2250 loss_mean: 2.8127262783050537
step: 2300 loss_mean: 2.798297128677368
step: 2350 loss_mean: 2.754106149673462
step: 2400 loss_mean: 2.7461759853363037
step: 2450 loss_mean: 2.7557709646224975
step: 2500 loss_mean: 2.784752411842346
step: 2550 loss_mean: 2.7694649505615234
step: 2600 loss_mean: 2.7710537576675414
step: 2650 loss_mean: 2.8013881492614745
step: 2700 loss_mean: 2.793998098373413
step: 2750 loss_mean: 2.802756052017212
step: 2800 loss_mean: 2.7682497549057006
step: 2850 loss_mean: 2.7773583936691284
step: 2900 loss_mean: 2.8254961013793944
step: 2950 loss_mean: 2.785961561203003
step: 3000 loss_mean: 2.7697669458389282
step: 3050 loss_mean: 2.7826876878738402
step: 3100 loss_mean: 2.78095055103302
step: 50 loss_mean: 2.8028724193573
step: 100 loss_mean: 2.8464287996292112
step: 150 loss_mean: 2.7856911039352417
step: 200 loss_mean: 2.898917350769043
step: 250 loss_mean: 2.811793351173401
step: 300 loss_mean: 2.892272539138794
step: 350 loss_mean: 2.7417948198318483
step: 400 loss_mean: 2.8195066928863524
step: 450 loss_mean: 2.787871856689453
step: 500 loss_mean: 2.833348412513733
step: 550 loss_mean: 2.8219680309295656
step: 600 loss_mean: 2.7444387245178223
Epoch: 38 | Run time: 625.0 s | Train loss: 2.79 | Valid loss: 2.82
Saved checkpoint 38 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0580 s.
run_epoch
step: 50 loss_mean: 2.7770419359207152
step: 100 loss_mean: 2.773606843948364
step: 150 loss_mean: 2.7656618928909302
step: 200 loss_mean: 2.7808700609207153
step: 250 loss_mean: 2.82017032623291
step: 300 loss_mean: 2.7372488498687746
step: 350 loss_mean: 2.764029006958008
step: 400 loss_mean: 2.817163586616516
step: 450 loss_mean: 2.791647782325745
step: 500 loss_mean: 2.7829667806625364
step: 550 loss_mean: 2.778780474662781
step: 600 loss_mean: 2.7880425024032593
step: 650 loss_mean: 2.7787947463989258
step: 700 loss_mean: 2.8020091581344606
step: 750 loss_mean: 2.795348916053772
step: 800 loss_mean: 2.807348442077637
step: 850 loss_mean: 2.7809838533401487
step: 900 loss_mean: 2.825930829048157
step: 950 loss_mean: 2.8444346141815187
step: 1000 loss_mean: 2.8023038482666016
step: 1050 loss_mean: 2.7171928882598877
step: 1100 loss_mean: 2.7677289390563966
step: 1150 loss_mean: 2.8144841241836547
step: 1200 loss_mean: 2.7577695178985597
step: 1250 loss_mean: 2.823429822921753
step: 1300 loss_mean: 2.780990815162659
step: 1350 loss_mean: 2.7458466482162476
step: 1400 loss_mean: 2.788194398880005
step: 1450 loss_mean: 2.7338545083999635
step: 1500 loss_mean: 2.811423211097717
step: 1550 loss_mean: 2.8012269258499147
step: 1600 loss_mean: 2.7066484451293946
step: 1650 loss_mean: 2.7999723529815674
step: 1700 loss_mean: 2.7910508728027343
step: 1750 loss_mean: 2.7827244138717653
step: 1800 loss_mean: 2.8192439889907837
step: 1850 loss_mean: 2.8164824056625366
step: 1900 loss_mean: 2.7715782356262206
step: 1950 loss_mean: 2.790684976577759
step: 2000 loss_mean: 2.791164445877075
step: 2050 loss_mean: 2.826235213279724
step: 2100 loss_mean: 2.7650012445449828
step: 2150 loss_mean: 2.771621279716492
step: 2200 loss_mean: 2.8484733629226686
step: 2250 loss_mean: 2.7886792278289794
step: 2300 loss_mean: 2.8159282064437865
step: 2350 loss_mean: 2.7814018964767455
step: 2400 loss_mean: 2.8137563848495484
step: 2450 loss_mean: 2.778375296592712
step: 2500 loss_mean: 2.8334654664993284
step: 2550 loss_mean: 2.766095232963562
step: 2600 loss_mean: 2.7800205993652343
step: 2650 loss_mean: 2.8102807712554934
step: 2700 loss_mean: 2.825228180885315
step: 2750 loss_mean: 2.8025785160064696
step: 2800 loss_mean: 2.7549400520324707
step: 2850 loss_mean: 2.803893122673035
step: 2900 loss_mean: 2.742166361808777
step: 2950 loss_mean: 2.7510411977767943
step: 3000 loss_mean: 2.793544955253601
step: 3050 loss_mean: 2.7619004535675047
step: 3100 loss_mean: 2.8184875440597534
step: 50 loss_mean: 2.7793724822998045
step: 100 loss_mean: 2.825926785469055
step: 150 loss_mean: 2.750723719596863
step: 200 loss_mean: 2.889812932014465
step: 250 loss_mean: 2.8042696952819823
step: 300 loss_mean: 2.8763108015060426
step: 350 loss_mean: 2.73476509809494
step: 400 loss_mean: 2.773738579750061
step: 450 loss_mean: 2.7578630304336547
step: 500 loss_mean: 2.826057724952698
step: 550 loss_mean: 2.809393072128296
step: 600 loss_mean: 2.714273509979248
Epoch: 39 | Run time: 621.0 s | Train loss: 2.79 | Valid loss: 2.8
Saved checkpoint 39 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0593 s.
run_epoch
step: 50 loss_mean: 2.815555257797241
step: 100 loss_mean: 2.768628101348877
step: 150 loss_mean: 2.8267050552368165
step: 200 loss_mean: 2.773642258644104
step: 250 loss_mean: 2.7777840518951415
step: 300 loss_mean: 2.7812639331817626
step: 350 loss_mean: 2.8320336866378786
step: 400 loss_mean: 2.790489525794983
step: 450 loss_mean: 2.767098388671875
step: 500 loss_mean: 2.7201710987091063
step: 550 loss_mean: 2.7949585485458375
step: 600 loss_mean: 2.8246043872833253
step: 650 loss_mean: 2.8162916469573975
step: 700 loss_mean: 2.7514413166046143
step: 750 loss_mean: 2.754704270362854
step: 800 loss_mean: 2.761193118095398
step: 850 loss_mean: 2.8134976959228517
step: 900 loss_mean: 2.738078923225403
step: 950 loss_mean: 2.774978132247925
step: 1000 loss_mean: 2.746938400268555
step: 1050 loss_mean: 2.7993845462799074
step: 1100 loss_mean: 2.8484671115875244
step: 1150 loss_mean: 2.7510981702804567
step: 1200 loss_mean: 2.7795099020004272
step: 1250 loss_mean: 2.7944580268859864
step: 1300 loss_mean: 2.793842797279358
step: 1350 loss_mean: 2.7654913854599
step: 1400 loss_mean: 2.7969011878967285
step: 1450 loss_mean: 2.7577020168304442
step: 1500 loss_mean: 2.8140421295166016
step: 1550 loss_mean: 2.7819540023803713
step: 1600 loss_mean: 2.743905167579651
step: 1650 loss_mean: 2.7725862216949464
step: 1700 loss_mean: 2.8219733333587644
step: 1750 loss_mean: 2.7757501888275145
step: 1800 loss_mean: 2.784439067840576
step: 1850 loss_mean: 2.7956084966659547
step: 1900 loss_mean: 2.75647780418396
step: 1950 loss_mean: 2.7673271179199217
step: 2000 loss_mean: 2.7875107860565187
step: 2050 loss_mean: 2.8078768062591553
step: 2100 loss_mean: 2.727837333679199
step: 2150 loss_mean: 2.8521161556243895
step: 2200 loss_mean: 2.790639009475708
step: 2250 loss_mean: 2.7711581420898437
step: 2300 loss_mean: 2.7860971689224243
step: 2350 loss_mean: 2.754992275238037
step: 2400 loss_mean: 2.783951144218445
step: 2450 loss_mean: 2.698397135734558
step: 2500 loss_mean: 2.732111778259277
step: 2550 loss_mean: 2.768922758102417
step: 2600 loss_mean: 2.7684284543991087
step: 2650 loss_mean: 2.7788993644714357
step: 2700 loss_mean: 2.860656762123108
step: 2750 loss_mean: 2.7990751171112063
step: 2800 loss_mean: 2.8345123672485353
step: 2850 loss_mean: 2.772738742828369
step: 2900 loss_mean: 2.787718653678894
step: 2950 loss_mean: 2.7484686851501463
step: 3000 loss_mean: 2.7510988664627076
step: 3050 loss_mean: 2.7648024463653567
step: 3100 loss_mean: 2.7825394010543825
step: 50 loss_mean: 2.789792423248291
step: 100 loss_mean: 2.823296241760254
step: 150 loss_mean: 2.7411371183395388
step: 200 loss_mean: 2.8671325206756593
step: 250 loss_mean: 2.8159911918640135
step: 300 loss_mean: 2.8766029596328737
step: 350 loss_mean: 2.7316056585311888
step: 400 loss_mean: 2.7640087032318115
step: 450 loss_mean: 2.7630504035949706
step: 500 loss_mean: 2.8203847074508666
step: 550 loss_mean: 2.795737738609314
step: 600 loss_mean: 2.730378246307373
Epoch: 40 | Run time: 621.0 s | Train loss: 2.78 | Valid loss: 2.79
Saved checkpoint 40 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0608 s.
run_epoch
step: 50 loss_mean: 2.7851402044296263
step: 100 loss_mean: 2.7241854953765867
step: 150 loss_mean: 2.7790767478942873
step: 200 loss_mean: 2.7941900444030763
step: 250 loss_mean: 2.795450739860535
step: 300 loss_mean: 2.8005189514160156
step: 350 loss_mean: 2.8386277627944945
step: 400 loss_mean: 2.7723086786270144
step: 450 loss_mean: 2.7449050998687743
step: 500 loss_mean: 2.7467336559295656
step: 550 loss_mean: 2.7904110813140868
step: 600 loss_mean: 2.769231743812561
step: 650 loss_mean: 2.7721527576446534
step: 700 loss_mean: 2.8035049200057984
step: 750 loss_mean: 2.8320784521102906
step: 800 loss_mean: 2.7261451387405398
step: 850 loss_mean: 2.7286923789978026
step: 900 loss_mean: 2.7879273748397826
step: 950 loss_mean: 2.783996205329895
step: 1000 loss_mean: 2.7730424451828
step: 1050 loss_mean: 2.7728303480148315
step: 1100 loss_mean: 2.7967698431015013
step: 1150 loss_mean: 2.802727737426758
step: 1200 loss_mean: 2.7976154565811155
step: 1250 loss_mean: 2.7518911170959472
step: 1300 loss_mean: 2.782030875682831
step: 1350 loss_mean: 2.77022310256958
step: 1400 loss_mean: 2.78199059009552
step: 1450 loss_mean: 2.7276470470428467
step: 1500 loss_mean: 2.793094811439514
step: 1550 loss_mean: 2.7836291694641115
step: 1600 loss_mean: 2.764474229812622
step: 1650 loss_mean: 2.73582067489624
step: 1700 loss_mean: 2.776143832206726
step: 1750 loss_mean: 2.828524031639099
step: 1800 loss_mean: 2.811929030418396
step: 1850 loss_mean: 2.771009817123413
step: 1900 loss_mean: 2.7621066522598268
step: 1950 loss_mean: 2.819342222213745
step: 2000 loss_mean: 2.7819152212142946
step: 2050 loss_mean: 2.7248795509338377
step: 2100 loss_mean: 2.8048911333084106
step: 2150 loss_mean: 2.7680248165130616
step: 2200 loss_mean: 2.791292972564697
step: 2250 loss_mean: 2.787430739402771
step: 2300 loss_mean: 2.7952676725387575
step: 2350 loss_mean: 2.82802435874939
step: 2400 loss_mean: 2.784063377380371
step: 2450 loss_mean: 2.7681725883483885
step: 2500 loss_mean: 2.75154257774353
step: 2550 loss_mean: 2.7647517633438112
step: 2600 loss_mean: 2.7899678993225097
step: 2650 loss_mean: 2.806332221031189
step: 2700 loss_mean: 2.798224477767944
step: 2750 loss_mean: 2.7700231170654295
step: 2800 loss_mean: 2.806552972793579
step: 2850 loss_mean: 2.780014462471008
step: 2900 loss_mean: 2.7598460006713865
step: 2950 loss_mean: 2.827974238395691
step: 3000 loss_mean: 2.8253141689300536
step: 3050 loss_mean: 2.804388961791992
step: 3100 loss_mean: 2.7886157131195066
step: 50 loss_mean: 2.794729514122009
step: 100 loss_mean: 2.849188370704651
step: 150 loss_mean: 2.7527100896835326
step: 200 loss_mean: 2.8991817331314085
step: 250 loss_mean: 2.8116997194290163
step: 300 loss_mean: 2.9014764070510863
step: 350 loss_mean: 2.7297698616981507
step: 400 loss_mean: 2.786679439544678
step: 450 loss_mean: 2.778327078819275
step: 500 loss_mean: 2.838987765312195
step: 550 loss_mean: 2.826569595336914
step: 600 loss_mean: 2.7221399545669556
Epoch: 41 | Run time: 624.0 s | Train loss: 2.78 | Valid loss: 2.81
Saved checkpoint 41 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0622 s.
run_epoch
step: 50 loss_mean: 2.7848094511032104
step: 100 loss_mean: 2.8168432474136353
step: 150 loss_mean: 2.7613040447235107
step: 200 loss_mean: 2.789450454711914
step: 250 loss_mean: 2.773896379470825
step: 300 loss_mean: 2.7971901655197144
step: 350 loss_mean: 2.783055791854858
step: 400 loss_mean: 2.765528635978699
step: 450 loss_mean: 2.8198446464538574
step: 500 loss_mean: 2.7439068508148194
step: 550 loss_mean: 2.732902388572693
step: 600 loss_mean: 2.7648753929138183
step: 650 loss_mean: 2.7605115747451783
step: 700 loss_mean: 2.744163408279419
step: 750 loss_mean: 2.7800341749191286
step: 800 loss_mean: 2.724409432411194
step: 850 loss_mean: 2.7488408851623536
step: 900 loss_mean: 2.7736621046066285
step: 950 loss_mean: 2.755817666053772
step: 1000 loss_mean: 2.8396238613128664
step: 1050 loss_mean: 2.7271313285827636
step: 1100 loss_mean: 2.777683539390564
step: 1150 loss_mean: 2.7775750970840454
step: 1200 loss_mean: 2.7777520179748536
step: 1250 loss_mean: 2.7424912071228027
step: 1300 loss_mean: 2.798127770423889
step: 1350 loss_mean: 2.7752237558364867
step: 1400 loss_mean: 2.7798982858657837
step: 1450 loss_mean: 2.77046133518219
step: 1500 loss_mean: 2.762810564041138
step: 1550 loss_mean: 2.7253180170059204
step: 1600 loss_mean: 2.7603309106826783
step: 1650 loss_mean: 2.755256161689758
step: 1700 loss_mean: 2.775216417312622
step: 1750 loss_mean: 2.735719976425171
step: 1800 loss_mean: 2.8346130228042603
step: 1850 loss_mean: 2.7809434509277344
step: 1900 loss_mean: 2.8369831943511965
step: 1950 loss_mean: 2.7829306983947752
step: 2000 loss_mean: 2.8277854776382445
step: 2050 loss_mean: 2.825076675415039
step: 2100 loss_mean: 2.7897353601455688
step: 2150 loss_mean: 2.8008574724197386
step: 2200 loss_mean: 2.762698917388916
step: 2250 loss_mean: 2.7607649612426757
step: 2300 loss_mean: 2.8147822284698485
step: 2350 loss_mean: 2.6682564067840575
step: 2400 loss_mean: 2.756764793395996
step: 2450 loss_mean: 2.745478320121765
step: 2500 loss_mean: 2.670847806930542
step: 2550 loss_mean: 2.7934594917297364
step: 2600 loss_mean: 2.855035467147827
step: 2650 loss_mean: 2.774076671600342
step: 2700 loss_mean: 2.7723284006118774
step: 2750 loss_mean: 2.79633873462677
step: 2800 loss_mean: 2.7679237842559816
step: 2850 loss_mean: 2.8340684843063353
step: 2900 loss_mean: 2.74544620513916
step: 2950 loss_mean: 2.7830630779266357
step: 3000 loss_mean: 2.8151804542541505
step: 3050 loss_mean: 2.798465313911438
step: 3100 loss_mean: 2.777598752975464
step: 50 loss_mean: 2.8024643230438233
step: 100 loss_mean: 2.8521424388885497
step: 150 loss_mean: 2.768119468688965
step: 200 loss_mean: 2.8942587900161745
step: 250 loss_mean: 2.8134450721740722
step: 300 loss_mean: 2.909512372016907
step: 350 loss_mean: 2.751428027153015
step: 400 loss_mean: 2.773958911895752
step: 450 loss_mean: 2.8011483764648437
step: 500 loss_mean: 2.8431797552108766
step: 550 loss_mean: 2.8265520238876345
step: 600 loss_mean: 2.741605825424194
Epoch: 42 | Run time: 624.0 s | Train loss: 2.78 | Valid loss: 2.82
Saved checkpoint 42 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0630 s.
run_epoch
step: 50 loss_mean: 2.759043960571289
step: 100 loss_mean: 2.721746954917908
step: 150 loss_mean: 2.742641124725342
step: 200 loss_mean: 2.822045297622681
step: 250 loss_mean: 2.790380163192749
step: 300 loss_mean: 2.7642800331115724
step: 350 loss_mean: 2.7568659019470214
step: 400 loss_mean: 2.7948162508010865
step: 450 loss_mean: 2.7635935592651366
step: 500 loss_mean: 2.800255370140076
step: 550 loss_mean: 2.7479022693634034
step: 600 loss_mean: 2.755248579978943
step: 650 loss_mean: 2.8096800374984743
step: 700 loss_mean: 2.771820778846741
step: 750 loss_mean: 2.724954767227173
step: 800 loss_mean: 2.7720306396484373
step: 850 loss_mean: 2.809244251251221
step: 900 loss_mean: 2.7359906101226805
step: 950 loss_mean: 2.794786324501038
step: 1000 loss_mean: 2.7562622690200804
step: 1050 loss_mean: 2.7486618518829347
step: 1100 loss_mean: 2.784990892410278
step: 1150 loss_mean: 2.7677509212493896
step: 1200 loss_mean: 2.8047653341293337
step: 1250 loss_mean: 2.7795423746109007
step: 1300 loss_mean: 2.7760480308532713
step: 1350 loss_mean: 2.778340253829956
step: 1400 loss_mean: 2.7512934398651123
step: 1450 loss_mean: 2.8176541328430176
step: 1500 loss_mean: 2.7581773376464844
step: 1550 loss_mean: 2.7427851819992064
step: 1600 loss_mean: 2.777798819541931
step: 1650 loss_mean: 2.784953064918518
step: 1700 loss_mean: 2.816831121444702
step: 1750 loss_mean: 2.7677511835098265
step: 1800 loss_mean: 2.746762585639954
step: 1850 loss_mean: 2.792605538368225
step: 1900 loss_mean: 2.8460845804214476
step: 1950 loss_mean: 2.8106331205368043
step: 2000 loss_mean: 2.7970019483566286
step: 2050 loss_mean: 2.775384478569031
step: 2100 loss_mean: 2.7593952941894533
step: 2150 loss_mean: 2.7899722814559937
step: 2200 loss_mean: 2.801885986328125
step: 2250 loss_mean: 2.8568803215026857
step: 2300 loss_mean: 2.716302671432495
step: 2350 loss_mean: 2.7990001392364503
step: 2400 loss_mean: 2.775228638648987
step: 2450 loss_mean: 2.743401608467102
step: 2500 loss_mean: 2.808541259765625
step: 2550 loss_mean: 2.7446785306930543
step: 2600 loss_mean: 2.7685378265380858
step: 2650 loss_mean: 2.7671998739242554
step: 2700 loss_mean: 2.776291790008545
step: 2750 loss_mean: 2.8111425638198853
step: 2800 loss_mean: 2.7379562282562255
step: 2850 loss_mean: 2.7882481622695923
step: 2900 loss_mean: 2.7705944061279295
step: 2950 loss_mean: 2.7577121925354002
step: 3000 loss_mean: 2.7829692649841307
step: 3050 loss_mean: 2.735399074554443
step: 3100 loss_mean: 2.8076043033599856
step: 50 loss_mean: 2.775107002258301
step: 100 loss_mean: 2.820016374588013
step: 150 loss_mean: 2.747512950897217
step: 200 loss_mean: 2.8799303817749022
step: 250 loss_mean: 2.792112879753113
step: 300 loss_mean: 2.867034068107605
step: 350 loss_mean: 2.733871138095856
step: 400 loss_mean: 2.7718800783157347
step: 450 loss_mean: 2.7555466079711914
step: 500 loss_mean: 2.806594967842102
step: 550 loss_mean: 2.7798183441162108
step: 600 loss_mean: 2.705330185890198
Epoch: 43 | Run time: 624.0 s | Train loss: 2.78 | Valid loss: 2.79
Saved checkpoint 43 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0647 s.
run_epoch
step: 50 loss_mean: 2.811271324157715
step: 100 loss_mean: 2.8218375778198244
step: 150 loss_mean: 2.7817635345458984
step: 200 loss_mean: 2.7853012895584106
step: 250 loss_mean: 2.742067379951477
step: 300 loss_mean: 2.7911220026016235
step: 350 loss_mean: 2.747433876991272
step: 400 loss_mean: 2.705696773529053
step: 450 loss_mean: 2.804282236099243
step: 500 loss_mean: 2.80065363407135
step: 550 loss_mean: 2.8004570245742797
step: 600 loss_mean: 2.8179027032852173
step: 650 loss_mean: 2.796108269691467
step: 700 loss_mean: 2.8057225847244265
step: 750 loss_mean: 2.746994285583496
step: 800 loss_mean: 2.8160832500457764
step: 850 loss_mean: 2.784297022819519
step: 900 loss_mean: 2.7763937425613405
step: 950 loss_mean: 2.7654907941818236
step: 1000 loss_mean: 2.7755158233642576
step: 1050 loss_mean: 2.7629980754852297
step: 1100 loss_mean: 2.7280334043502807
step: 1150 loss_mean: 2.7440410375595095
step: 1200 loss_mean: 2.782453861236572
step: 1250 loss_mean: 2.7399661922454834
step: 1300 loss_mean: 2.792357301712036
step: 1350 loss_mean: 2.7379587173461912
step: 1400 loss_mean: 2.736768546104431
step: 1450 loss_mean: 2.7200593328475953
step: 1500 loss_mean: 2.740120792388916
step: 1550 loss_mean: 2.7769289875030516
step: 1600 loss_mean: 2.8129798316955568
step: 1650 loss_mean: 2.7439086723327635
step: 1700 loss_mean: 2.8185821390151977
step: 1750 loss_mean: 2.8002081298828125
step: 1800 loss_mean: 2.722321982383728
step: 1850 loss_mean: 2.8043745660781862
step: 1900 loss_mean: 2.7787780952453613
step: 1950 loss_mean: 2.8106571292877196
step: 2000 loss_mean: 2.8045120668411254
step: 2050 loss_mean: 2.816065912246704
step: 2100 loss_mean: 2.7115139055252073
step: 2150 loss_mean: 2.8042222166061403
step: 2200 loss_mean: 2.8295635890960695
step: 2250 loss_mean: 2.707236909866333
step: 2300 loss_mean: 2.8003104448318483
step: 2350 loss_mean: 2.771285080909729
step: 2400 loss_mean: 2.7740470361709595
step: 2450 loss_mean: 2.7900854110717774
step: 2500 loss_mean: 2.7405396842956544
step: 2550 loss_mean: 2.73999933719635
step: 2600 loss_mean: 2.7363617420196533
step: 2650 loss_mean: 2.7732867288589476
step: 2700 loss_mean: 2.7156653594970703
step: 2750 loss_mean: 2.746895680427551
step: 2800 loss_mean: 2.7655418920516968
step: 2850 loss_mean: 2.7448277044296265
step: 2900 loss_mean: 2.7756218576431273
step: 2950 loss_mean: 2.6947378158569335
step: 3000 loss_mean: 2.777088050842285
step: 3050 loss_mean: 2.7328280544281007
step: 3100 loss_mean: 2.7739629793167113
step: 50 loss_mean: 2.809172024726868
step: 100 loss_mean: 2.857372579574585
step: 150 loss_mean: 2.7661422729492187
step: 200 loss_mean: 2.889784140586853
step: 250 loss_mean: 2.819286198616028
step: 300 loss_mean: 2.9020834302902223
step: 350 loss_mean: 2.7599246883392334
step: 400 loss_mean: 2.7930508756637575
step: 450 loss_mean: 2.7846725606918334
step: 500 loss_mean: 2.832848973274231
step: 550 loss_mean: 2.7794047594070435
step: 600 loss_mean: 2.742639546394348
Epoch: 44 | Run time: 625.0 s | Train loss: 2.77 | Valid loss: 2.81
Saved checkpoint 44 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0660 s.
run_epoch
step: 50 loss_mean: 2.779008889198303
step: 100 loss_mean: 2.783031630516052
step: 150 loss_mean: 2.7128321313858033
step: 200 loss_mean: 2.7636149501800538
step: 250 loss_mean: 2.807999949455261
step: 300 loss_mean: 2.7449173879623414
step: 350 loss_mean: 2.7928809833526613
step: 400 loss_mean: 2.774840440750122
step: 450 loss_mean: 2.775666551589966
step: 500 loss_mean: 2.796887230873108
step: 550 loss_mean: 2.7514373111724852
step: 600 loss_mean: 2.7986474514007567
step: 650 loss_mean: 2.792731523513794
step: 700 loss_mean: 2.7837672233581543
step: 750 loss_mean: 2.7802605724334715
step: 800 loss_mean: 2.771674060821533
step: 850 loss_mean: 2.7407583141326906
step: 900 loss_mean: 2.7273742866516115
step: 950 loss_mean: 2.713918752670288
step: 1000 loss_mean: 2.8018724632263186
step: 1050 loss_mean: 2.7720686483383177
step: 1100 loss_mean: 2.8348568058013917
step: 1150 loss_mean: 2.7750332021713255
step: 1200 loss_mean: 2.765903363227844
step: 1250 loss_mean: 2.7788838577270507
step: 1300 loss_mean: 2.767440505027771
step: 1350 loss_mean: 2.792384343147278
step: 1400 loss_mean: 2.730990948677063
step: 1450 loss_mean: 2.775670223236084
step: 1500 loss_mean: 2.7554846668243407
step: 1550 loss_mean: 2.7812750053405764
step: 1600 loss_mean: 2.7722160053253173
step: 1650 loss_mean: 2.765675177574158
step: 1700 loss_mean: 2.8099018478393556
step: 1750 loss_mean: 2.7762635469436647
step: 1800 loss_mean: 2.7739918422698975
step: 1850 loss_mean: 2.7260215997695925
step: 1900 loss_mean: 2.700005946159363
step: 1950 loss_mean: 2.7762818002700804
step: 2000 loss_mean: 2.7762186765670775
step: 2050 loss_mean: 2.8428877544403077
step: 2100 loss_mean: 2.788172068595886
step: 2150 loss_mean: 2.7665503358840944
step: 2200 loss_mean: 2.7636232709884645
step: 2250 loss_mean: 2.7464721488952635
step: 2300 loss_mean: 2.767117214202881
step: 2350 loss_mean: 2.7729937124252317
step: 2400 loss_mean: 2.747376627922058
step: 2450 loss_mean: 2.754329175949097
step: 2500 loss_mean: 2.798732781410217
step: 2550 loss_mean: 2.7692141103744508
step: 2600 loss_mean: 2.7498232650756838
step: 2650 loss_mean: 2.746072187423706
step: 2700 loss_mean: 2.8177102041244506
step: 2750 loss_mean: 2.774695553779602
step: 2800 loss_mean: 2.7309543323516845
step: 2850 loss_mean: 2.8276263856887818
step: 2900 loss_mean: 2.7970863246917723
step: 2950 loss_mean: 2.74046658039093
step: 3000 loss_mean: 2.8751523542404174
step: 3050 loss_mean: 2.7659522533416747
step: 3100 loss_mean: 2.7876399660110476
step: 50 loss_mean: 2.803978567123413
step: 100 loss_mean: 2.854120988845825
step: 150 loss_mean: 2.7600193214416504
step: 200 loss_mean: 2.8584872007369997
step: 250 loss_mean: 2.8050709676742556
step: 300 loss_mean: 2.884243493080139
step: 350 loss_mean: 2.751379675865173
step: 400 loss_mean: 2.7959532165527343
step: 450 loss_mean: 2.783244004249573
step: 500 loss_mean: 2.812972373962402
step: 550 loss_mean: 2.7806031608581545
step: 600 loss_mean: 2.726927409172058
Epoch: 45 | Run time: 620.0 s | Train loss: 2.77 | Valid loss: 2.8
Saved checkpoint 45 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0678 s.
run_epoch
step: 50 loss_mean: 2.7718912887573244
step: 100 loss_mean: 2.769498209953308
step: 150 loss_mean: 2.785391664505005
step: 200 loss_mean: 2.7269577550888062
step: 250 loss_mean: 2.7861953163146973
step: 300 loss_mean: 2.7107237339019776
step: 350 loss_mean: 2.791901593208313
step: 400 loss_mean: 2.74326895236969
step: 450 loss_mean: 2.698556842803955
step: 500 loss_mean: 2.7608785152435305
step: 550 loss_mean: 2.777839779853821
step: 600 loss_mean: 2.701659698486328
step: 650 loss_mean: 2.7739440393447876
step: 700 loss_mean: 2.732705969810486
step: 750 loss_mean: 2.783410120010376
step: 800 loss_mean: 2.761271619796753
step: 850 loss_mean: 2.7483154249191286
step: 900 loss_mean: 2.8034445428848267
step: 950 loss_mean: 2.7308805894851687
step: 1000 loss_mean: 2.804446096420288
step: 1050 loss_mean: 2.8017217540740966
step: 1100 loss_mean: 2.750406198501587
step: 1150 loss_mean: 2.7945481872558595
step: 1200 loss_mean: 2.7656108808517454
step: 1250 loss_mean: 2.7342764234542845
step: 1300 loss_mean: 2.8023010683059693
step: 1350 loss_mean: 2.7743242025375365
step: 1400 loss_mean: 2.8082006216049193
step: 1450 loss_mean: 2.755933094024658
step: 1500 loss_mean: 2.7306922817230226
step: 1550 loss_mean: 2.763057951927185
step: 1600 loss_mean: 2.7289859580993654
step: 1650 loss_mean: 2.798403606414795
step: 1700 loss_mean: 2.773310241699219
step: 1750 loss_mean: 2.774076051712036
step: 1800 loss_mean: 2.7592835330963137
step: 1850 loss_mean: 2.8046045303344727
step: 1900 loss_mean: 2.751636791229248
step: 1950 loss_mean: 2.787724003791809
step: 2000 loss_mean: 2.766618447303772
step: 2050 loss_mean: 2.7633639764785767
step: 2100 loss_mean: 2.770528268814087
step: 2150 loss_mean: 2.7343474674224852
step: 2200 loss_mean: 2.7543875932693482
step: 2250 loss_mean: 2.776794390678406
step: 2300 loss_mean: 2.8496224164962767
step: 2350 loss_mean: 2.7122425031661987
step: 2400 loss_mean: 2.7548459362983704
step: 2450 loss_mean: 2.7856057119369506
step: 2500 loss_mean: 2.8363334608078
step: 2550 loss_mean: 2.8109514141082763
step: 2600 loss_mean: 2.7904527807235717
step: 2650 loss_mean: 2.788891997337341
step: 2700 loss_mean: 2.76833345413208
step: 2750 loss_mean: 2.7246573162078858
step: 2800 loss_mean: 2.7488807964324953
step: 2850 loss_mean: 2.728953652381897
step: 2900 loss_mean: 2.780838942527771
step: 2950 loss_mean: 2.772840723991394
step: 3000 loss_mean: 2.747577443122864
step: 3050 loss_mean: 2.7830457973480223
step: 3100 loss_mean: 2.8007824182510377
step: 50 loss_mean: 2.7713196420669557
step: 100 loss_mean: 2.8199172163009645
step: 150 loss_mean: 2.7336246299743654
step: 200 loss_mean: 2.8581885719299316
step: 250 loss_mean: 2.793325161933899
step: 300 loss_mean: 2.8692945623397828
step: 350 loss_mean: 2.729715356826782
step: 400 loss_mean: 2.774189205169678
step: 450 loss_mean: 2.7599265241622923
step: 500 loss_mean: 2.812841296195984
step: 550 loss_mean: 2.772991862297058
step: 600 loss_mean: 2.724786763191223
Epoch: 46 | Run time: 623.0 s | Train loss: 2.77 | Valid loss: 2.79
Saved checkpoint 46 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0688 s.
run_epoch
step: 50 loss_mean: 2.7312064075469973
step: 100 loss_mean: 2.77058000087738
step: 150 loss_mean: 2.7134491348266603
step: 200 loss_mean: 2.7540081977844237
step: 250 loss_mean: 2.78786780834198
step: 300 loss_mean: 2.7354152536392213
step: 350 loss_mean: 2.730404677391052
step: 400 loss_mean: 2.70824339389801
step: 450 loss_mean: 2.776342387199402
step: 500 loss_mean: 2.765064182281494
step: 550 loss_mean: 2.7850819540023806
step: 600 loss_mean: 2.778670840263367
step: 650 loss_mean: 2.7110402393341064
step: 700 loss_mean: 2.741090512275696
step: 750 loss_mean: 2.7650076580047607
step: 800 loss_mean: 2.7487971639633177
step: 850 loss_mean: 2.7965823650360107
step: 900 loss_mean: 2.8153318119049073
step: 950 loss_mean: 2.748139419555664
step: 1000 loss_mean: 2.746495666503906
step: 1050 loss_mean: 2.77948016166687
step: 1100 loss_mean: 2.753025507926941
step: 1150 loss_mean: 2.777664232254028
step: 1200 loss_mean: 2.768629746437073
step: 1250 loss_mean: 2.7732476139068605
step: 1300 loss_mean: 2.7299761247634886
step: 1350 loss_mean: 2.8129152822494508
step: 1400 loss_mean: 2.756831769943237
step: 1450 loss_mean: 2.800487403869629
step: 1500 loss_mean: 2.7681441736221313
step: 1550 loss_mean: 2.7365185976028443
step: 1600 loss_mean: 2.7591050624847413
step: 1650 loss_mean: 2.7049214267730712
step: 1700 loss_mean: 2.7457508754730227
step: 1750 loss_mean: 2.8305769395828246
step: 1800 loss_mean: 2.764483027458191
step: 1850 loss_mean: 2.8068300342559813
step: 1900 loss_mean: 2.776120495796204
step: 1950 loss_mean: 2.730195837020874
step: 2000 loss_mean: 2.835458154678345
step: 2050 loss_mean: 2.809424796104431
step: 2100 loss_mean: 2.818881993293762
step: 2150 loss_mean: 2.749368762969971
step: 2200 loss_mean: 2.7515779638290407
step: 2250 loss_mean: 2.7442398977279665
step: 2300 loss_mean: 2.7876178884506224
step: 2350 loss_mean: 2.7796489286422728
step: 2400 loss_mean: 2.7669128608703613
step: 2450 loss_mean: 2.778149576187134
step: 2500 loss_mean: 2.777260684967041
step: 2550 loss_mean: 2.8034450721740725
step: 2600 loss_mean: 2.755877456665039
step: 2650 loss_mean: 2.7744060611724852
step: 2700 loss_mean: 2.767560887336731
step: 2750 loss_mean: 2.7563349914550783
step: 2800 loss_mean: 2.8011673402786257
step: 2850 loss_mean: 2.8257381963729857
step: 2900 loss_mean: 2.7765699338912966
step: 2950 loss_mean: 2.7664597749710085
step: 3000 loss_mean: 2.8508338356018066
step: 3050 loss_mean: 2.7198980140686033
step: 3100 loss_mean: 2.776523456573486
step: 50 loss_mean: 2.7689313220977785
step: 100 loss_mean: 2.8026013278961184
step: 150 loss_mean: 2.7375416707992555
step: 200 loss_mean: 2.8432393789291384
step: 250 loss_mean: 2.781402850151062
step: 300 loss_mean: 2.861418743133545
step: 350 loss_mean: 2.7193871927261353
step: 400 loss_mean: 2.752865252494812
step: 450 loss_mean: 2.752733950614929
step: 500 loss_mean: 2.7935994815826417
step: 550 loss_mean: 2.764298243522644
step: 600 loss_mean: 2.700988435745239
Epoch: 47 | Run time: 623.0 s | Train loss: 2.77 | Valid loss: 2.77
Saved checkpoint 47 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0701 s.
run_epoch
step: 50 loss_mean: 2.790963878631592
step: 100 loss_mean: 2.7773251485824586
step: 150 loss_mean: 2.7026244688034056
step: 200 loss_mean: 2.750649242401123
step: 250 loss_mean: 2.768067193031311
step: 300 loss_mean: 2.7680355167388915
step: 350 loss_mean: 2.798754358291626
step: 400 loss_mean: 2.768294167518616
step: 450 loss_mean: 2.7289977836608887
step: 500 loss_mean: 2.7979640054702757
step: 550 loss_mean: 2.85674177646637
step: 600 loss_mean: 2.738260941505432
step: 650 loss_mean: 2.810381989479065
step: 700 loss_mean: 2.738212561607361
step: 750 loss_mean: 2.7453810787200927
step: 800 loss_mean: 2.794760699272156
step: 850 loss_mean: 2.754796543121338
step: 900 loss_mean: 2.7755615091323853
step: 950 loss_mean: 2.7269092798233032
step: 1000 loss_mean: 2.7317911100387575
step: 1050 loss_mean: 2.7788062953948973
step: 1100 loss_mean: 2.758810839653015
step: 1150 loss_mean: 2.7774830293655395
step: 1200 loss_mean: 2.7547599363327024
step: 1250 loss_mean: 2.7907439661026
step: 1300 loss_mean: 2.717365345954895
step: 1350 loss_mean: 2.7673399305343627
step: 1400 loss_mean: 2.7675297260284424
step: 1450 loss_mean: 2.7375286197662354
step: 1500 loss_mean: 2.729650411605835
step: 1550 loss_mean: 2.7137051725387575
step: 1600 loss_mean: 2.801876382827759
step: 1650 loss_mean: 2.7528919792175293
step: 1700 loss_mean: 2.774454674720764
step: 1750 loss_mean: 2.736333703994751
step: 1800 loss_mean: 2.7607633018493654
step: 1850 loss_mean: 2.726107954978943
step: 1900 loss_mean: 2.824109754562378
step: 1950 loss_mean: 2.8054314041137696
step: 2000 loss_mean: 2.740963158607483
step: 2050 loss_mean: 2.7221161556243896
step: 2100 loss_mean: 2.7016242122650147
step: 2150 loss_mean: 2.7372478723526
step: 2200 loss_mean: 2.7578660440444946
step: 2250 loss_mean: 2.7686585807800292
step: 2300 loss_mean: 2.7411718463897703
step: 2350 loss_mean: 2.808678412437439
step: 2400 loss_mean: 2.752727007865906
step: 2450 loss_mean: 2.7594347620010375
step: 2500 loss_mean: 2.8053056621551513
step: 2550 loss_mean: 2.798141140937805
step: 2600 loss_mean: 2.8073233795166015
step: 2650 loss_mean: 2.7707446432113647
step: 2700 loss_mean: 2.7197483348846436
step: 2750 loss_mean: 2.7398724937438965
step: 2800 loss_mean: 2.746333832740784
step: 2850 loss_mean: 2.792013955116272
step: 2900 loss_mean: 2.7461572217941286
step: 2950 loss_mean: 2.7644907999038697
step: 3000 loss_mean: 2.778188457489014
step: 3050 loss_mean: 2.834609794616699
step: 3100 loss_mean: 2.827182159423828
step: 50 loss_mean: 2.7955019760131834
step: 100 loss_mean: 2.827039637565613
step: 150 loss_mean: 2.7417076873779296
step: 200 loss_mean: 2.8670655584335325
step: 250 loss_mean: 2.78291805267334
step: 300 loss_mean: 2.890218653678894
step: 350 loss_mean: 2.7428361797332763
step: 400 loss_mean: 2.790578103065491
step: 450 loss_mean: 2.758459277153015
step: 500 loss_mean: 2.8230025720596315
step: 550 loss_mean: 2.781441626548767
step: 600 loss_mean: 2.7313738870620727
Epoch: 48 | Run time: 623.0 s | Train loss: 2.76 | Valid loss: 2.8
Saved checkpoint 48 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0721 s.
run_epoch
step: 50 loss_mean: 2.721133408546448
step: 100 loss_mean: 2.7549756956100464
step: 150 loss_mean: 2.8584639024734497
step: 200 loss_mean: 2.7259302520751953
step: 250 loss_mean: 2.7273908281326293
step: 300 loss_mean: 2.807394585609436
step: 350 loss_mean: 2.8461798810958863
step: 400 loss_mean: 2.726402134895325
step: 450 loss_mean: 2.792513575553894
step: 500 loss_mean: 2.7435962533950806
step: 550 loss_mean: 2.7380360984802246
step: 600 loss_mean: 2.8068348217010497
step: 650 loss_mean: 2.7708088541030884
step: 700 loss_mean: 2.7262993097305297
step: 750 loss_mean: 2.7699895095825195
step: 800 loss_mean: 2.785185694694519
step: 850 loss_mean: 2.842666916847229
step: 900 loss_mean: 2.742651467323303
step: 950 loss_mean: 2.75556836605072
step: 1000 loss_mean: 2.7071633100509644
step: 1050 loss_mean: 2.7686626625061037
step: 1100 loss_mean: 2.7474358177185056
step: 1150 loss_mean: 2.7718105840682985
step: 1200 loss_mean: 2.6952316331863404
step: 1250 loss_mean: 2.732813444137573
step: 1300 loss_mean: 2.7469366884231565
step: 1350 loss_mean: 2.729211935997009
step: 1400 loss_mean: 2.778984317779541
step: 1450 loss_mean: 2.78939911365509
step: 1500 loss_mean: 2.773817677497864
step: 1550 loss_mean: 2.752230100631714
step: 1600 loss_mean: 2.7922805023193358
step: 1650 loss_mean: 2.7669531440734865
step: 1700 loss_mean: 2.7708576202392576
step: 1750 loss_mean: 2.7148334646224974
step: 1800 loss_mean: 2.76643310546875
step: 1850 loss_mean: 2.7783972454071044
step: 1900 loss_mean: 2.69396550655365
step: 1950 loss_mean: 2.7519468307495116
step: 2000 loss_mean: 2.710643491744995
step: 2050 loss_mean: 2.7768712568283083
step: 2100 loss_mean: 2.7491439628601073
step: 2150 loss_mean: 2.7488587474823
step: 2200 loss_mean: 2.7941831016540526
step: 2250 loss_mean: 2.760169186592102
step: 2300 loss_mean: 2.783693389892578
step: 2350 loss_mean: 2.7254493331909178
step: 2400 loss_mean: 2.7751152610778806
step: 2450 loss_mean: 2.7849672269821166
step: 2500 loss_mean: 2.774357433319092
step: 2550 loss_mean: 2.798440351486206
step: 2600 loss_mean: 2.8069032049179077
step: 2650 loss_mean: 2.788032283782959
step: 2700 loss_mean: 2.780033984184265
step: 2750 loss_mean: 2.739005126953125
step: 2800 loss_mean: 2.7258517217636107
step: 2850 loss_mean: 2.8213417768478393
step: 2900 loss_mean: 2.7257828950881957
step: 2950 loss_mean: 2.7439872217178345
step: 3000 loss_mean: 2.7639349842071534
step: 3050 loss_mean: 2.7708768367767336
step: 3100 loss_mean: 2.748278193473816
step: 50 loss_mean: 2.7717001628875733
step: 100 loss_mean: 2.801991639137268
step: 150 loss_mean: 2.737985529899597
step: 200 loss_mean: 2.842294692993164
step: 250 loss_mean: 2.7922957372665405
step: 300 loss_mean: 2.8556517505645753
step: 350 loss_mean: 2.735507650375366
step: 400 loss_mean: 2.7518438291549683
step: 450 loss_mean: 2.7592740631103516
step: 500 loss_mean: 2.7977521276474
step: 550 loss_mean: 2.765792956352234
step: 600 loss_mean: 2.706213827133179
Epoch: 49 | Run time: 620.0 s | Train loss: 2.76 | Valid loss: 2.78
Saved checkpoint 49 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0733 s.
run_epoch
step: 50 loss_mean: 2.7702056884765627
step: 100 loss_mean: 2.778933072090149
step: 150 loss_mean: 2.7817874145507813
step: 200 loss_mean: 2.799694709777832
step: 250 loss_mean: 2.7404345989227297
step: 300 loss_mean: 2.848545160293579
step: 350 loss_mean: 2.7545657205581664
step: 400 loss_mean: 2.7262529277801515
step: 450 loss_mean: 2.704614996910095
step: 500 loss_mean: 2.753155617713928
step: 550 loss_mean: 2.8026950407028197
step: 600 loss_mean: 2.792142777442932
step: 650 loss_mean: 2.730636510848999
step: 700 loss_mean: 2.7847033405303954
step: 750 loss_mean: 2.7664264631271362
step: 800 loss_mean: 2.750319676399231
step: 850 loss_mean: 2.754973502159119
step: 900 loss_mean: 2.7957855033874512
step: 950 loss_mean: 2.6984573078155516
step: 1000 loss_mean: 2.741091446876526
step: 1050 loss_mean: 2.75908308506012
step: 1100 loss_mean: 2.779921326637268
step: 1150 loss_mean: 2.7782510709762573
step: 1200 loss_mean: 2.7418164157867433
step: 1250 loss_mean: 2.739593505859375
step: 1300 loss_mean: 2.8079188013076783
step: 1350 loss_mean: 2.7909813261032106
step: 1400 loss_mean: 2.7279679346084595
step: 1450 loss_mean: 2.7179563665390014
step: 1500 loss_mean: 2.8370871639251707
step: 1550 loss_mean: 2.8004625129699705
step: 1600 loss_mean: 2.7490624475479124
step: 1650 loss_mean: 2.8147142505645752
step: 1700 loss_mean: 2.737817854881287
step: 1750 loss_mean: 2.7207402038574218
step: 1800 loss_mean: 2.7396637630462646
step: 1850 loss_mean: 2.7316120147705076
step: 1900 loss_mean: 2.671474504470825
step: 1950 loss_mean: 2.7691376972198487
step: 2000 loss_mean: 2.774790892601013
step: 2050 loss_mean: 2.8096129274368287
step: 2100 loss_mean: 2.6962340021133424
step: 2150 loss_mean: 2.760903868675232
step: 2200 loss_mean: 2.74101881980896
step: 2250 loss_mean: 2.7759949827194212
step: 2300 loss_mean: 2.820692386627197
step: 2350 loss_mean: 2.7441434192657472
step: 2400 loss_mean: 2.705315546989441
step: 2450 loss_mean: 2.722289762496948
step: 2500 loss_mean: 2.770884561538696
step: 2550 loss_mean: 2.74018039226532
step: 2600 loss_mean: 2.7356683111190794
step: 2650 loss_mean: 2.757458028793335
step: 2700 loss_mean: 2.788427166938782
step: 2750 loss_mean: 2.758994388580322
step: 2800 loss_mean: 2.7937819242477415
step: 2850 loss_mean: 2.7681648397445677
step: 2900 loss_mean: 2.779399161338806
step: 2950 loss_mean: 2.7950133752822874
step: 3000 loss_mean: 2.758619031906128
step: 3050 loss_mean: 2.726378393173218
step: 3100 loss_mean: 2.7047956371307373
step: 50 loss_mean: 2.770297102928162
step: 100 loss_mean: 2.798098201751709
step: 150 loss_mean: 2.7455037879943847
step: 200 loss_mean: 2.8492305421829225
step: 250 loss_mean: 2.778129291534424
step: 300 loss_mean: 2.8624349355697634
step: 350 loss_mean: 2.7350220251083375
step: 400 loss_mean: 2.7568802642822265
step: 450 loss_mean: 2.746188268661499
step: 500 loss_mean: 2.785299234390259
step: 550 loss_mean: 2.7577514934539793
step: 600 loss_mean: 2.6944817161560057
Epoch: 50 | Run time: 623.0 s | Train loss: 2.76 | Valid loss: 2.77
Saved checkpoint 50 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0751 s.
run_epoch
step: 50 loss_mean: 2.7825426149368284
step: 100 loss_mean: 2.7686505651474
step: 150 loss_mean: 2.8506147718429564
step: 200 loss_mean: 2.773351273536682
step: 250 loss_mean: 2.7457476902008056
step: 300 loss_mean: 2.7704429483413695
step: 350 loss_mean: 2.7289304399490355
step: 400 loss_mean: 2.7779778480529784
step: 450 loss_mean: 2.826927900314331
step: 500 loss_mean: 2.766653733253479
step: 550 loss_mean: 2.7766671228408812
step: 600 loss_mean: 2.77175226688385
step: 650 loss_mean: 2.736320147514343
step: 700 loss_mean: 2.743546714782715
step: 750 loss_mean: 2.7368069744110106
step: 800 loss_mean: 2.7879077291488645
step: 850 loss_mean: 2.726522455215454
step: 900 loss_mean: 2.768657183647156
step: 950 loss_mean: 2.728513011932373
step: 1000 loss_mean: 2.7518055295944213
step: 1050 loss_mean: 2.7700230407714845
step: 1100 loss_mean: 2.7943582248687746
step: 1150 loss_mean: 2.784538073539734
step: 1200 loss_mean: 2.722418775558472
step: 1250 loss_mean: 2.7535317087173463
step: 1300 loss_mean: 2.7636695957183837
step: 1350 loss_mean: 2.723408784866333
step: 1400 loss_mean: 2.7593947505950926
step: 1450 loss_mean: 2.780856924057007
step: 1500 loss_mean: 2.7400636529922484
step: 1550 loss_mean: 2.728784594535828
step: 1600 loss_mean: 2.7065155792236326
step: 1650 loss_mean: 2.7647524547576903
step: 1700 loss_mean: 2.7583656406402586
step: 1750 loss_mean: 2.7946019887924196
step: 1800 loss_mean: 2.745561385154724
step: 1850 loss_mean: 2.7695069885253907
step: 1900 loss_mean: 2.7552931451797487
step: 1950 loss_mean: 2.7257718992233277
step: 2000 loss_mean: 2.753394227027893
step: 2050 loss_mean: 2.7845025300979613
step: 2100 loss_mean: 2.764849371910095
step: 2150 loss_mean: 2.746585431098938
step: 2200 loss_mean: 2.6791308975219725
step: 2250 loss_mean: 2.7825679874420164
step: 2300 loss_mean: 2.708951392173767
step: 2350 loss_mean: 2.7093746948242186
step: 2400 loss_mean: 2.761558256149292
step: 2450 loss_mean: 2.7445545530319215
step: 2500 loss_mean: 2.7525965070724485
step: 2550 loss_mean: 2.7760363626480102
step: 2600 loss_mean: 2.738806791305542
step: 2650 loss_mean: 2.7728912734985354
step: 2700 loss_mean: 2.725926203727722
step: 2750 loss_mean: 2.854386959075928
step: 2800 loss_mean: 2.7286215591430665
step: 2850 loss_mean: 2.756911053657532
step: 2900 loss_mean: 2.7775579833984376
step: 2950 loss_mean: 2.7100285291671753
step: 3000 loss_mean: 2.8032716703414917
step: 3050 loss_mean: 2.7311490201950073
step: 3100 loss_mean: 2.7213212633132935
step: 50 loss_mean: 2.759613766670227
step: 100 loss_mean: 2.7918051290512085
step: 150 loss_mean: 2.7312581443786623
step: 200 loss_mean: 2.835684118270874
step: 250 loss_mean: 2.759947347640991
step: 300 loss_mean: 2.856378583908081
step: 350 loss_mean: 2.7177599382400515
step: 400 loss_mean: 2.73695821762085
step: 450 loss_mean: 2.740380072593689
step: 500 loss_mean: 2.787409439086914
step: 550 loss_mean: 2.7591845846176146
step: 600 loss_mean: 2.6871883344650267
Epoch: 51 | Run time: 625.0 s | Train loss: 2.76 | Valid loss: 2.76
Saved checkpoint 51 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0786 s.
run_epoch
step: 50 loss_mean: 2.7726204681396482
step: 100 loss_mean: 2.7100550317764283
step: 150 loss_mean: 2.776547417640686
step: 200 loss_mean: 2.7383403301239015
step: 250 loss_mean: 2.741021127700806
step: 300 loss_mean: 2.7754354238510133
step: 350 loss_mean: 2.7682046842575074
step: 400 loss_mean: 2.70896201133728
step: 450 loss_mean: 2.7743392181396485
step: 500 loss_mean: 2.6840798902511596
step: 550 loss_mean: 2.758553771972656
step: 600 loss_mean: 2.7114964580535887
step: 650 loss_mean: 2.7662871980667116
step: 700 loss_mean: 2.75216139793396
step: 750 loss_mean: 2.765573935508728
step: 800 loss_mean: 2.739697823524475
step: 850 loss_mean: 2.7914662981033325
step: 900 loss_mean: 2.8019075536727907
step: 950 loss_mean: 2.7216784715652467
step: 1000 loss_mean: 2.7462839603424074
step: 1050 loss_mean: 2.734259457588196
step: 1100 loss_mean: 2.7562955904006956
step: 1150 loss_mean: 2.72023154258728
step: 1200 loss_mean: 2.801079440116882
step: 1250 loss_mean: 2.784395384788513
step: 1300 loss_mean: 2.7678665781021117
step: 1350 loss_mean: 2.8245449113845824
step: 1400 loss_mean: 2.779528093338013
step: 1450 loss_mean: 2.735958070755005
step: 1500 loss_mean: 2.7299036026000976
step: 1550 loss_mean: 2.757216010093689
step: 1600 loss_mean: 2.7092784643173218
step: 1650 loss_mean: 2.7296997690200806
step: 1700 loss_mean: 2.7781475019454955
step: 1750 loss_mean: 2.7547635650634765
step: 1800 loss_mean: 2.7646294927597044
step: 1850 loss_mean: 2.717428741455078
step: 1900 loss_mean: 2.808937678337097
step: 1950 loss_mean: 2.756617980003357
step: 2000 loss_mean: 2.748175778388977
step: 2050 loss_mean: 2.7782424211502077
step: 2100 loss_mean: 2.7200878858566284
step: 2150 loss_mean: 2.7409417533874514
step: 2200 loss_mean: 2.7822644472122193
step: 2250 loss_mean: 2.8160187578201294
step: 2300 loss_mean: 2.7489550161361693
step: 2350 loss_mean: 2.7102853155136106
step: 2400 loss_mean: 2.788383626937866
step: 2450 loss_mean: 2.7866318607330323
step: 2500 loss_mean: 2.739783229827881
step: 2550 loss_mean: 2.746687912940979
step: 2600 loss_mean: 2.7705248594284058
step: 2650 loss_mean: 2.7593986320495607
step: 2700 loss_mean: 2.7547013902664186
step: 2750 loss_mean: 2.7786388111114504
step: 2800 loss_mean: 2.754297766685486
step: 2850 loss_mean: 2.7253351497650145
step: 2900 loss_mean: 2.7462947368621826
step: 2950 loss_mean: 2.7621073961257934
step: 3000 loss_mean: 2.7655993413925173
step: 3050 loss_mean: 2.7456413888931275
step: 3100 loss_mean: 2.753554706573486
step: 50 loss_mean: 2.763471384048462
step: 100 loss_mean: 2.7988036108016967
step: 150 loss_mean: 2.75172646522522
step: 200 loss_mean: 2.848257346153259
step: 250 loss_mean: 2.776815662384033
step: 300 loss_mean: 2.850447473526001
step: 350 loss_mean: 2.744044723510742
step: 400 loss_mean: 2.7630570268630983
step: 450 loss_mean: 2.7496540641784666
step: 500 loss_mean: 2.7899607276916503
step: 550 loss_mean: 2.765054535865784
step: 600 loss_mean: 2.71171103477478
Epoch: 52 | Run time: 619.0 s | Train loss: 2.76 | Valid loss: 2.78
Saved checkpoint 52 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0776 s.
run_epoch
step: 50 loss_mean: 2.7513044595718386
step: 100 loss_mean: 2.75476761341095
step: 150 loss_mean: 2.742844967842102
step: 200 loss_mean: 2.751596655845642
step: 250 loss_mean: 2.7514972162246703
step: 300 loss_mean: 2.759312620162964
step: 350 loss_mean: 2.8006383562088013
step: 400 loss_mean: 2.7368953943252565
step: 450 loss_mean: 2.8344369459152223
step: 500 loss_mean: 2.750499200820923
step: 550 loss_mean: 2.7647956371307374
step: 600 loss_mean: 2.7806810617446898
step: 650 loss_mean: 2.7766752576828004
step: 700 loss_mean: 2.749183135032654
step: 750 loss_mean: 2.763868598937988
step: 800 loss_mean: 2.725382342338562
step: 850 loss_mean: 2.7873063564300535
step: 900 loss_mean: 2.757557497024536
step: 950 loss_mean: 2.796471357345581
step: 1000 loss_mean: 2.692041001319885
step: 1050 loss_mean: 2.741851077079773
step: 1100 loss_mean: 2.7533662796020506
step: 1150 loss_mean: 2.759609317779541
step: 1200 loss_mean: 2.736645793914795
step: 1250 loss_mean: 2.777263870239258
step: 1300 loss_mean: 2.7732559728622435
step: 1350 loss_mean: 2.7491847610473634
step: 1400 loss_mean: 2.7048632383346556
step: 1450 loss_mean: 2.746009211540222
step: 1500 loss_mean: 2.7008339595794677
step: 1550 loss_mean: 2.745616021156311
step: 1600 loss_mean: 2.7245203638076783
step: 1650 loss_mean: 2.7328168869018556
step: 1700 loss_mean: 2.718187108039856
step: 1750 loss_mean: 2.7393741369247437
step: 1800 loss_mean: 2.7386247158050536
step: 1850 loss_mean: 2.7819485664367676
step: 1900 loss_mean: 2.7089404702186584
step: 1950 loss_mean: 2.756070513725281
step: 2000 loss_mean: 2.6842726182937624
step: 2050 loss_mean: 2.7529846000671387
step: 2100 loss_mean: 2.7737014961242674
step: 2150 loss_mean: 2.7668162870407103
step: 2200 loss_mean: 2.762450404167175
step: 2250 loss_mean: 2.763514213562012
step: 2300 loss_mean: 2.718559823036194
step: 2350 loss_mean: 2.809817614555359
step: 2400 loss_mean: 2.7468866109848022
step: 2450 loss_mean: 2.7096734189987184
step: 2500 loss_mean: 2.772179069519043
step: 2550 loss_mean: 2.7473488330841063
step: 2600 loss_mean: 2.801672172546387
step: 2650 loss_mean: 2.794118490219116
step: 2700 loss_mean: 2.724666266441345
step: 2750 loss_mean: 2.7615482091903685
step: 2800 loss_mean: 2.772074842453003
step: 2850 loss_mean: 2.7714756202697752
step: 2900 loss_mean: 2.8081810092926025
step: 2950 loss_mean: 2.751097478866577
step: 3000 loss_mean: 2.730928111076355
step: 3050 loss_mean: 2.7544199562072755
step: 3100 loss_mean: 2.8214305448532104
step: 50 loss_mean: 2.78731999874115
step: 100 loss_mean: 2.829935083389282
step: 150 loss_mean: 2.7585726594924926
step: 200 loss_mean: 2.8742025089263916
step: 250 loss_mean: 2.818211679458618
step: 300 loss_mean: 2.8896059656143187
step: 350 loss_mean: 2.7489602184295654
step: 400 loss_mean: 2.7992650651931763
step: 450 loss_mean: 2.763543667793274
step: 500 loss_mean: 2.8073154497146606
step: 550 loss_mean: 2.779652304649353
step: 600 loss_mean: 2.7184667110443117
Epoch: 53 | Run time: 624.0 s | Train loss: 2.75 | Valid loss: 2.8
Saved checkpoint 53 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0792 s.
run_epoch
step: 50 loss_mean: 2.7491857862472533
step: 100 loss_mean: 2.7576880407333375
step: 150 loss_mean: 2.758424596786499
step: 200 loss_mean: 2.730562129020691
step: 250 loss_mean: 2.7256998920440676
step: 300 loss_mean: 2.739955368041992
step: 350 loss_mean: 2.8137021589279176
step: 400 loss_mean: 2.72645450592041
step: 450 loss_mean: 2.6764150142669676
step: 500 loss_mean: 2.749517593383789
step: 550 loss_mean: 2.796186685562134
step: 600 loss_mean: 2.7103875041007996
step: 650 loss_mean: 2.805070848464966
step: 700 loss_mean: 2.742928023338318
step: 750 loss_mean: 2.6991989421844482
step: 800 loss_mean: 2.726148042678833
step: 850 loss_mean: 2.777282977104187
step: 900 loss_mean: 2.724787368774414
step: 950 loss_mean: 2.7844453382492067
step: 1000 loss_mean: 2.73147075176239
step: 1050 loss_mean: 2.757921824455261
step: 1100 loss_mean: 2.7776962327957153
step: 1150 loss_mean: 2.7946279001235963
step: 1200 loss_mean: 2.7234886360168455
step: 1250 loss_mean: 2.763042469024658
step: 1300 loss_mean: 2.7058354902267454
step: 1350 loss_mean: 2.748488883972168
step: 1400 loss_mean: 2.749202709197998
step: 1450 loss_mean: 2.7391181135177614
step: 1500 loss_mean: 2.703537917137146
step: 1550 loss_mean: 2.743116927146912
step: 1600 loss_mean: 2.80392870426178
step: 1650 loss_mean: 2.7594005489349365
step: 1700 loss_mean: 2.7910924768447876
step: 1750 loss_mean: 2.787125573158264
step: 1800 loss_mean: 2.7191177344322206
step: 1850 loss_mean: 2.7673722457885743
step: 1900 loss_mean: 2.763298397064209
step: 1950 loss_mean: 2.7383934593200685
step: 2000 loss_mean: 2.7370087766647337
step: 2050 loss_mean: 2.7060186672210693
step: 2100 loss_mean: 2.7864691066741942
step: 2150 loss_mean: 2.7163263845443724
step: 2200 loss_mean: 2.7520009088516235
step: 2250 loss_mean: 2.776602659225464
step: 2300 loss_mean: 2.7660427331924438
step: 2350 loss_mean: 2.746685514450073
step: 2400 loss_mean: 2.764184069633484
step: 2450 loss_mean: 2.6812226247787474
step: 2500 loss_mean: 2.7114949941635134
step: 2550 loss_mean: 2.7364879512786864
step: 2600 loss_mean: 2.78859619140625
step: 2650 loss_mean: 2.754750442504883
step: 2700 loss_mean: 2.738197283744812
step: 2750 loss_mean: 2.808824610710144
step: 2800 loss_mean: 2.7531824207305906
step: 2850 loss_mean: 2.758984694480896
step: 2900 loss_mean: 2.806511025428772
step: 2950 loss_mean: 2.731110773086548
step: 3000 loss_mean: 2.745375952720642
step: 3050 loss_mean: 2.747908477783203
step: 3100 loss_mean: 2.7715597105026246
step: 50 loss_mean: 2.7719952726364134
step: 100 loss_mean: 2.803370785713196
step: 150 loss_mean: 2.7325219917297363
step: 200 loss_mean: 2.8249404621124268
step: 250 loss_mean: 2.77913459777832
step: 300 loss_mean: 2.8638161849975585
step: 350 loss_mean: 2.7470779323577883
step: 400 loss_mean: 2.7693539524078368
step: 450 loss_mean: 2.737924880981445
step: 500 loss_mean: 2.805365915298462
step: 550 loss_mean: 2.7568336868286134
step: 600 loss_mean: 2.7146409559249878
Epoch: 54 | Run time: 613.0 s | Train loss: 2.75 | Valid loss: 2.78
Saved checkpoint 54 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0815 s.
run_epoch
step: 50 loss_mean: 2.7576913738250735
step: 100 loss_mean: 2.7011098003387453
step: 150 loss_mean: 2.740711932182312
step: 200 loss_mean: 2.7731657361984254
step: 250 loss_mean: 2.755345220565796
step: 300 loss_mean: 2.7800452280044556
step: 350 loss_mean: 2.7244307088851927
step: 400 loss_mean: 2.765023775100708
step: 450 loss_mean: 2.7035878038406373
step: 500 loss_mean: 2.76179780960083
step: 550 loss_mean: 2.7637833070755007
step: 600 loss_mean: 2.7773709154129027
step: 650 loss_mean: 2.769971981048584
step: 700 loss_mean: 2.7321716213226317
step: 750 loss_mean: 2.739664673805237
step: 800 loss_mean: 2.765826144218445
step: 850 loss_mean: 2.692040147781372
step: 900 loss_mean: 2.6863354063034057
step: 950 loss_mean: 2.733378319740295
step: 1000 loss_mean: 2.7467512893676758
step: 1050 loss_mean: 2.7659229373931886
step: 1100 loss_mean: 2.7371720123291015
step: 1150 loss_mean: 2.766853132247925
step: 1200 loss_mean: 2.7322591161727905
step: 1250 loss_mean: 2.7329880666732786
step: 1300 loss_mean: 2.769911551475525
step: 1350 loss_mean: 2.78939151763916
step: 1400 loss_mean: 2.7695516538619995
step: 1450 loss_mean: 2.6937240886688234
step: 1500 loss_mean: 2.7615662527084353
step: 1550 loss_mean: 2.82552818775177
step: 1600 loss_mean: 2.7162767362594606
step: 1650 loss_mean: 2.812182960510254
step: 1700 loss_mean: 2.780495648384094
step: 1750 loss_mean: 2.746301612854004
step: 1800 loss_mean: 2.8063947057723997
step: 1850 loss_mean: 2.765807242393494
step: 1900 loss_mean: 2.7064657545089723
step: 1950 loss_mean: 2.7270756483078005
step: 2000 loss_mean: 2.7679165744781495
step: 2050 loss_mean: 2.7019801330566406
step: 2100 loss_mean: 2.7214831829071047
step: 2150 loss_mean: 2.7231060123443602
step: 2200 loss_mean: 2.765889105796814
step: 2250 loss_mean: 2.7898920488357546
step: 2300 loss_mean: 2.7528884315490725
step: 2350 loss_mean: 2.7276407575607298
step: 2400 loss_mean: 2.8097752714157105
step: 2450 loss_mean: 2.738721742630005
step: 2500 loss_mean: 2.7048251247406006
step: 2550 loss_mean: 2.761434850692749
step: 2600 loss_mean: 2.7310037279129027
step: 2650 loss_mean: 2.800441837310791
step: 2700 loss_mean: 2.7681245899200437
step: 2750 loss_mean: 2.7366456508636476
step: 2800 loss_mean: 2.7086146688461303
step: 2850 loss_mean: 2.74169415473938
step: 2900 loss_mean: 2.813950471878052
step: 2950 loss_mean: 2.694139723777771
step: 3000 loss_mean: 2.7396721935272215
step: 3050 loss_mean: 2.7633305764198304
step: 3100 loss_mean: 2.7364965200424196
step: 50 loss_mean: 2.7835386991500854
step: 100 loss_mean: 2.829634680747986
step: 150 loss_mean: 2.763533549308777
step: 200 loss_mean: 2.8439729833602905
step: 250 loss_mean: 2.7880649089813234
step: 300 loss_mean: 2.8715661859512327
step: 350 loss_mean: 2.7469025564193728
step: 400 loss_mean: 2.789738645553589
step: 450 loss_mean: 2.759834451675415
step: 500 loss_mean: 2.816879596710205
step: 550 loss_mean: 2.765884165763855
step: 600 loss_mean: 2.736357684135437
Epoch: 55 | Run time: 623.0 s | Train loss: 2.75 | Valid loss: 2.79
Saved checkpoint 55 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0814 s.
run_epoch
step: 50 loss_mean: 2.7437347841262816
step: 100 loss_mean: 2.736948175430298
step: 150 loss_mean: 2.851898679733276
step: 200 loss_mean: 2.782927165031433
step: 250 loss_mean: 2.770769739151001
step: 300 loss_mean: 2.781762342453003
step: 350 loss_mean: 2.7516421842575074
step: 400 loss_mean: 2.762590789794922
step: 450 loss_mean: 2.7674876475334167
step: 500 loss_mean: 2.7497252893447874
step: 550 loss_mean: 2.738815507888794
step: 600 loss_mean: 2.7736572217941284
step: 650 loss_mean: 2.7795528411865233
step: 700 loss_mean: 2.757919178009033
step: 750 loss_mean: 2.762856421470642
step: 800 loss_mean: 2.7447551107406616
step: 850 loss_mean: 2.8072951507568358
step: 900 loss_mean: 2.7551895666122435
step: 950 loss_mean: 2.717932605743408
step: 1000 loss_mean: 2.725548686981201
step: 1050 loss_mean: 2.7514953088760374
step: 1100 loss_mean: 2.751441631317139
step: 1150 loss_mean: 2.712104806900024
step: 1200 loss_mean: 2.7544468736648557
step: 1250 loss_mean: 2.6829116439819334
step: 1300 loss_mean: 2.7697762203216554
step: 1350 loss_mean: 2.7771215867996215
step: 1400 loss_mean: 2.753464684486389
step: 1450 loss_mean: 2.7274707841873167
step: 1500 loss_mean: 2.6834957504272463
step: 1550 loss_mean: 2.7386581039428712
step: 1600 loss_mean: 2.7377839517593383
step: 1650 loss_mean: 2.7483280420303347
step: 1700 loss_mean: 2.6925653839111328
step: 1750 loss_mean: 2.732927441596985
step: 1800 loss_mean: 2.743041543960571
step: 1850 loss_mean: 2.6641025066375734
step: 1900 loss_mean: 2.791146230697632
step: 1950 loss_mean: 2.7205770587921143
step: 2000 loss_mean: 2.6546185874938963
step: 2050 loss_mean: 2.753034243583679
step: 2100 loss_mean: 2.764961562156677
step: 2150 loss_mean: 2.70594135761261
step: 2200 loss_mean: 2.790674662590027
step: 2250 loss_mean: 2.775187382698059
step: 2300 loss_mean: 2.7513653230667114
step: 2350 loss_mean: 2.7691954040527342
step: 2400 loss_mean: 2.7377188491821287
step: 2450 loss_mean: 2.765921573638916
step: 2500 loss_mean: 2.7504119539260863
step: 2550 loss_mean: 2.696194052696228
step: 2600 loss_mean: 2.7047838973999023
step: 2650 loss_mean: 2.773405842781067
step: 2700 loss_mean: 2.7868899393081663
step: 2750 loss_mean: 2.7393021631240844
step: 2800 loss_mean: 2.7711371421813964
step: 2850 loss_mean: 2.7576952743530274
step: 2900 loss_mean: 2.7492133522033693
step: 2950 loss_mean: 2.7853144454956054
step: 3000 loss_mean: 2.724439640045166
step: 3050 loss_mean: 2.7356070280075073
step: 3100 loss_mean: 2.7359342193603515
step: 50 loss_mean: 2.7749166250228883
step: 100 loss_mean: 2.796376404762268
step: 150 loss_mean: 2.736783528327942
step: 200 loss_mean: 2.8461514949798583
step: 250 loss_mean: 2.7852812385559083
step: 300 loss_mean: 2.883814225196838
step: 350 loss_mean: 2.73314049243927
step: 400 loss_mean: 2.743260159492493
step: 450 loss_mean: 2.753140687942505
step: 500 loss_mean: 2.7945852279663086
step: 550 loss_mean: 2.7618162631988525
step: 600 loss_mean: 2.7116408586502074
Epoch: 56 | Run time: 623.0 s | Train loss: 2.75 | Valid loss: 2.78
Saved checkpoint 56 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0835 s.
run_epoch
step: 50 loss_mean: 2.7936591339111327
step: 100 loss_mean: 2.7721074390411378
step: 150 loss_mean: 2.7440392684936525
step: 200 loss_mean: 2.8076127672195437
step: 250 loss_mean: 2.7529638719558718
step: 300 loss_mean: 2.7530564737319945
step: 350 loss_mean: 2.7156723976135253
step: 400 loss_mean: 2.7760307025909423
step: 450 loss_mean: 2.775154914855957
step: 500 loss_mean: 2.721791162490845
step: 550 loss_mean: 2.719365396499634
step: 600 loss_mean: 2.77255096912384
step: 650 loss_mean: 2.7715455055236817
step: 700 loss_mean: 2.781641983985901
step: 750 loss_mean: 2.7341542196273805
step: 800 loss_mean: 2.7440223789215086
step: 850 loss_mean: 2.701069378852844
step: 900 loss_mean: 2.6891166257858274
step: 950 loss_mean: 2.7779413938522337
step: 1000 loss_mean: 2.739661660194397
step: 1050 loss_mean: 2.7843544054031373
step: 1100 loss_mean: 2.733625135421753
step: 1150 loss_mean: 2.766058669090271
step: 1200 loss_mean: 2.8091601753234863
step: 1250 loss_mean: 2.7140379047393797
step: 1300 loss_mean: 2.7455140495300294
step: 1350 loss_mean: 2.702409601211548
step: 1400 loss_mean: 2.7194657802581785
step: 1450 loss_mean: 2.7602251195907592
step: 1500 loss_mean: 2.727947487831116
step: 1550 loss_mean: 2.7654304265975953
step: 1600 loss_mean: 2.7621767854690553
step: 1650 loss_mean: 2.719088759422302
step: 1700 loss_mean: 2.72494113445282
step: 1750 loss_mean: 2.753618755340576
step: 1800 loss_mean: 2.7256277322769167
step: 1850 loss_mean: 2.761814546585083
step: 1900 loss_mean: 2.741287317276001
step: 1950 loss_mean: 2.7489873647689818
step: 2000 loss_mean: 2.7823664236068724
step: 2050 loss_mean: 2.7647229862213134
step: 2100 loss_mean: 2.810862355232239
step: 2150 loss_mean: 2.7450211429595948
step: 2200 loss_mean: 2.806907458305359
step: 2250 loss_mean: 2.7011844301223755
step: 2300 loss_mean: 2.6626319885253906
step: 2350 loss_mean: 2.737157006263733
step: 2400 loss_mean: 2.7621971559524536
step: 2450 loss_mean: 2.7569411993026733
step: 2500 loss_mean: 2.7535721683502197
step: 2550 loss_mean: 2.689990620613098
step: 2600 loss_mean: 2.76617769241333
step: 2650 loss_mean: 2.7212978649139403
step: 2700 loss_mean: 2.692181706428528
step: 2750 loss_mean: 2.784574556350708
step: 2800 loss_mean: 2.715818819999695
step: 2850 loss_mean: 2.7779971075057985
step: 2900 loss_mean: 2.747322449684143
step: 2950 loss_mean: 2.7106384372711183
step: 3000 loss_mean: 2.739056262969971
step: 3050 loss_mean: 2.7090515518188476
step: 3100 loss_mean: 2.7487362813949585
step: 50 loss_mean: 2.7564726781845095
step: 100 loss_mean: 2.7798566961288453
step: 150 loss_mean: 2.7300106954574583
step: 200 loss_mean: 2.8201574325561523
step: 250 loss_mean: 2.7895985794067384
step: 300 loss_mean: 2.8493723917007445
step: 350 loss_mean: 2.7007092571258546
step: 400 loss_mean: 2.742886953353882
step: 450 loss_mean: 2.7404251956939696
step: 500 loss_mean: 2.7737402153015136
step: 550 loss_mean: 2.743224620819092
step: 600 loss_mean: 2.689985589981079
Epoch: 57 | Run time: 625.0 s | Train loss: 2.75 | Valid loss: 2.76
Saved checkpoint 57 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0846 s.
run_epoch
step: 50 loss_mean: 2.721346826553345
step: 100 loss_mean: 2.794481692314148
step: 150 loss_mean: 2.727661533355713
step: 200 loss_mean: 2.763311014175415
step: 250 loss_mean: 2.7343063068389895
step: 300 loss_mean: 2.734922890663147
step: 350 loss_mean: 2.789198145866394
step: 400 loss_mean: 2.8027458429336547
step: 450 loss_mean: 2.7584106588363646
step: 500 loss_mean: 2.713707151412964
step: 550 loss_mean: 2.7407257652282713
step: 600 loss_mean: 2.7415001153945924
step: 650 loss_mean: 2.703372893333435
step: 700 loss_mean: 2.7445104789733885
step: 750 loss_mean: 2.7800525569915773
step: 800 loss_mean: 2.756615762710571
step: 850 loss_mean: 2.7139660120010376
step: 900 loss_mean: 2.798958463668823
step: 950 loss_mean: 2.7390004777908326
step: 1000 loss_mean: 2.717203540802002
step: 1050 loss_mean: 2.7792918062210084
step: 1100 loss_mean: 2.744414324760437
step: 1150 loss_mean: 2.687810935974121
step: 1200 loss_mean: 2.7306059980392456
step: 1250 loss_mean: 2.715262746810913
step: 1300 loss_mean: 2.7554788827896117
step: 1350 loss_mean: 2.763142547607422
step: 1400 loss_mean: 2.7615916109085084
step: 1450 loss_mean: 2.734884748458862
step: 1500 loss_mean: 2.7303971529006956
step: 1550 loss_mean: 2.7273370122909544
step: 1600 loss_mean: 2.788966598510742
step: 1650 loss_mean: 2.72787127494812
step: 1700 loss_mean: 2.7497252798080445
step: 1750 loss_mean: 2.7731837129592893
step: 1800 loss_mean: 2.7635133838653565
step: 1850 loss_mean: 2.7493625116348266
step: 1900 loss_mean: 2.6844527435302736
step: 1950 loss_mean: 2.7159144639968873
step: 2000 loss_mean: 2.7470965337753297
step: 2050 loss_mean: 2.7179334926605225
step: 2100 loss_mean: 2.7038213634490966
step: 2150 loss_mean: 2.7072784090042115
step: 2200 loss_mean: 2.8072681999206544
step: 2250 loss_mean: 2.768804278373718
step: 2300 loss_mean: 2.724881501197815
step: 2350 loss_mean: 2.7039010095596314
step: 2400 loss_mean: 2.7104143285751343
step: 2450 loss_mean: 2.706267056465149
step: 2500 loss_mean: 2.7036627435684206
step: 2550 loss_mean: 2.7442592716217042
step: 2600 loss_mean: 2.7645635652542113
step: 2650 loss_mean: 2.7509146690368653
step: 2700 loss_mean: 2.768646697998047
step: 2750 loss_mean: 2.8038372707366945
step: 2800 loss_mean: 2.7629825496673583
step: 2850 loss_mean: 2.7532902431488036
step: 2900 loss_mean: 2.834291710853577
step: 2950 loss_mean: 2.679535255432129
step: 3000 loss_mean: 2.7519665431976317
step: 3050 loss_mean: 2.6939929294586182
step: 3100 loss_mean: 2.751860399246216
step: 50 loss_mean: 2.7782521295547484
step: 100 loss_mean: 2.815705304145813
step: 150 loss_mean: 2.756387553215027
step: 200 loss_mean: 2.872633500099182
step: 250 loss_mean: 2.8026313018798827
step: 300 loss_mean: 2.882090311050415
step: 350 loss_mean: 2.7534889364242554
step: 400 loss_mean: 2.802921051979065
step: 450 loss_mean: 2.767454080581665
step: 500 loss_mean: 2.804226431846619
step: 550 loss_mean: 2.764601411819458
step: 600 loss_mean: 2.725778675079346
Epoch: 58 | Run time: 624.0 s | Train loss: 2.74 | Valid loss: 2.8
Saved checkpoint 58 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0856 s.
run_epoch
step: 50 loss_mean: 2.7315859270095824
step: 100 loss_mean: 2.787924919128418
step: 150 loss_mean: 2.674143171310425
step: 200 loss_mean: 2.749504632949829
step: 250 loss_mean: 2.7508371686935424
step: 300 loss_mean: 2.7220846033096313
step: 350 loss_mean: 2.7481914615631102
step: 400 loss_mean: 2.6979915714263916
step: 450 loss_mean: 2.7724255466461183
step: 500 loss_mean: 2.8015188121795656
step: 550 loss_mean: 2.7643979358673096
step: 600 loss_mean: 2.7203028917312624
step: 650 loss_mean: 2.740151743888855
step: 700 loss_mean: 2.7209506320953367
step: 750 loss_mean: 2.7692586898803713
step: 800 loss_mean: 2.7690870428085326
step: 850 loss_mean: 2.7645160388946532
step: 900 loss_mean: 2.725621609687805
step: 950 loss_mean: 2.7399781847000124
step: 1000 loss_mean: 2.763776478767395
step: 1050 loss_mean: 2.7285957765579223
step: 1100 loss_mean: 2.7814010429382323
step: 1150 loss_mean: 2.752840824127197
step: 1200 loss_mean: 2.776951560974121
step: 1250 loss_mean: 2.727713932991028
step: 1300 loss_mean: 2.7537754011154174
step: 1350 loss_mean: 2.720875930786133
step: 1400 loss_mean: 2.7476989603042603
step: 1450 loss_mean: 2.7725619173049925
step: 1500 loss_mean: 2.7474446153640746
step: 1550 loss_mean: 2.7650865364074706
step: 1600 loss_mean: 2.7702211427688597
step: 1650 loss_mean: 2.740464572906494
step: 1700 loss_mean: 2.7098397541046144
step: 1750 loss_mean: 2.7313217544555664
step: 1800 loss_mean: 2.780745849609375
step: 1850 loss_mean: 2.7519126987457274
step: 1900 loss_mean: 2.7327733278274535
step: 1950 loss_mean: 2.7092777061462403
step: 2000 loss_mean: 2.7698001766204836
step: 2050 loss_mean: 2.7163251066207885
step: 2100 loss_mean: 2.723704466819763
step: 2150 loss_mean: 2.753471097946167
step: 2200 loss_mean: 2.7789576387405397
step: 2250 loss_mean: 2.7632809257507325
step: 2300 loss_mean: 2.7231944370269776
step: 2350 loss_mean: 2.739097332954407
step: 2400 loss_mean: 2.748001980781555
step: 2450 loss_mean: 2.703522515296936
step: 2500 loss_mean: 2.725968613624573
step: 2550 loss_mean: 2.7590805292129517
step: 2600 loss_mean: 2.7449849367141725
step: 2650 loss_mean: 2.7474441719055176
step: 2700 loss_mean: 2.756626353263855
step: 2750 loss_mean: 2.727104916572571
step: 2800 loss_mean: 2.7413256692886354
step: 2850 loss_mean: 2.7739324140548707
step: 2900 loss_mean: 2.765771427154541
step: 2950 loss_mean: 2.7500295066833496
step: 3000 loss_mean: 2.757027020454407
step: 3050 loss_mean: 2.749786548614502
step: 3100 loss_mean: 2.7298294639587404
step: 50 loss_mean: 2.7747119951248167
step: 100 loss_mean: 2.807999749183655
step: 150 loss_mean: 2.7454517555236815
step: 200 loss_mean: 2.8500865650177003
step: 250 loss_mean: 2.7750530147552492
step: 300 loss_mean: 2.8756906270980833
step: 350 loss_mean: 2.7254130148887636
step: 400 loss_mean: 2.765423536300659
step: 450 loss_mean: 2.754183192253113
step: 500 loss_mean: 2.7862857246398924
step: 550 loss_mean: 2.7649256086349485
step: 600 loss_mean: 2.70806453704834
Epoch: 59 | Run time: 624.0 s | Train loss: 2.75 | Valid loss: 2.78
Saved checkpoint 59 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0877 s.
run_epoch
step: 50 loss_mean: 2.6813556337356568
step: 100 loss_mean: 2.8032167530059815
step: 150 loss_mean: 2.678427152633667
step: 200 loss_mean: 2.7424125051498414
step: 250 loss_mean: 2.775254192352295
step: 300 loss_mean: 2.7417613649368286
step: 350 loss_mean: 2.7472033309936523
step: 400 loss_mean: 2.7295109033584595
step: 450 loss_mean: 2.716042137145996
step: 500 loss_mean: 2.6918815660476683
step: 550 loss_mean: 2.7587124729156494
step: 600 loss_mean: 2.7485986375808715
step: 650 loss_mean: 2.6783934259414672
step: 700 loss_mean: 2.7575513887405396
step: 750 loss_mean: 2.753657808303833
step: 800 loss_mean: 2.752177510261536
step: 850 loss_mean: 2.7476137685775757
step: 900 loss_mean: 2.7644844818115235
step: 950 loss_mean: 2.7467766189575196
step: 1000 loss_mean: 2.6862709665298463
step: 1050 loss_mean: 2.780780763626099
step: 1100 loss_mean: 2.7101842308044435
step: 1150 loss_mean: 2.6612734508514406
step: 1200 loss_mean: 2.714336009025574
step: 1250 loss_mean: 2.750336847305298
step: 1300 loss_mean: 2.7630317068099974
step: 1350 loss_mean: 2.78758131980896
step: 1400 loss_mean: 2.751128730773926
step: 1450 loss_mean: 2.8095477294921873
step: 1500 loss_mean: 2.741394395828247
step: 1550 loss_mean: 2.759745864868164
step: 1600 loss_mean: 2.788516640663147
step: 1650 loss_mean: 2.7157873010635374
step: 1700 loss_mean: 2.7344310331344603
step: 1750 loss_mean: 2.760389370918274
step: 1800 loss_mean: 2.728285942077637
step: 1850 loss_mean: 2.7476285171508787
step: 1900 loss_mean: 2.7663026666641235
step: 1950 loss_mean: 2.6913204526901247
step: 2000 loss_mean: 2.722633991241455
step: 2050 loss_mean: 2.7216155910491944
step: 2100 loss_mean: 2.741563539505005
step: 2150 loss_mean: 2.775864381790161
step: 2200 loss_mean: 2.7445823049545286
step: 2250 loss_mean: 2.7800616359710695
step: 2300 loss_mean: 2.815844292640686
step: 2350 loss_mean: 2.7385451126098634
step: 2400 loss_mean: 2.710109167098999
step: 2450 loss_mean: 2.7487859964370727
step: 2500 loss_mean: 2.7483736944198607
step: 2550 loss_mean: 2.762623815536499
step: 2600 loss_mean: 2.713032112121582
step: 2650 loss_mean: 2.7349642419815066
step: 2700 loss_mean: 2.7504977798461914
step: 2750 loss_mean: 2.772191252708435
step: 2800 loss_mean: 2.75409547328949
step: 2850 loss_mean: 2.7064491891860962
step: 2900 loss_mean: 2.7397334146499634
step: 2950 loss_mean: 2.810544700622559
step: 3000 loss_mean: 2.749689121246338
step: 3050 loss_mean: 2.720463399887085
step: 3100 loss_mean: 2.7500072813034055
step: 50 loss_mean: 2.783517069816589
step: 100 loss_mean: 2.823101396560669
step: 150 loss_mean: 2.7507204532623293
step: 200 loss_mean: 2.893721761703491
step: 250 loss_mean: 2.78222225189209
step: 300 loss_mean: 2.9010943841934203
step: 350 loss_mean: 2.758046088218689
step: 400 loss_mean: 2.7696418523788453
step: 450 loss_mean: 2.7534995889663696
step: 500 loss_mean: 2.824474964141846
step: 550 loss_mean: 2.7999383306503294
step: 600 loss_mean: 2.72186393737793
Epoch: 60 | Run time: 619.0 s | Train loss: 2.74 | Valid loss: 2.8
Saved checkpoint 60 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0891 s.
run_epoch
step: 50 loss_mean: 2.6534248542785646
step: 100 loss_mean: 2.7540910196304322
step: 150 loss_mean: 2.752724080085754
step: 200 loss_mean: 2.777564034461975
step: 250 loss_mean: 2.7732248640060426
step: 300 loss_mean: 2.786494174003601
step: 350 loss_mean: 2.769684796333313
step: 400 loss_mean: 2.7296009922027586
step: 450 loss_mean: 2.710961575508118
step: 500 loss_mean: 2.766036229133606
step: 550 loss_mean: 2.7066694021224977
step: 600 loss_mean: 2.7320938968658446
step: 650 loss_mean: 2.713737587928772
step: 700 loss_mean: 2.741063814163208
step: 750 loss_mean: 2.762235803604126
step: 800 loss_mean: 2.7081810092926024
step: 850 loss_mean: 2.7085227489471437
step: 900 loss_mean: 2.7169188117980956
step: 950 loss_mean: 2.693393416404724
step: 1000 loss_mean: 2.723457431793213
step: 1050 loss_mean: 2.7306023836135864
step: 1100 loss_mean: 2.735683603286743
step: 1150 loss_mean: 2.736903872489929
step: 1200 loss_mean: 2.7994291591644287
step: 1250 loss_mean: 2.7504033613204957
step: 1300 loss_mean: 2.709980173110962
step: 1350 loss_mean: 2.755209197998047
step: 1400 loss_mean: 2.705970115661621
step: 1450 loss_mean: 2.7888614463806154
step: 1500 loss_mean: 2.7106805801391602
step: 1550 loss_mean: 2.7258570909500124
step: 1600 loss_mean: 2.789578456878662
step: 1650 loss_mean: 2.7245478057861328
step: 1700 loss_mean: 2.7720899868011473
step: 1750 loss_mean: 2.779161214828491
step: 1800 loss_mean: 2.735976719856262
step: 1850 loss_mean: 2.7577908992767335
step: 1900 loss_mean: 2.699279384613037
step: 1950 loss_mean: 2.78849778175354
step: 2000 loss_mean: 2.7012400150299074
step: 2050 loss_mean: 2.672740087509155
step: 2100 loss_mean: 2.713646535873413
step: 2150 loss_mean: 2.726214232444763
step: 2200 loss_mean: 2.754348917007446
step: 2250 loss_mean: 2.741788430213928
step: 2300 loss_mean: 2.7740832471847536
step: 2350 loss_mean: 2.759544153213501
step: 2400 loss_mean: 2.7498198843002317
step: 2450 loss_mean: 2.7348954677581787
step: 2500 loss_mean: 2.7291626834869387
step: 2550 loss_mean: 2.759146375656128
step: 2600 loss_mean: 2.735690393447876
step: 2650 loss_mean: 2.7583016920089722
step: 2700 loss_mean: 2.7514083623886108
step: 2750 loss_mean: 2.714351372718811
step: 2800 loss_mean: 2.6786964988708495
step: 2850 loss_mean: 2.7481452131271364
step: 2900 loss_mean: 2.7751661729812622
step: 2950 loss_mean: 2.7718151807785034
step: 3000 loss_mean: 2.7827947425842283
step: 3050 loss_mean: 2.7597819805145263
step: 3100 loss_mean: 2.7335713052749635
step: 50 loss_mean: 2.778300757408142
step: 100 loss_mean: 2.8040349817276002
step: 150 loss_mean: 2.742042784690857
step: 200 loss_mean: 2.8342070960998536
step: 250 loss_mean: 2.803665270805359
step: 300 loss_mean: 2.8684685754776003
step: 350 loss_mean: 2.737390694618225
step: 400 loss_mean: 2.750602059364319
step: 450 loss_mean: 2.7487997722625734
step: 500 loss_mean: 2.7948552227020262
step: 550 loss_mean: 2.7648807621002196
step: 600 loss_mean: 2.716030783653259
Epoch: 61 | Run time: 622.0 s | Train loss: 2.74 | Valid loss: 2.78
Saved checkpoint 61 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0913 s.
run_epoch
step: 50 loss_mean: 2.761580061912537
step: 100 loss_mean: 2.754394917488098
step: 150 loss_mean: 2.769153742790222
step: 200 loss_mean: 2.778248691558838
step: 250 loss_mean: 2.760412082672119
step: 300 loss_mean: 2.715027289390564
step: 350 loss_mean: 2.7345396995544435
step: 400 loss_mean: 2.7593307876586914
step: 450 loss_mean: 2.7274179792404176
step: 500 loss_mean: 2.738858299255371
step: 550 loss_mean: 2.752677083015442
step: 600 loss_mean: 2.7004930353164673
step: 650 loss_mean: 2.7476124954223633
step: 700 loss_mean: 2.7704131698608396
step: 750 loss_mean: 2.6654468059539793
step: 800 loss_mean: 2.6898068141937257
step: 850 loss_mean: 2.7098897218704225
step: 900 loss_mean: 2.7349513721466066
step: 950 loss_mean: 2.7395102405548095
step: 1000 loss_mean: 2.7157118940353393
step: 1050 loss_mean: 2.7054541015625
step: 1100 loss_mean: 2.6945866441726682
step: 1150 loss_mean: 2.710317416191101
step: 1200 loss_mean: 2.731989426612854
step: 1250 loss_mean: 2.746988306045532
step: 1300 loss_mean: 2.7181042337417605
step: 1350 loss_mean: 2.78947874546051
step: 1400 loss_mean: 2.7716821098327635
step: 1450 loss_mean: 2.6942188024520872
step: 1500 loss_mean: 2.7665291261672973
step: 1550 loss_mean: 2.7140365982055665
step: 1600 loss_mean: 2.736370463371277
step: 1650 loss_mean: 2.745449595451355
step: 1700 loss_mean: 2.698787503242493
step: 1750 loss_mean: 2.7502890539169313
step: 1800 loss_mean: 2.758462691307068
step: 1850 loss_mean: 2.745376663208008
step: 1900 loss_mean: 2.75693962097168
step: 1950 loss_mean: 2.7627541780471803
step: 2000 loss_mean: 2.7802959442138673
step: 2050 loss_mean: 2.7544155502319336
step: 2100 loss_mean: 2.7041304779052733
step: 2150 loss_mean: 2.748659219741821
step: 2200 loss_mean: 2.698348355293274
step: 2250 loss_mean: 2.783265027999878
step: 2300 loss_mean: 2.7564422082901
step: 2350 loss_mean: 2.827575249671936
step: 2400 loss_mean: 2.6861049222946165
step: 2450 loss_mean: 2.6966364765167237
step: 2500 loss_mean: 2.78763156414032
step: 2550 loss_mean: 2.8288992929458616
step: 2600 loss_mean: 2.74494900226593
step: 2650 loss_mean: 2.810381460189819
step: 2700 loss_mean: 2.7739095640182496
step: 2750 loss_mean: 2.734348692893982
step: 2800 loss_mean: 2.742003893852234
step: 2850 loss_mean: 2.726288857460022
step: 2900 loss_mean: 2.7191240072250364
step: 2950 loss_mean: 2.717648506164551
step: 3000 loss_mean: 2.714735498428345
step: 3050 loss_mean: 2.6820612239837645
step: 3100 loss_mean: 2.773579740524292
step: 50 loss_mean: 2.853539457321167
step: 100 loss_mean: 2.914327926635742
step: 150 loss_mean: 2.8197117233276368
step: 200 loss_mean: 2.9387220525741578
step: 250 loss_mean: 2.846688723564148
step: 300 loss_mean: 2.9591354942321777
step: 350 loss_mean: 2.808567271232605
step: 400 loss_mean: 2.822684102058411
step: 450 loss_mean: 2.8532675552368163
step: 500 loss_mean: 2.8629325008392335
step: 550 loss_mean: 2.8357303285598756
step: 600 loss_mean: 2.7750941801071165
Epoch: 62 | Run time: 624.0 s | Train loss: 2.74 | Valid loss: 2.86
Saved checkpoint 62 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0916 s.
run_epoch
step: 50 loss_mean: 2.755964341163635
step: 100 loss_mean: 2.7663487911224367
step: 150 loss_mean: 2.7232195043563845
step: 200 loss_mean: 2.803708872795105
step: 250 loss_mean: 2.763445839881897
step: 300 loss_mean: 2.7286841106414794
step: 350 loss_mean: 2.7257113790512086
step: 400 loss_mean: 2.724668712615967
step: 450 loss_mean: 2.7731594800949098
step: 500 loss_mean: 2.7220254707336426
step: 550 loss_mean: 2.792943391799927
step: 600 loss_mean: 2.7234541988372802
step: 650 loss_mean: 2.6868777751922606
step: 700 loss_mean: 2.7132061100006104
step: 750 loss_mean: 2.7044343519210816
step: 800 loss_mean: 2.6845157098770143
step: 850 loss_mean: 2.775183696746826
step: 900 loss_mean: 2.7204496574401857
step: 950 loss_mean: 2.697621202468872
step: 1000 loss_mean: 2.698476948738098
step: 1050 loss_mean: 2.779155297279358
step: 1100 loss_mean: 2.7607903051376343
step: 1150 loss_mean: 2.7174560594558717
step: 1200 loss_mean: 2.750797610282898
step: 1250 loss_mean: 2.6911294794082643
step: 1300 loss_mean: 2.701731061935425
step: 1350 loss_mean: 2.7754326105117797
step: 1400 loss_mean: 2.709732117652893
step: 1450 loss_mean: 2.7327098178863527
step: 1500 loss_mean: 2.770172805786133
step: 1550 loss_mean: 2.791945581436157
step: 1600 loss_mean: 2.692903370857239
step: 1650 loss_mean: 2.7053489828109742
step: 1700 loss_mean: 2.7626710319519043
step: 1750 loss_mean: 2.748797907829285
step: 1800 loss_mean: 2.7447349786758424
step: 1850 loss_mean: 2.693672389984131
step: 1900 loss_mean: 2.7049867296218872
step: 1950 loss_mean: 2.7543990993499756
step: 2000 loss_mean: 2.7368339109420776
step: 2050 loss_mean: 2.7731972980499267
step: 2100 loss_mean: 2.713109860420227
step: 2150 loss_mean: 2.737681050300598
step: 2200 loss_mean: 2.7320258617401123
step: 2250 loss_mean: 2.7430744409561156
step: 2300 loss_mean: 2.739389967918396
step: 2350 loss_mean: 2.7606460189819337
step: 2400 loss_mean: 2.786085124015808
step: 2450 loss_mean: 2.7163069438934326
step: 2500 loss_mean: 2.7999409103393553
step: 2550 loss_mean: 2.735444941520691
step: 2600 loss_mean: 2.7133769178390503
step: 2650 loss_mean: 2.7592024755477906
step: 2700 loss_mean: 2.7641983222961426
step: 2750 loss_mean: 2.7606734895706175
step: 2800 loss_mean: 2.7413948917388917
step: 2850 loss_mean: 2.726244177818298
step: 2900 loss_mean: 2.7675787591934204
step: 2950 loss_mean: 2.707246813774109
step: 3000 loss_mean: 2.753406171798706
step: 3050 loss_mean: 2.7409043884277344
step: 3100 loss_mean: 2.7194787502288817
step: 50 loss_mean: 2.786478967666626
step: 100 loss_mean: 2.8403636980056763
step: 150 loss_mean: 2.7625337171554567
step: 200 loss_mean: 2.8687898302078247
step: 250 loss_mean: 2.8034351539611815
step: 300 loss_mean: 2.9015598487854004
step: 350 loss_mean: 2.7538912773132322
step: 400 loss_mean: 2.765279450416565
step: 450 loss_mean: 2.7587269163131714
step: 500 loss_mean: 2.8314496803283693
step: 550 loss_mean: 2.795902690887451
step: 600 loss_mean: 2.7187357187271117
Epoch: 63 | Run time: 620.0 s | Train loss: 2.74 | Valid loss: 2.8
Saved checkpoint 63 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0930 s.
run_epoch
step: 50 loss_mean: 2.7260177183151244
step: 100 loss_mean: 2.7573035860061648
step: 150 loss_mean: 2.7254625415802
step: 200 loss_mean: 2.6962261772155762
step: 250 loss_mean: 2.7863211822509766
step: 300 loss_mean: 2.752635669708252
step: 350 loss_mean: 2.7143866777420045
step: 400 loss_mean: 2.7485345220565796
step: 450 loss_mean: 2.7570909070968628
step: 500 loss_mean: 2.7334624242782595
step: 550 loss_mean: 2.7450213193893434
step: 600 loss_mean: 2.747895164489746
step: 650 loss_mean: 2.7462028789520265
step: 700 loss_mean: 2.7113893938064577
step: 750 loss_mean: 2.6983230447769166
step: 800 loss_mean: 2.722691617012024
step: 850 loss_mean: 2.7490202140808107
step: 900 loss_mean: 2.7161874532699586
step: 950 loss_mean: 2.7432902431488038
step: 1000 loss_mean: 2.748007445335388
step: 1050 loss_mean: 2.761106309890747
step: 1100 loss_mean: 2.714393754005432
step: 1150 loss_mean: 2.737554836273193
step: 1200 loss_mean: 2.7619242525100707
step: 1250 loss_mean: 2.689567890167236
step: 1300 loss_mean: 2.68633620262146
step: 1350 loss_mean: 2.7145882654190063
step: 1400 loss_mean: 2.7949201393127443
step: 1450 loss_mean: 2.754300744533539
step: 1500 loss_mean: 2.7288754034042357
step: 1550 loss_mean: 2.7067148661613465
step: 1600 loss_mean: 2.6663906240463255
step: 1650 loss_mean: 2.744063725471497
step: 1700 loss_mean: 2.807506980895996
step: 1750 loss_mean: 2.703184237480164
step: 1800 loss_mean: 2.7078881549835203
step: 1850 loss_mean: 2.757624650001526
step: 1900 loss_mean: 2.7536923027038576
step: 1950 loss_mean: 2.7524196434020998
step: 2000 loss_mean: 2.7149102592468264
step: 2050 loss_mean: 2.747988405227661
step: 2100 loss_mean: 2.7315011405944825
step: 2150 loss_mean: 2.8012225580215455
step: 2200 loss_mean: 2.7690792512893676
step: 2250 loss_mean: 2.7186115026474
step: 2300 loss_mean: 2.7682660531997683
step: 2350 loss_mean: 2.7618214082717896
step: 2400 loss_mean: 2.7540311717987063
step: 2450 loss_mean: 2.7550018882751464
step: 2500 loss_mean: 2.7762262535095217
step: 2550 loss_mean: 2.751730690002441
step: 2600 loss_mean: 2.7302928161621094
step: 2650 loss_mean: 2.723952989578247
step: 2700 loss_mean: 2.77063898563385
step: 2750 loss_mean: 2.7060982608795165
step: 2800 loss_mean: 2.718631553649902
step: 2850 loss_mean: 2.771376724243164
step: 2900 loss_mean: 2.753685989379883
step: 2950 loss_mean: 2.73743926525116
step: 3000 loss_mean: 2.6804567098617555
step: 3050 loss_mean: 2.7100690460205077
step: 3100 loss_mean: 2.686075167655945
step: 50 loss_mean: 2.758542561531067
step: 100 loss_mean: 2.8020673274993895
step: 150 loss_mean: 2.7383643579483032
step: 200 loss_mean: 2.82403790473938
step: 250 loss_mean: 2.7727154731750487
step: 300 loss_mean: 2.872151131629944
step: 350 loss_mean: 2.7247800254821777
step: 400 loss_mean: 2.747152042388916
step: 450 loss_mean: 2.7435412645339965
step: 500 loss_mean: 2.7891477966308593
step: 550 loss_mean: 2.75213463306427
step: 600 loss_mean: 2.7030145740509033
Epoch: 64 | Run time: 618.0 s | Train loss: 2.74 | Valid loss: 2.77
Saved checkpoint 64 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0942 s.
run_epoch
step: 50 loss_mean: 2.765409302711487
step: 100 loss_mean: 2.743713445663452
step: 150 loss_mean: 2.7282250928878784
step: 200 loss_mean: 2.692272300720215
step: 250 loss_mean: 2.721773099899292
step: 300 loss_mean: 2.7361987447738647
step: 350 loss_mean: 2.7227163410186765
step: 400 loss_mean: 2.7327257585525513
step: 450 loss_mean: 2.744404368400574
step: 500 loss_mean: 2.756001286506653
step: 550 loss_mean: 2.705922408103943
step: 600 loss_mean: 2.684237837791443
step: 650 loss_mean: 2.7051481771469117
step: 700 loss_mean: 2.7816876983642578
step: 750 loss_mean: 2.7190194511413575
step: 800 loss_mean: 2.8164788484573364
step: 850 loss_mean: 2.712803020477295
step: 900 loss_mean: 2.740987286567688
step: 950 loss_mean: 2.712797327041626
step: 1000 loss_mean: 2.7234703969955443
step: 1050 loss_mean: 2.7965160846710204
step: 1100 loss_mean: 2.7410203552246095
step: 1150 loss_mean: 2.777891073226929
step: 1200 loss_mean: 2.7626040506362917
step: 1250 loss_mean: 2.714730806350708
step: 1300 loss_mean: 2.759401936531067
step: 1350 loss_mean: 2.709164605140686
step: 1400 loss_mean: 2.736178379058838
step: 1450 loss_mean: 2.7343585681915283
step: 1500 loss_mean: 2.70526997089386
step: 1550 loss_mean: 2.738853597640991
step: 1600 loss_mean: 2.7477416515350344
step: 1650 loss_mean: 2.6680793237686156
step: 1700 loss_mean: 2.728371195793152
step: 1750 loss_mean: 2.75905788898468
step: 1800 loss_mean: 2.786212291717529
step: 1850 loss_mean: 2.758346872329712
step: 1900 loss_mean: 2.7929794073104857
step: 1950 loss_mean: 2.7584022855758668
step: 2000 loss_mean: 2.6881000328063966
step: 2050 loss_mean: 2.66426718711853
step: 2100 loss_mean: 2.651545886993408
step: 2150 loss_mean: 2.7533154296875
step: 2200 loss_mean: 2.768061304092407
step: 2250 loss_mean: 2.7611697006225584
step: 2300 loss_mean: 2.7879594802856444
step: 2350 loss_mean: 2.6967095279693605
step: 2400 loss_mean: 2.6653067874908447
step: 2450 loss_mean: 2.7330256032943727
step: 2500 loss_mean: 2.72915123462677
step: 2550 loss_mean: 2.7407515716552733
step: 2600 loss_mean: 2.759020013809204
step: 2650 loss_mean: 2.754295973777771
step: 2700 loss_mean: 2.741252522468567
step: 2750 loss_mean: 2.727125020027161
step: 2800 loss_mean: 2.700131778717041
step: 2850 loss_mean: 2.7807460975646974
step: 2900 loss_mean: 2.710583348274231
step: 2950 loss_mean: 2.7358455085754394
step: 3000 loss_mean: 2.6884529542922975
step: 3050 loss_mean: 2.7336276817321776
step: 3100 loss_mean: 2.7347770404815672
step: 50 loss_mean: 2.769459776878357
step: 100 loss_mean: 2.8192204999923707
step: 150 loss_mean: 2.7395553159713746
step: 200 loss_mean: 2.8228489923477174
step: 250 loss_mean: 2.774972553253174
step: 300 loss_mean: 2.8705096530914305
step: 350 loss_mean: 2.7429575443267824
step: 400 loss_mean: 2.7407097673416136
step: 450 loss_mean: 2.7568085479736326
step: 500 loss_mean: 2.799903244972229
step: 550 loss_mean: 2.7423976278305053
step: 600 loss_mean: 2.713703055381775
Epoch: 65 | Run time: 623.0 s | Train loss: 2.73 | Valid loss: 2.78
Saved checkpoint 65 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0975 s.
run_epoch
step: 50 loss_mean: 2.699365243911743
step: 100 loss_mean: 2.7493874549865724
step: 150 loss_mean: 2.7047015690803526
step: 200 loss_mean: 2.74517786026001
step: 250 loss_mean: 2.7240654850006103
step: 300 loss_mean: 2.720642604827881
step: 350 loss_mean: 2.70081627368927
step: 400 loss_mean: 2.763683385848999
step: 450 loss_mean: 2.7649292087554933
step: 500 loss_mean: 2.717277946472168
step: 550 loss_mean: 2.676802387237549
step: 600 loss_mean: 2.7102975606918336
step: 650 loss_mean: 2.7869767713546754
step: 700 loss_mean: 2.7505302572250367
step: 750 loss_mean: 2.7399910926818847
step: 800 loss_mean: 2.683836841583252
step: 850 loss_mean: 2.7212076330184938
step: 900 loss_mean: 2.7667479276657105
step: 950 loss_mean: 2.7044923210144045
step: 1000 loss_mean: 2.7625240898132324
step: 1050 loss_mean: 2.7401606273651122
step: 1100 loss_mean: 2.690506196022034
step: 1150 loss_mean: 2.738341417312622
step: 1200 loss_mean: 2.752359972000122
step: 1250 loss_mean: 2.7696596002578735
step: 1300 loss_mean: 2.7880348443984984
step: 1350 loss_mean: 2.7600823545455935
step: 1400 loss_mean: 2.7338891983032227
step: 1450 loss_mean: 2.7900133991241454
step: 1500 loss_mean: 2.7080283784866332
step: 1550 loss_mean: 2.803851771354675
step: 1600 loss_mean: 2.7111248683929445
step: 1650 loss_mean: 2.6787156772613527
step: 1700 loss_mean: 2.7511131381988525
step: 1750 loss_mean: 2.7503414249420164
step: 1800 loss_mean: 2.670295157432556
step: 1850 loss_mean: 2.6922591495513917
step: 1900 loss_mean: 2.7488797664642335
step: 1950 loss_mean: 2.7753906631469727
step: 2000 loss_mean: 2.701021990776062
step: 2050 loss_mean: 2.7572016286849976
step: 2100 loss_mean: 2.651998405456543
step: 2150 loss_mean: 2.705064449310303
step: 2200 loss_mean: 2.725423731803894
step: 2250 loss_mean: 2.6744062280654908
step: 2300 loss_mean: 2.7170130252838134
step: 2350 loss_mean: 2.767495617866516
step: 2400 loss_mean: 2.767356824874878
step: 2450 loss_mean: 2.7293971300125124
step: 2500 loss_mean: 2.697961616516113
step: 2550 loss_mean: 2.7587908124923706
step: 2600 loss_mean: 2.71878791809082
step: 2650 loss_mean: 2.7059250354766844
step: 2700 loss_mean: 2.7049360418319703
step: 2750 loss_mean: 2.678756833076477
step: 2800 loss_mean: 2.7437650299072267
step: 2850 loss_mean: 2.7627534818649293
step: 2900 loss_mean: 2.739075503349304
step: 2950 loss_mean: 2.81717435836792
step: 3000 loss_mean: 2.744114499092102
step: 3050 loss_mean: 2.735457782745361
step: 3100 loss_mean: 2.739226613044739
step: 50 loss_mean: 2.7500995779037476
step: 100 loss_mean: 2.7853380155563356
step: 150 loss_mean: 2.7270326948165895
step: 200 loss_mean: 2.8384730434417724
step: 250 loss_mean: 2.7609743547439574
step: 300 loss_mean: 2.8630060386657714
step: 350 loss_mean: 2.7097736549377442
step: 400 loss_mean: 2.7230172824859618
step: 450 loss_mean: 2.7407996082305908
step: 500 loss_mean: 2.770380277633667
step: 550 loss_mean: 2.75382830619812
step: 600 loss_mean: 2.6830263137817383
Epoch: 66 | Run time: 614.0 s | Train loss: 2.73 | Valid loss: 2.76
Saved checkpoint 66 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0987 s.
run_epoch
step: 50 loss_mean: 2.730862135887146
step: 100 loss_mean: 2.706783423423767
step: 150 loss_mean: 2.73343713760376
step: 200 loss_mean: 2.769657187461853
step: 250 loss_mean: 2.752354974746704
step: 300 loss_mean: 2.8079509687423707
step: 350 loss_mean: 2.739701714515686
step: 400 loss_mean: 2.794580383300781
step: 450 loss_mean: 2.6829845094680786
step: 500 loss_mean: 2.7505311012268066
step: 550 loss_mean: 2.76738778591156
step: 600 loss_mean: 2.677418603897095
step: 650 loss_mean: 2.750817070007324
step: 700 loss_mean: 2.7225664472579956
step: 750 loss_mean: 2.7105101490020753
step: 800 loss_mean: 2.7290207719802857
step: 850 loss_mean: 2.726133723258972
step: 900 loss_mean: 2.7197647714614868
step: 950 loss_mean: 2.7349277687072755
step: 1000 loss_mean: 2.683960723876953
step: 1050 loss_mean: 2.703678665161133
step: 1100 loss_mean: 2.748646464347839
step: 1150 loss_mean: 2.7165612030029296
step: 1200 loss_mean: 2.7412863826751708
step: 1250 loss_mean: 2.691795463562012
step: 1300 loss_mean: 2.790606918334961
step: 1350 loss_mean: 2.750357837677002
step: 1400 loss_mean: 2.732642316818237
step: 1450 loss_mean: 2.7122052812576296
step: 1500 loss_mean: 2.705471477508545
step: 1550 loss_mean: 2.685129313468933
step: 1600 loss_mean: 2.743770160675049
step: 1650 loss_mean: 2.7458337640762327
step: 1700 loss_mean: 2.7224254512786867
step: 1750 loss_mean: 2.694824051856995
step: 1800 loss_mean: 2.7121207523345947
step: 1850 loss_mean: 2.6518573856353758
step: 1900 loss_mean: 2.7634329652786254
step: 1950 loss_mean: 2.7650757026672363
step: 2000 loss_mean: 2.7421753072738646
step: 2050 loss_mean: 2.691950492858887
step: 2100 loss_mean: 2.769530291557312
step: 2150 loss_mean: 2.7447534370422364
step: 2200 loss_mean: 2.7264477682113646
step: 2250 loss_mean: 2.710696983337402
step: 2300 loss_mean: 2.7328869915008545
step: 2350 loss_mean: 2.7400204706192017
step: 2400 loss_mean: 2.7221980905532837
step: 2450 loss_mean: 2.7309872770309447
step: 2500 loss_mean: 2.703026251792908
step: 2550 loss_mean: 2.7022643089294434
step: 2600 loss_mean: 2.815737476348877
step: 2650 loss_mean: 2.691556158065796
step: 2700 loss_mean: 2.7271216201782225
step: 2750 loss_mean: 2.7244031190872193
step: 2800 loss_mean: 2.7469200611114504
step: 2850 loss_mean: 2.7420705652236936
step: 2900 loss_mean: 2.679512176513672
step: 2950 loss_mean: 2.730256371498108
step: 3000 loss_mean: 2.720324058532715
step: 3050 loss_mean: 2.7155644607543947
step: 3100 loss_mean: 2.798155951499939
step: 50 loss_mean: 2.7665492582321165
step: 100 loss_mean: 2.8083626985549928
step: 150 loss_mean: 2.730713381767273
step: 200 loss_mean: 2.8323791646957397
step: 250 loss_mean: 2.7828757667541506
step: 300 loss_mean: 2.877460856437683
step: 350 loss_mean: 2.7262221336364747
step: 400 loss_mean: 2.7586892890930175
step: 450 loss_mean: 2.752059135437012
step: 500 loss_mean: 2.784482870101929
step: 550 loss_mean: 2.7498430156707765
step: 600 loss_mean: 2.7036659622192385
Epoch: 67 | Run time: 624.0 s | Train loss: 2.73 | Valid loss: 2.77
Saved checkpoint 67 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0988 s.
run_epoch
step: 50 loss_mean: 2.713115472793579
step: 100 loss_mean: 2.752655029296875
step: 150 loss_mean: 2.743308882713318
step: 200 loss_mean: 2.758083701133728
step: 250 loss_mean: 2.7038209772109987
step: 300 loss_mean: 2.709745988845825
step: 350 loss_mean: 2.73976767539978
step: 400 loss_mean: 2.7385821962356567
step: 450 loss_mean: 2.7608510971069338
step: 500 loss_mean: 2.744477181434631
step: 550 loss_mean: 2.7250649404525755
step: 600 loss_mean: 2.7317266941070555
step: 650 loss_mean: 2.7372841358184816
step: 700 loss_mean: 2.7556630754470826
step: 750 loss_mean: 2.7289471006393433
step: 800 loss_mean: 2.7515802240371703
step: 850 loss_mean: 2.7380256700515746
step: 900 loss_mean: 2.7452027940750123
step: 950 loss_mean: 2.7266896057128904
step: 1000 loss_mean: 2.7686135387420654
step: 1050 loss_mean: 2.7052211332321168
step: 1100 loss_mean: 2.7033638620376585
step: 1150 loss_mean: 2.7400590467453
step: 1200 loss_mean: 2.680497260093689
step: 1250 loss_mean: 2.772190685272217
step: 1300 loss_mean: 2.7050603342056276
step: 1350 loss_mean: 2.6698715448379517
step: 1400 loss_mean: 2.76980272769928
step: 1450 loss_mean: 2.6825011444091795
step: 1500 loss_mean: 2.7418721103668213
step: 1550 loss_mean: 2.6985433864593507
step: 1600 loss_mean: 2.6884652614593505
step: 1650 loss_mean: 2.747713179588318
step: 1700 loss_mean: 2.7364604377746584
step: 1750 loss_mean: 2.7100196743011473
step: 1800 loss_mean: 2.751938500404358
step: 1850 loss_mean: 2.7460547399520876
step: 1900 loss_mean: 2.710808696746826
step: 1950 loss_mean: 2.722431960105896
step: 2000 loss_mean: 2.7147204208374025
step: 2050 loss_mean: 2.70697359085083
step: 2100 loss_mean: 2.7703149700164795
step: 2150 loss_mean: 2.7428187990188597
step: 2200 loss_mean: 2.723770270347595
step: 2250 loss_mean: 2.769236283302307
step: 2300 loss_mean: 2.7372642660140993
step: 2350 loss_mean: 2.7689713859558105
step: 2400 loss_mean: 2.7054797887802122
step: 2450 loss_mean: 2.7231265354156493
step: 2500 loss_mean: 2.7343647956848143
step: 2550 loss_mean: 2.732309913635254
step: 2600 loss_mean: 2.783921046257019
step: 2650 loss_mean: 2.7431189680099486
step: 2700 loss_mean: 2.709825758934021
step: 2750 loss_mean: 2.678303112983704
step: 2800 loss_mean: 2.7278509616851805
step: 2850 loss_mean: 2.7578197383880614
step: 2900 loss_mean: 2.742431287765503
step: 2950 loss_mean: 2.7176056861877442
step: 3000 loss_mean: 2.704620990753174
step: 3050 loss_mean: 2.7555175685882567
step: 3100 loss_mean: 2.7269155025482177
step: 50 loss_mean: 2.778894820213318
step: 100 loss_mean: 2.8079088497161866
step: 150 loss_mean: 2.7510275268554687
step: 200 loss_mean: 2.861913800239563
step: 250 loss_mean: 2.7896011209487916
step: 300 loss_mean: 2.913211498260498
step: 350 loss_mean: 2.7209256649017335
step: 400 loss_mean: 2.729750051498413
step: 450 loss_mean: 2.761343412399292
step: 500 loss_mean: 2.798414697647095
step: 550 loss_mean: 2.7759040927886964
step: 600 loss_mean: 2.7060988044738767
Epoch: 68 | Run time: 622.0 s | Train loss: 2.73 | Valid loss: 2.78
Saved checkpoint 68 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.0999 s.
run_epoch
step: 50 loss_mean: 2.760482625961304
step: 100 loss_mean: 2.777184634208679
step: 150 loss_mean: 2.7744795417785646
step: 200 loss_mean: 2.6966578006744384
step: 250 loss_mean: 2.74470787525177
step: 300 loss_mean: 2.706519365310669
step: 350 loss_mean: 2.749082522392273
step: 400 loss_mean: 2.663735046386719
step: 450 loss_mean: 2.752525653839111
step: 500 loss_mean: 2.6939482831954957
step: 550 loss_mean: 2.7994287633895873
step: 600 loss_mean: 2.7062379932403564
step: 650 loss_mean: 2.750122365951538
step: 700 loss_mean: 2.7500173091888427
step: 750 loss_mean: 2.713441548347473
step: 800 loss_mean: 2.665166554450989
step: 850 loss_mean: 2.6499021434783936
step: 900 loss_mean: 2.7505639362335206
step: 950 loss_mean: 2.7173040246963502
step: 1000 loss_mean: 2.719302248954773
step: 1050 loss_mean: 2.7192652320861814
step: 1100 loss_mean: 2.7903442335128785
step: 1150 loss_mean: 2.812035388946533
step: 1200 loss_mean: 2.7433172416687013
step: 1250 loss_mean: 2.7653556728363036
step: 1300 loss_mean: 2.6721798896789553
step: 1350 loss_mean: 2.7578355073928833
step: 1400 loss_mean: 2.7429149532318116
step: 1450 loss_mean: 2.7390450286865233
step: 1500 loss_mean: 2.7483510303497316
step: 1550 loss_mean: 2.704191040992737
step: 1600 loss_mean: 2.6869168090820312
step: 1650 loss_mean: 2.677637720108032
step: 1700 loss_mean: 2.704909324645996
step: 1750 loss_mean: 2.7553340721130373
step: 1800 loss_mean: 2.7544957637786864
step: 1850 loss_mean: 2.780712890625
step: 1900 loss_mean: 2.764846153259277
step: 1950 loss_mean: 2.7273075342178346
step: 2000 loss_mean: 2.7392266035079955
step: 2050 loss_mean: 2.71155396938324
step: 2100 loss_mean: 2.7410642194747923
step: 2150 loss_mean: 2.763780002593994
step: 2200 loss_mean: 2.732939796447754
step: 2250 loss_mean: 2.6956612730026244
step: 2300 loss_mean: 2.732918453216553
step: 2350 loss_mean: 2.7121688604354857
step: 2400 loss_mean: 2.754735326766968
step: 2450 loss_mean: 2.760761103630066
step: 2500 loss_mean: 2.714221453666687
step: 2550 loss_mean: 2.700193190574646
step: 2600 loss_mean: 2.7541141223907473
step: 2650 loss_mean: 2.717899513244629
step: 2700 loss_mean: 2.6891431427001953
step: 2750 loss_mean: 2.674725251197815
step: 2800 loss_mean: 2.7627122926712038
step: 2850 loss_mean: 2.7036901712417603
step: 2900 loss_mean: 2.7601429271697997
step: 2950 loss_mean: 2.7655019092559816
step: 3000 loss_mean: 2.795637001991272
step: 3050 loss_mean: 2.744671940803528
step: 3100 loss_mean: 2.694452428817749
step: 50 loss_mean: 2.7649889039993285
step: 100 loss_mean: 2.8009387731552122
step: 150 loss_mean: 2.7191044759750365
step: 200 loss_mean: 2.8401520681381225
step: 250 loss_mean: 2.7794412994384765
step: 300 loss_mean: 2.8698489332199095
step: 350 loss_mean: 2.7339628744125366
step: 400 loss_mean: 2.749848256111145
step: 450 loss_mean: 2.733872818946838
step: 500 loss_mean: 2.786651258468628
step: 550 loss_mean: 2.7343721103668215
step: 600 loss_mean: 2.6811637592315676
Epoch: 69 | Run time: 622.0 s | Train loss: 2.73 | Valid loss: 2.77
Saved checkpoint 69 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1007 s.
run_epoch
step: 50 loss_mean: 2.720690484046936
step: 100 loss_mean: 2.77788191318512
step: 150 loss_mean: 2.7146704149246217
step: 200 loss_mean: 2.7357306432724
step: 250 loss_mean: 2.695330457687378
step: 300 loss_mean: 2.739378228187561
step: 350 loss_mean: 2.687820086479187
step: 400 loss_mean: 2.7437013816833495
step: 450 loss_mean: 2.765693483352661
step: 500 loss_mean: 2.7577019929885864
step: 550 loss_mean: 2.7483261680603026
step: 600 loss_mean: 2.714655427932739
step: 650 loss_mean: 2.725278687477112
step: 700 loss_mean: 2.7259909439086916
step: 750 loss_mean: 2.6962003326416015
step: 800 loss_mean: 2.7121083402633666
step: 850 loss_mean: 2.710536298751831
step: 900 loss_mean: 2.6956784772872924
step: 950 loss_mean: 2.7351908254623414
step: 1000 loss_mean: 2.7439061069488524
step: 1050 loss_mean: 2.7589468145370484
step: 1100 loss_mean: 2.724321236610413
step: 1150 loss_mean: 2.6704095649719237
step: 1200 loss_mean: 2.687931604385376
step: 1250 loss_mean: 2.7679687547683716
step: 1300 loss_mean: 2.73381911277771
step: 1350 loss_mean: 2.7371646690368654
step: 1400 loss_mean: 2.739291481971741
step: 1450 loss_mean: 2.737355546951294
step: 1500 loss_mean: 2.6619413232803346
step: 1550 loss_mean: 2.720045404434204
step: 1600 loss_mean: 2.7426010274887087
step: 1650 loss_mean: 2.745750403404236
step: 1700 loss_mean: 2.6778587245941163
step: 1750 loss_mean: 2.7281754446029662
step: 1800 loss_mean: 2.717715711593628
step: 1850 loss_mean: 2.7353434991836547
step: 1900 loss_mean: 2.7199965620040896
step: 1950 loss_mean: 2.7121192693710325
step: 2000 loss_mean: 2.7274650001525877
step: 2050 loss_mean: 2.7870311832427976
step: 2100 loss_mean: 2.717206587791443
step: 2150 loss_mean: 2.667742147445679
step: 2200 loss_mean: 2.774786310195923
step: 2250 loss_mean: 2.7843869590759276
step: 2300 loss_mean: 2.724579358100891
step: 2350 loss_mean: 2.6619824171066284
step: 2400 loss_mean: 2.7878650999069214
step: 2450 loss_mean: 2.70202467918396
step: 2500 loss_mean: 2.7582685375213623
step: 2550 loss_mean: 2.7555053567886354
step: 2600 loss_mean: 2.722155361175537
step: 2650 loss_mean: 2.72892746925354
step: 2700 loss_mean: 2.726242141723633
step: 2750 loss_mean: 2.757390675544739
step: 2800 loss_mean: 2.7227009201049803
step: 2850 loss_mean: 2.840791869163513
step: 2900 loss_mean: 2.7729886293411257
step: 2950 loss_mean: 2.728543801307678
step: 3000 loss_mean: 2.7358326625823977
step: 3050 loss_mean: 2.72645959854126
step: 3100 loss_mean: 2.7443904209136964
step: 50 loss_mean: 2.7610899686813353
step: 100 loss_mean: 2.797467784881592
step: 150 loss_mean: 2.7468736600875854
step: 200 loss_mean: 2.8492375230789184
step: 250 loss_mean: 2.769200530052185
step: 300 loss_mean: 2.880564250946045
step: 350 loss_mean: 2.724508318901062
step: 400 loss_mean: 2.7396046257019044
step: 450 loss_mean: 2.748025965690613
step: 500 loss_mean: 2.7903285932540896
step: 550 loss_mean: 2.7662344360351563
step: 600 loss_mean: 2.6916917181015014
Epoch: 70 | Run time: 622.0 s | Train loss: 2.73 | Valid loss: 2.77
Saved checkpoint 70 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1022 s.
run_epoch
step: 50 loss_mean: 2.708712544441223
step: 100 loss_mean: 2.7129925870895386
step: 150 loss_mean: 2.7121916151046754
step: 200 loss_mean: 2.722716908454895
step: 250 loss_mean: 2.697387833595276
step: 300 loss_mean: 2.72257794380188
step: 350 loss_mean: 2.709986910820007
step: 400 loss_mean: 2.7424933433532717
step: 450 loss_mean: 2.7495587682724
step: 500 loss_mean: 2.7175882625579835
step: 550 loss_mean: 2.7652981948852537
step: 600 loss_mean: 2.7150296449661253
step: 650 loss_mean: 2.7756706285476684
step: 700 loss_mean: 2.7445441341400145
step: 750 loss_mean: 2.7171815490722655
step: 800 loss_mean: 2.7064931058883666
step: 850 loss_mean: 2.7345774459838865
step: 900 loss_mean: 2.749198942184448
step: 950 loss_mean: 2.743931698799133
step: 1000 loss_mean: 2.7394048881530764
step: 1050 loss_mean: 2.776910161972046
step: 1100 loss_mean: 2.706292757987976
step: 1150 loss_mean: 2.766659445762634
step: 1200 loss_mean: 2.7448166036605834
step: 1250 loss_mean: 2.7516824626922607
step: 1300 loss_mean: 2.677607421875
step: 1350 loss_mean: 2.7153610515594484
step: 1400 loss_mean: 2.686879024505615
step: 1450 loss_mean: 2.7347355556488036
step: 1500 loss_mean: 2.7065439414978028
step: 1550 loss_mean: 2.753190588951111
step: 1600 loss_mean: 2.684228501319885
step: 1650 loss_mean: 2.72778546333313
step: 1700 loss_mean: 2.772367124557495
step: 1750 loss_mean: 2.7149792671203614
step: 1800 loss_mean: 2.70539204120636
step: 1850 loss_mean: 2.7371476602554323
step: 1900 loss_mean: 2.7808916759490967
step: 1950 loss_mean: 2.749584016799927
step: 2000 loss_mean: 2.7587229347229005
step: 2050 loss_mean: 2.7698574352264402
step: 2100 loss_mean: 2.7342518186569214
step: 2150 loss_mean: 2.76547447681427
step: 2200 loss_mean: 2.6960200119018554
step: 2250 loss_mean: 2.6766226720809936
step: 2300 loss_mean: 2.764226198196411
step: 2350 loss_mean: 2.6841360473632814
step: 2400 loss_mean: 2.7311514091491698
step: 2450 loss_mean: 2.731369595527649
step: 2500 loss_mean: 2.696094536781311
step: 2550 loss_mean: 2.7689032697677614
step: 2600 loss_mean: 2.676186180114746
step: 2650 loss_mean: 2.736643433570862
step: 2700 loss_mean: 2.7349616479873657
step: 2750 loss_mean: 2.69588369846344
step: 2800 loss_mean: 2.6336280274391175
step: 2850 loss_mean: 2.733820328712463
step: 2900 loss_mean: 2.6966025114059446
step: 2950 loss_mean: 2.7898140954971313
step: 3000 loss_mean: 2.740498080253601
step: 3050 loss_mean: 2.754442505836487
step: 3100 loss_mean: 2.732087550163269
step: 50 loss_mean: 2.747216215133667
step: 100 loss_mean: 2.7840636014938354
step: 150 loss_mean: 2.723659243583679
step: 200 loss_mean: 2.824078483581543
step: 250 loss_mean: 2.7570827436447143
step: 300 loss_mean: 2.862690782546997
step: 350 loss_mean: 2.727953243255615
step: 400 loss_mean: 2.709460997581482
step: 450 loss_mean: 2.721299576759338
step: 500 loss_mean: 2.784651560783386
step: 550 loss_mean: 2.737091588973999
step: 600 loss_mean: 2.687674140930176
Epoch: 71 | Run time: 623.0 s | Train loss: 2.73 | Valid loss: 2.76
Saved checkpoint 71 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1041 s.
run_epoch
step: 50 loss_mean: 2.725173010826111
step: 100 loss_mean: 2.737460479736328
step: 150 loss_mean: 2.7032779359817507
step: 200 loss_mean: 2.7097844886779785
step: 250 loss_mean: 2.632172384262085
step: 300 loss_mean: 2.7329938411712646
step: 350 loss_mean: 2.7553827142715455
step: 400 loss_mean: 2.760891151428223
step: 450 loss_mean: 2.7173264980316163
step: 500 loss_mean: 2.755540928840637
step: 550 loss_mean: 2.7461093997955324
step: 600 loss_mean: 2.758452501296997
step: 650 loss_mean: 2.758965482711792
step: 700 loss_mean: 2.6894128036499025
step: 750 loss_mean: 2.7335367441177367
step: 800 loss_mean: 2.7256936120986937
step: 850 loss_mean: 2.733115386962891
step: 900 loss_mean: 2.667916188240051
step: 950 loss_mean: 2.7809522914886475
step: 1000 loss_mean: 2.675605535507202
step: 1050 loss_mean: 2.685137348175049
step: 1100 loss_mean: 2.686406383514404
step: 1150 loss_mean: 2.7405204153060914
step: 1200 loss_mean: 2.720993633270264
step: 1250 loss_mean: 2.7641695594787596
step: 1300 loss_mean: 2.743438096046448
step: 1350 loss_mean: 2.741681818962097
step: 1400 loss_mean: 2.7191456031799315
step: 1450 loss_mean: 2.6886693382263185
step: 1500 loss_mean: 2.7202628564834597
step: 1550 loss_mean: 2.742004842758179
step: 1600 loss_mean: 2.703622555732727
step: 1650 loss_mean: 2.7701290369033815
step: 1700 loss_mean: 2.7423666381835936
step: 1750 loss_mean: 2.7434268379211426
step: 1800 loss_mean: 2.7273860120773317
step: 1850 loss_mean: 2.7017461395263673
step: 1900 loss_mean: 2.701897692680359
step: 1950 loss_mean: 2.7541149759292605
step: 2000 loss_mean: 2.7063030099868772
step: 2050 loss_mean: 2.74662024974823
step: 2100 loss_mean: 2.7472439050674438
step: 2150 loss_mean: 2.7083736419677735
step: 2200 loss_mean: 2.7545740270614623
step: 2250 loss_mean: 2.729726586341858
step: 2300 loss_mean: 2.7677760028839113
step: 2350 loss_mean: 2.711515326499939
step: 2400 loss_mean: 2.718610615730286
step: 2450 loss_mean: 2.7342744207382204
step: 2500 loss_mean: 2.736804361343384
step: 2550 loss_mean: 2.701432147026062
step: 2600 loss_mean: 2.6756393098831177
step: 2650 loss_mean: 2.7538151502609254
step: 2700 loss_mean: 2.718036093711853
step: 2750 loss_mean: 2.7123299837112427
step: 2800 loss_mean: 2.7217748737335206
step: 2850 loss_mean: 2.7385986995697023
step: 2900 loss_mean: 2.708733139038086
step: 2950 loss_mean: 2.7671201705932615
step: 3000 loss_mean: 2.759371156692505
step: 3050 loss_mean: 2.7567693614959716
step: 3100 loss_mean: 2.7051101207733153
step: 50 loss_mean: 2.776081862449646
step: 100 loss_mean: 2.8363163089752197
step: 150 loss_mean: 2.750253505706787
step: 200 loss_mean: 2.8570459127426147
step: 250 loss_mean: 2.782598309516907
step: 300 loss_mean: 2.891924805641174
step: 350 loss_mean: 2.7485231256484983
step: 400 loss_mean: 2.763676505088806
step: 450 loss_mean: 2.75846951007843
step: 500 loss_mean: 2.8164319467544554
step: 550 loss_mean: 2.762993559837341
step: 600 loss_mean: 2.709605140686035
Epoch: 72 | Run time: 621.0 s | Train loss: 2.73 | Valid loss: 2.79
Saved checkpoint 72 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1057 s.
run_epoch
step: 50 loss_mean: 2.7203028202056885
step: 100 loss_mean: 2.7329889965057372
step: 150 loss_mean: 2.7166684293746948
step: 200 loss_mean: 2.692702569961548
step: 250 loss_mean: 2.6948478746414186
step: 300 loss_mean: 2.7150743532180788
step: 350 loss_mean: 2.725496196746826
step: 400 loss_mean: 2.667631731033325
step: 450 loss_mean: 2.7227599000930787
step: 500 loss_mean: 2.763210344314575
step: 550 loss_mean: 2.731400184631348
step: 600 loss_mean: 2.733434681892395
step: 650 loss_mean: 2.768848443031311
step: 700 loss_mean: 2.7412656593322753
step: 750 loss_mean: 2.7073367500305174
step: 800 loss_mean: 2.7232590389251707
step: 850 loss_mean: 2.721846036911011
step: 900 loss_mean: 2.797207455635071
step: 950 loss_mean: 2.71810715675354
step: 1000 loss_mean: 2.7069280767440795
step: 1050 loss_mean: 2.6938031005859373
step: 1100 loss_mean: 2.687362914085388
step: 1150 loss_mean: 2.726013569831848
step: 1200 loss_mean: 2.704849195480347
step: 1250 loss_mean: 2.7847789096832276
step: 1300 loss_mean: 2.7003860521316527
step: 1350 loss_mean: 2.743106331825256
step: 1400 loss_mean: 2.6677282285690307
step: 1450 loss_mean: 2.7474526739120484
step: 1500 loss_mean: 2.7325402688980103
step: 1550 loss_mean: 2.751192846298218
step: 1600 loss_mean: 2.7643235397338866
step: 1650 loss_mean: 2.6767854738235473
step: 1700 loss_mean: 2.707339491844177
step: 1750 loss_mean: 2.733708591461182
step: 1800 loss_mean: 2.7716473054885866
step: 1850 loss_mean: 2.7346843814849855
step: 1900 loss_mean: 2.7519989442825317
step: 1950 loss_mean: 2.7125081396102906
step: 2000 loss_mean: 2.720552225112915
step: 2050 loss_mean: 2.697360610961914
step: 2100 loss_mean: 2.7366040468215944
step: 2150 loss_mean: 2.7790008926391603
step: 2200 loss_mean: 2.7451517391204834
step: 2250 loss_mean: 2.7279243659973145
step: 2300 loss_mean: 2.6759694528579714
step: 2350 loss_mean: 2.735986466407776
step: 2400 loss_mean: 2.7593083906173708
step: 2450 loss_mean: 2.677112867832184
step: 2500 loss_mean: 2.7199863815307617
step: 2550 loss_mean: 2.765760350227356
step: 2600 loss_mean: 2.7478562211990356
step: 2650 loss_mean: 2.695346302986145
step: 2700 loss_mean: 2.7254569005966185
step: 2750 loss_mean: 2.796587796211243
step: 2800 loss_mean: 2.7313137245178223
step: 2850 loss_mean: 2.726230263710022
step: 2900 loss_mean: 2.676867332458496
step: 2950 loss_mean: 2.707009310722351
step: 3000 loss_mean: 2.7174851751327513
step: 3050 loss_mean: 2.6787240362167357
step: 3100 loss_mean: 2.7500974130630493
step: 50 loss_mean: 2.7608319902420044
step: 100 loss_mean: 2.8086589193344116
step: 150 loss_mean: 2.737097964286804
step: 200 loss_mean: 2.838449625968933
step: 250 loss_mean: 2.7747774505615235
step: 300 loss_mean: 2.867087845802307
step: 350 loss_mean: 2.730163531303406
step: 400 loss_mean: 2.742035083770752
step: 450 loss_mean: 2.7397999000549316
step: 500 loss_mean: 2.7776603174209593
step: 550 loss_mean: 2.7439752578735352
step: 600 loss_mean: 2.696585202217102
Epoch: 73 | Run time: 623.0 s | Train loss: 2.73 | Valid loss: 2.77
Saved checkpoint 73 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1078 s.
run_epoch
step: 50 loss_mean: 2.73404531955719
step: 100 loss_mean: 2.7271244859695436
step: 150 loss_mean: 2.734215817451477
step: 200 loss_mean: 2.663980121612549
step: 250 loss_mean: 2.7297112846374514
step: 300 loss_mean: 2.7439602851867675
step: 350 loss_mean: 2.689236588478088
step: 400 loss_mean: 2.6803584051132203
step: 450 loss_mean: 2.7136639022827147
step: 500 loss_mean: 2.7439722919464113
step: 550 loss_mean: 2.7360838270187378
step: 600 loss_mean: 2.7661146688461304
step: 650 loss_mean: 2.759417028427124
step: 700 loss_mean: 2.6982987689971925
step: 750 loss_mean: 2.7708711719512937
step: 800 loss_mean: 2.6961393404006957
step: 850 loss_mean: 2.7454644727706907
step: 900 loss_mean: 2.7080940341949464
step: 950 loss_mean: 2.7061045217514037
step: 1000 loss_mean: 2.7794778871536256
step: 1050 loss_mean: 2.659257616996765
step: 1100 loss_mean: 2.7110853672027586
step: 1150 loss_mean: 2.7145696544647215
step: 1200 loss_mean: 2.7197388362884523
step: 1250 loss_mean: 2.684132251739502
step: 1300 loss_mean: 2.7234636783599853
step: 1350 loss_mean: 2.7837099838256836
step: 1400 loss_mean: 2.7730091190338135
step: 1450 loss_mean: 2.7134239959716795
step: 1500 loss_mean: 2.708168821334839
step: 1550 loss_mean: 2.715631675720215
step: 1600 loss_mean: 2.7665835762023927
step: 1650 loss_mean: 2.717108302116394
step: 1700 loss_mean: 2.681272339820862
step: 1750 loss_mean: 2.7002245140075685
step: 1800 loss_mean: 2.7553970336914064
step: 1850 loss_mean: 2.715956115722656
step: 1900 loss_mean: 2.765395121574402
step: 1950 loss_mean: 2.710736799240112
step: 2000 loss_mean: 2.7662789964675905
step: 2050 loss_mean: 2.7248999881744385
step: 2100 loss_mean: 2.7256775426864626
step: 2150 loss_mean: 2.6757477951049804
step: 2200 loss_mean: 2.6941438722610473
step: 2250 loss_mean: 2.7158823156356813
step: 2300 loss_mean: 2.7596114015579225
step: 2350 loss_mean: 2.725548415184021
step: 2400 loss_mean: 2.726024408340454
step: 2450 loss_mean: 2.708222255706787
step: 2500 loss_mean: 2.7197278690338136
step: 2550 loss_mean: 2.673151845932007
step: 2600 loss_mean: 2.740749144554138
step: 2650 loss_mean: 2.7128694009780885
step: 2700 loss_mean: 2.684346981048584
step: 2750 loss_mean: 2.694077706336975
step: 2800 loss_mean: 2.7109877634048463
step: 2850 loss_mean: 2.751786904335022
step: 2900 loss_mean: 2.7231018781661986
step: 2950 loss_mean: 2.700831289291382
step: 3000 loss_mean: 2.7265274000167845
step: 3050 loss_mean: 2.779447326660156
step: 3100 loss_mean: 2.7358789014816285
step: 50 loss_mean: 2.752568311691284
step: 100 loss_mean: 2.786750693321228
step: 150 loss_mean: 2.7248399448394776
step: 200 loss_mean: 2.8225334692001343
step: 250 loss_mean: 2.764395623207092
step: 300 loss_mean: 2.8568211460113524
step: 350 loss_mean: 2.7127614498138426
step: 400 loss_mean: 2.756235523223877
step: 450 loss_mean: 2.742075114250183
step: 500 loss_mean: 2.7776214838027955
step: 550 loss_mean: 2.7418829345703126
step: 600 loss_mean: 2.7062650775909423
Epoch: 74 | Run time: 682.0 s | Train loss: 2.72 | Valid loss: 2.76
Saved checkpoint 74 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1084 s.
run_epoch
step: 50 loss_mean: 2.738796591758728
step: 100 loss_mean: 2.6910641384124756
step: 150 loss_mean: 2.709498279094696
step: 200 loss_mean: 2.7523234415054323
step: 250 loss_mean: 2.766470990180969
step: 300 loss_mean: 2.6732267570495605
step: 350 loss_mean: 2.6808603048324584
step: 400 loss_mean: 2.6988396549224856
step: 450 loss_mean: 2.69057373046875
step: 500 loss_mean: 2.7434749031066894
step: 550 loss_mean: 2.7743217611312865
step: 600 loss_mean: 2.7142277765274048
step: 650 loss_mean: 2.7685306882858276
step: 700 loss_mean: 2.7309741830825804
step: 750 loss_mean: 2.7341472864151
step: 800 loss_mean: 2.728581848144531
step: 850 loss_mean: 2.749403381347656
step: 900 loss_mean: 2.737800178527832
step: 950 loss_mean: 2.7213392162323
step: 1000 loss_mean: 2.6888506317138674
step: 1050 loss_mean: 2.749429087638855
step: 1100 loss_mean: 2.7440213346481324
step: 1150 loss_mean: 2.6766579294204713
step: 1200 loss_mean: 2.656909952163696
step: 1250 loss_mean: 2.691668791770935
step: 1300 loss_mean: 2.7452174377441407
step: 1350 loss_mean: 2.6838161373138427
step: 1400 loss_mean: 2.722352771759033
step: 1450 loss_mean: 2.671302819252014
step: 1500 loss_mean: 2.742383131980896
step: 1550 loss_mean: 2.695300660133362
step: 1600 loss_mean: 2.7463880252838133
step: 1650 loss_mean: 2.737175536155701
step: 1700 loss_mean: 2.700829224586487
step: 1750 loss_mean: 2.768026785850525
step: 1800 loss_mean: 2.746857500076294
step: 1850 loss_mean: 2.731792812347412
step: 1900 loss_mean: 2.758831706047058
step: 1950 loss_mean: 2.798498330116272
step: 2000 loss_mean: 2.7518337106704713
step: 2050 loss_mean: 2.6870984506607054
step: 2100 loss_mean: 2.6994739294052126
step: 2150 loss_mean: 2.7444490432739257
step: 2200 loss_mean: 2.6966268396377564
step: 2250 loss_mean: 2.6958815908432006
step: 2300 loss_mean: 2.727441682815552
step: 2350 loss_mean: 2.7061050367355346
step: 2400 loss_mean: 2.732438554763794
step: 2450 loss_mean: 2.6474093079566954
step: 2500 loss_mean: 2.7118230199813844
step: 2550 loss_mean: 2.7324884843826296
step: 2600 loss_mean: 2.726512999534607
step: 2650 loss_mean: 2.7014776849746704
step: 2700 loss_mean: 2.703225450515747
step: 2750 loss_mean: 2.769236807823181
step: 2800 loss_mean: 2.7674340200424195
step: 2850 loss_mean: 2.742224025726318
step: 2900 loss_mean: 2.734879274368286
step: 2950 loss_mean: 2.7085452556610106
step: 3000 loss_mean: 2.7174081993103028
step: 3050 loss_mean: 2.7114134550094606
step: 3100 loss_mean: 2.7076631593704223
step: 50 loss_mean: 2.7406145763397216
step: 100 loss_mean: 2.7763177156448364
step: 150 loss_mean: 2.7175505924224854
step: 200 loss_mean: 2.804348750114441
step: 250 loss_mean: 2.751813111305237
step: 300 loss_mean: 2.8412012577056887
step: 350 loss_mean: 2.7026223278045656
step: 400 loss_mean: 2.7356258630752563
step: 450 loss_mean: 2.7232839632034302
step: 500 loss_mean: 2.755056285858154
step: 550 loss_mean: 2.7309955167770386
step: 600 loss_mean: 2.6819325065612794
Epoch: 75 | Run time: 619.0 s | Train loss: 2.72 | Valid loss: 2.75
Saved checkpoint 75 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1093 s.
run_epoch
step: 50 loss_mean: 2.7229996633529665
step: 100 loss_mean: 2.682958650588989
step: 150 loss_mean: 2.7505513191223145
step: 200 loss_mean: 2.706295871734619
step: 250 loss_mean: 2.7849613904953
step: 300 loss_mean: 2.646859359741211
step: 350 loss_mean: 2.691957950592041
step: 400 loss_mean: 2.709290976524353
step: 450 loss_mean: 2.7256123876571654
step: 500 loss_mean: 2.6777214431762695
step: 550 loss_mean: 2.7484569120407105
step: 600 loss_mean: 2.759139084815979
step: 650 loss_mean: 2.7293771934509277
step: 700 loss_mean: 2.69778564453125
step: 750 loss_mean: 2.743602123260498
step: 800 loss_mean: 2.7556480026245116
step: 850 loss_mean: 2.732301478385925
step: 900 loss_mean: 2.775228419303894
step: 950 loss_mean: 2.7067869472503663
step: 1000 loss_mean: 2.7783326435089113
step: 1050 loss_mean: 2.664766550064087
step: 1100 loss_mean: 2.6785676288604736
step: 1150 loss_mean: 2.704251208305359
step: 1200 loss_mean: 2.7461919021606445
step: 1250 loss_mean: 2.7405633544921875
step: 1300 loss_mean: 2.7509113359451294
step: 1350 loss_mean: 2.737271537780762
step: 1400 loss_mean: 2.7115761470794677
step: 1450 loss_mean: 2.68110520362854
step: 1500 loss_mean: 2.7458953046798706
step: 1550 loss_mean: 2.7207445335388183
step: 1600 loss_mean: 2.7467277812957764
step: 1650 loss_mean: 2.6469617795944216
step: 1700 loss_mean: 2.7380188417434694
step: 1750 loss_mean: 2.769473180770874
step: 1800 loss_mean: 2.7262752723693846
step: 1850 loss_mean: 2.676837582588196
step: 1900 loss_mean: 2.732431149482727
step: 1950 loss_mean: 2.7214905881881712
step: 2000 loss_mean: 2.7194107723236085
step: 2050 loss_mean: 2.7345428800582887
step: 2100 loss_mean: 2.695646200180054
step: 2150 loss_mean: 2.7123354959487913
step: 2200 loss_mean: 2.6529679489135742
step: 2250 loss_mean: 2.625518765449524
step: 2300 loss_mean: 2.728124008178711
step: 2350 loss_mean: 2.7533250713348387
step: 2400 loss_mean: 2.699569878578186
step: 2450 loss_mean: 2.7250756311416624
step: 2500 loss_mean: 2.7360067415237426
step: 2550 loss_mean: 2.6767521619796755
step: 2600 loss_mean: 2.77160870552063
step: 2650 loss_mean: 2.7854497241973877
step: 2700 loss_mean: 2.765235538482666
step: 2750 loss_mean: 2.738002371788025
step: 2800 loss_mean: 2.7724993658065795
step: 2850 loss_mean: 2.7182705640792846
step: 2900 loss_mean: 2.7047873973846435
step: 2950 loss_mean: 2.775834331512451
step: 3000 loss_mean: 2.727790093421936
step: 3050 loss_mean: 2.698490719795227
step: 3100 loss_mean: 2.7474558305740358
step: 50 loss_mean: 2.7397087955474855
step: 100 loss_mean: 2.77243381023407
step: 150 loss_mean: 2.7063888692855835
step: 200 loss_mean: 2.8076553201675414
step: 250 loss_mean: 2.75289532661438
step: 300 loss_mean: 2.8485194778442384
step: 350 loss_mean: 2.703439407348633
step: 400 loss_mean: 2.7325395584106444
step: 450 loss_mean: 2.7178751611709595
step: 500 loss_mean: 2.7563042068481445
step: 550 loss_mean: 2.730367856025696
step: 600 loss_mean: 2.68221604347229
Epoch: 76 | Run time: 624.0 s | Train loss: 2.72 | Valid loss: 2.75
Saved checkpoint 76 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1128 s.
run_epoch
step: 50 loss_mean: 2.7514165449142456
step: 100 loss_mean: 2.7229121351242065
step: 150 loss_mean: 2.7484406042099
step: 200 loss_mean: 2.6581801414489745
step: 250 loss_mean: 2.7185388088226317
step: 300 loss_mean: 2.669534931182861
step: 350 loss_mean: 2.705121412277222
step: 400 loss_mean: 2.704658908843994
step: 450 loss_mean: 2.705826072692871
step: 500 loss_mean: 2.6854413175582885
step: 550 loss_mean: 2.719295787811279
step: 600 loss_mean: 2.7198579597473143
step: 650 loss_mean: 2.699687886238098
step: 700 loss_mean: 2.7333948898315428
step: 750 loss_mean: 2.7171466064453127
step: 800 loss_mean: 2.6967772388458253
step: 850 loss_mean: 2.710122237205505
step: 900 loss_mean: 2.709992344379425
step: 950 loss_mean: 2.699742600917816
step: 1000 loss_mean: 2.7478540754318237
step: 1050 loss_mean: 2.717265820503235
step: 1100 loss_mean: 2.725643286705017
step: 1150 loss_mean: 2.7013596153259276
step: 1200 loss_mean: 2.6775097703933715
step: 1250 loss_mean: 2.7368998193740843
step: 1300 loss_mean: 2.736526160240173
step: 1350 loss_mean: 2.7384261322021484
step: 1400 loss_mean: 2.751786775588989
step: 1450 loss_mean: 2.698005881309509
step: 1500 loss_mean: 2.7444233894348145
step: 1550 loss_mean: 2.708392357826233
step: 1600 loss_mean: 2.7295139455795288
step: 1650 loss_mean: 2.6911502647399903
step: 1700 loss_mean: 2.73367422580719
step: 1750 loss_mean: 2.7272828722000124
step: 1800 loss_mean: 2.76701162815094
step: 1850 loss_mean: 2.779012508392334
step: 1900 loss_mean: 2.7056741905212403
step: 1950 loss_mean: 2.7965594482421876
step: 2000 loss_mean: 2.7691025876998903
step: 2050 loss_mean: 2.7039675283432008
step: 2100 loss_mean: 2.720073881149292
step: 2150 loss_mean: 2.6854839372634887
step: 2200 loss_mean: 2.6962152481079102
step: 2250 loss_mean: 2.673808298110962
step: 2300 loss_mean: 2.6861110877990724
step: 2350 loss_mean: 2.711860146522522
step: 2400 loss_mean: 2.7220950603485106
step: 2450 loss_mean: 2.643102207183838
step: 2500 loss_mean: 2.7173234748840334
step: 2550 loss_mean: 2.7705863666534425
step: 2600 loss_mean: 2.781490604877472
step: 2650 loss_mean: 2.714491105079651
step: 2700 loss_mean: 2.7179105043411256
step: 2750 loss_mean: 2.714318633079529
step: 2800 loss_mean: 2.756687407493591
step: 2850 loss_mean: 2.7200000190734865
step: 2900 loss_mean: 2.7453014278411865
step: 2950 loss_mean: 2.7210066509246826
step: 3000 loss_mean: 2.731308784484863
step: 3050 loss_mean: 2.663298454284668
step: 3100 loss_mean: 2.7216113328933718
step: 50 loss_mean: 2.798533158302307
step: 100 loss_mean: 2.837797498703003
step: 150 loss_mean: 2.7550151252746584
step: 200 loss_mean: 2.8624810791015625
step: 250 loss_mean: 2.804207558631897
step: 300 loss_mean: 2.896838641166687
step: 350 loss_mean: 2.7550727939605713
step: 400 loss_mean: 2.7830093765258788
step: 450 loss_mean: 2.788251938819885
step: 500 loss_mean: 2.8077272891998293
step: 550 loss_mean: 2.7773676681518555
step: 600 loss_mean: 2.724457149505615
Epoch: 77 | Run time: 623.0 s | Train loss: 2.72 | Valid loss: 2.8
Saved checkpoint 77 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1135 s.
run_epoch
step: 50 loss_mean: 2.7658351373672487
step: 100 loss_mean: 2.6939738178253174
step: 150 loss_mean: 2.727883882522583
step: 200 loss_mean: 2.7915210056304933
step: 250 loss_mean: 2.7226660585403444
step: 300 loss_mean: 2.7213117456436158
step: 350 loss_mean: 2.689586124420166
step: 400 loss_mean: 2.75459237575531
step: 450 loss_mean: 2.7077427673339844
step: 500 loss_mean: 2.7528148555755614
step: 550 loss_mean: 2.747361602783203
step: 600 loss_mean: 2.7072607183456423
step: 650 loss_mean: 2.720950880050659
step: 700 loss_mean: 2.728100185394287
step: 750 loss_mean: 2.6755602836608885
step: 800 loss_mean: 2.7397634983062744
step: 850 loss_mean: 2.7348732376098632
step: 900 loss_mean: 2.746467127799988
step: 950 loss_mean: 2.694943585395813
step: 1000 loss_mean: 2.741634936332703
step: 1050 loss_mean: 2.7317003536224367
step: 1100 loss_mean: 2.6939178705215454
step: 1150 loss_mean: 2.668828444480896
step: 1200 loss_mean: 2.651762771606445
step: 1250 loss_mean: 2.713747305870056
step: 1300 loss_mean: 2.728746953010559
step: 1350 loss_mean: 2.758855619430542
step: 1400 loss_mean: 2.7457378673553468
step: 1450 loss_mean: 2.7159943628311156
step: 1500 loss_mean: 2.710457835197449
step: 1550 loss_mean: 2.7807480907440185
step: 1600 loss_mean: 2.690855851173401
step: 1650 loss_mean: 2.753805408477783
step: 1700 loss_mean: 2.6753856420516966
step: 1750 loss_mean: 2.733134346008301
step: 1800 loss_mean: 2.6996633529663088
step: 1850 loss_mean: 2.7250374126434327
step: 1900 loss_mean: 2.6750249910354613
step: 1950 loss_mean: 2.747780270576477
step: 2000 loss_mean: 2.656969199180603
step: 2050 loss_mean: 2.7268559026718138
step: 2100 loss_mean: 2.7156423711776734
step: 2150 loss_mean: 2.691717572212219
step: 2200 loss_mean: 2.717186770439148
step: 2250 loss_mean: 2.6943708848953247
step: 2300 loss_mean: 2.7190005588531494
step: 2350 loss_mean: 2.750450758934021
step: 2400 loss_mean: 2.718069863319397
step: 2450 loss_mean: 2.7641585063934326
step: 2500 loss_mean: 2.7173969411849974
step: 2550 loss_mean: 2.687662487030029
step: 2600 loss_mean: 2.720782299041748
step: 2650 loss_mean: 2.7307615423202516
step: 2700 loss_mean: 2.734776487350464
step: 2750 loss_mean: 2.7251615166664123
step: 2800 loss_mean: 2.7293343544006348
step: 2850 loss_mean: 2.7039305353164673
step: 2900 loss_mean: 2.7156043529510496
step: 2950 loss_mean: 2.668300096988678
step: 3000 loss_mean: 2.7160802936553954
step: 3050 loss_mean: 2.7525534057617187
step: 3100 loss_mean: 2.7621498250961305
step: 50 loss_mean: 2.7497438192367554
step: 100 loss_mean: 2.7716611337661745
step: 150 loss_mean: 2.721273646354675
step: 200 loss_mean: 2.8217921161651613
step: 250 loss_mean: 2.7503758764266966
step: 300 loss_mean: 2.85957932472229
step: 350 loss_mean: 2.7066127490997314
step: 400 loss_mean: 2.732926802635193
step: 450 loss_mean: 2.715270471572876
step: 500 loss_mean: 2.775367317199707
step: 550 loss_mean: 2.7339971351623533
step: 600 loss_mean: 2.680546774864197
Epoch: 78 | Run time: 623.0 s | Train loss: 2.72 | Valid loss: 2.75
Saved checkpoint 78 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1139 s.
run_epoch
step: 50 loss_mean: 2.725775218009949
step: 100 loss_mean: 2.7097353982925414
step: 150 loss_mean: 2.6647546768188475
step: 200 loss_mean: 2.7095909118652344
step: 250 loss_mean: 2.753088798522949
step: 300 loss_mean: 2.735079917907715
step: 350 loss_mean: 2.7000304317474364
step: 400 loss_mean: 2.7013570165634153
step: 450 loss_mean: 2.7490820121765136
step: 500 loss_mean: 2.7581074094772338
step: 550 loss_mean: 2.718326187133789
step: 600 loss_mean: 2.7197434854507447
step: 650 loss_mean: 2.7343637943267822
step: 700 loss_mean: 2.7058254861831665
step: 750 loss_mean: 2.6838466548919677
step: 800 loss_mean: 2.6678283643722533
step: 850 loss_mean: 2.7171239614486695
step: 900 loss_mean: 2.6902873945236205
step: 950 loss_mean: 2.737762656211853
step: 1000 loss_mean: 2.733729100227356
step: 1050 loss_mean: 2.7683630752563477
step: 1100 loss_mean: 2.67621835231781
step: 1150 loss_mean: 2.6978343725204468
step: 1200 loss_mean: 2.7029632568359374
step: 1250 loss_mean: 2.7419149112701415
step: 1300 loss_mean: 2.6902802848815917
step: 1350 loss_mean: 2.75146954536438
step: 1400 loss_mean: 2.733793702125549
step: 1450 loss_mean: 2.7004140329360964
step: 1500 loss_mean: 2.765481481552124
step: 1550 loss_mean: 2.740116820335388
step: 1600 loss_mean: 2.732582106590271
step: 1650 loss_mean: 2.6999810409545897
step: 1700 loss_mean: 2.656851167678833
step: 1750 loss_mean: 2.7727951431274414
step: 1800 loss_mean: 2.7495741128921507
step: 1850 loss_mean: 2.735929083824158
step: 1900 loss_mean: 2.697490587234497
step: 1950 loss_mean: 2.744831767082214
step: 2000 loss_mean: 2.7361269998550415
step: 2050 loss_mean: 2.728934659957886
step: 2100 loss_mean: 2.7189054536819457
step: 2150 loss_mean: 2.714785270690918
step: 2200 loss_mean: 2.7087251329421997
step: 2250 loss_mean: 2.6961106061935425
step: 2300 loss_mean: 2.6818864822387694
step: 2350 loss_mean: 2.739269742965698
step: 2400 loss_mean: 2.7113643312454223
step: 2450 loss_mean: 2.6979845476150515
step: 2500 loss_mean: 2.7581133127212523
step: 2550 loss_mean: 2.6903521966934205
step: 2600 loss_mean: 2.71029860496521
step: 2650 loss_mean: 2.688491072654724
step: 2700 loss_mean: 2.766398062705994
step: 2750 loss_mean: 2.702494168281555
step: 2800 loss_mean: 2.70723641872406
step: 2850 loss_mean: 2.732667784690857
step: 2900 loss_mean: 2.737214279174805
step: 2950 loss_mean: 2.6910867738723754
step: 3000 loss_mean: 2.746182928085327
step: 3050 loss_mean: 2.6775328397750853
step: 3100 loss_mean: 2.7186456441879274
step: 50 loss_mean: 2.762948579788208
step: 100 loss_mean: 2.7958415174484252
step: 150 loss_mean: 2.7237919998168945
step: 200 loss_mean: 2.808514337539673
step: 250 loss_mean: 2.7690858745574953
step: 300 loss_mean: 2.837352695465088
step: 350 loss_mean: 2.7434832191467287
step: 400 loss_mean: 2.7639302396774292
step: 450 loss_mean: 2.7291442918777467
step: 500 loss_mean: 2.7854976415634156
step: 550 loss_mean: 2.7420794105529787
step: 600 loss_mean: 2.6998925256729125
Epoch: 79 | Run time: 621.0 s | Train loss: 2.72 | Valid loss: 2.76
Saved checkpoint 79 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1179 s.
run_epoch
step: 50 loss_mean: 2.7418296003341673
step: 100 loss_mean: 2.704826488494873
step: 150 loss_mean: 2.6521836137771606
step: 200 loss_mean: 2.6988860845565794
step: 250 loss_mean: 2.680491819381714
step: 300 loss_mean: 2.671318235397339
step: 350 loss_mean: 2.7178770875930787
step: 400 loss_mean: 2.701324920654297
step: 450 loss_mean: 2.703787717819214
step: 500 loss_mean: 2.703102259635925
step: 550 loss_mean: 2.7010250806808473
step: 600 loss_mean: 2.752517275810242
step: 650 loss_mean: 2.7036570596694944
step: 700 loss_mean: 2.754728593826294
step: 750 loss_mean: 2.7666455841064455
step: 800 loss_mean: 2.7187276005744936
step: 850 loss_mean: 2.6887129068374636
step: 900 loss_mean: 2.7572723388671876
step: 950 loss_mean: 2.7754046297073365
step: 1000 loss_mean: 2.7551733589172365
step: 1050 loss_mean: 2.6532368421554566
step: 1100 loss_mean: 2.7661736965179444
step: 1150 loss_mean: 2.685367798805237
step: 1200 loss_mean: 2.6982598400115965
step: 1250 loss_mean: 2.713673987388611
step: 1300 loss_mean: 2.730400972366333
step: 1350 loss_mean: 2.676492805480957
step: 1400 loss_mean: 2.7082975339889526
step: 1450 loss_mean: 2.732241530418396
step: 1500 loss_mean: 2.679190282821655
step: 1550 loss_mean: 2.7455454206466676
step: 1600 loss_mean: 2.711942639350891
step: 1650 loss_mean: 2.7056902074813842
step: 1700 loss_mean: 2.6837568855285645
step: 1750 loss_mean: 2.7800267553329467
step: 1800 loss_mean: 2.685440950393677
step: 1850 loss_mean: 2.6775858545303346
step: 1900 loss_mean: 2.7446112203598023
step: 1950 loss_mean: 2.7214169454574586
step: 2000 loss_mean: 2.7110805463790895
step: 2050 loss_mean: 2.7533217239379884
step: 2100 loss_mean: 2.7784281253814695
step: 2150 loss_mean: 2.740879158973694
step: 2200 loss_mean: 2.730345244407654
step: 2250 loss_mean: 2.702031579017639
step: 2300 loss_mean: 2.7400162267684935
step: 2350 loss_mean: 2.740166912078857
step: 2400 loss_mean: 2.691097493171692
step: 2450 loss_mean: 2.7484075307846068
step: 2500 loss_mean: 2.725135688781738
step: 2550 loss_mean: 2.662937569618225
step: 2600 loss_mean: 2.705857238769531
step: 2650 loss_mean: 2.696120924949646
step: 2700 loss_mean: 2.681064658164978
step: 2750 loss_mean: 2.726887125968933
step: 2800 loss_mean: 2.742608232498169
step: 2850 loss_mean: 2.6500773143768313
step: 2900 loss_mean: 2.753400135040283
step: 2950 loss_mean: 2.777326202392578
step: 3000 loss_mean: 2.744677414894104
step: 3050 loss_mean: 2.7546503925323487
step: 3100 loss_mean: 2.741426024436951
step: 50 loss_mean: 2.742103810310364
step: 100 loss_mean: 2.8019120836257936
step: 150 loss_mean: 2.7207586097717287
step: 200 loss_mean: 2.823185291290283
step: 250 loss_mean: 2.7639527702331543
step: 300 loss_mean: 2.860501837730408
step: 350 loss_mean: 2.71783802986145
step: 400 loss_mean: 2.737369627952576
step: 450 loss_mean: 2.722834358215332
step: 500 loss_mean: 2.7782943201065065
step: 550 loss_mean: 2.7369574737548827
step: 600 loss_mean: 2.6892051839828492
Epoch: 80 | Run time: 623.0 s | Train loss: 2.72 | Valid loss: 2.76
Saved checkpoint 80 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1176 s.
run_epoch
step: 50 loss_mean: 2.6954389142990114
step: 100 loss_mean: 2.7343003034591673
step: 150 loss_mean: 2.709159483909607
step: 200 loss_mean: 2.721798844337463
step: 250 loss_mean: 2.7014641094207765
step: 300 loss_mean: 2.7255227756500244
step: 350 loss_mean: 2.7234833574295045
step: 400 loss_mean: 2.748973822593689
step: 450 loss_mean: 2.662605390548706
step: 500 loss_mean: 2.715466995239258
step: 550 loss_mean: 2.7115927648544313
step: 600 loss_mean: 2.699403805732727
step: 650 loss_mean: 2.7449202156066894
step: 700 loss_mean: 2.697630910873413
step: 750 loss_mean: 2.738048710823059
step: 800 loss_mean: 2.713250617980957
step: 850 loss_mean: 2.734497404098511
step: 900 loss_mean: 2.7265141344070436
step: 950 loss_mean: 2.733771228790283
step: 1000 loss_mean: 2.7099665880203245
step: 1050 loss_mean: 2.7094172286987304
step: 1100 loss_mean: 2.7576505041122434
step: 1150 loss_mean: 2.7192521142959594
step: 1200 loss_mean: 2.7185219383239745
step: 1250 loss_mean: 2.6881084632873535
step: 1300 loss_mean: 2.6858452272415163
step: 1350 loss_mean: 2.67704345703125
step: 1400 loss_mean: 2.7518237686157225
step: 1450 loss_mean: 2.7113152503967286
step: 1500 loss_mean: 2.700326714515686
step: 1550 loss_mean: 2.717488203048706
step: 1600 loss_mean: 2.7742689561843874
step: 1650 loss_mean: 2.7133755016326906
step: 1700 loss_mean: 2.6988421869277954
step: 1750 loss_mean: 2.651865348815918
step: 1800 loss_mean: 2.722817406654358
step: 1850 loss_mean: 2.681863660812378
step: 1900 loss_mean: 2.6537219190597536
step: 1950 loss_mean: 2.7083487510681152
step: 2000 loss_mean: 2.7141009378433227
step: 2050 loss_mean: 2.717957978248596
step: 2100 loss_mean: 2.664371485710144
step: 2150 loss_mean: 2.723667187690735
step: 2200 loss_mean: 2.7211660242080686
step: 2250 loss_mean: 2.748226914405823
step: 2300 loss_mean: 2.7367737007141115
step: 2350 loss_mean: 2.7175054216384886
step: 2400 loss_mean: 2.7379198122024535
step: 2450 loss_mean: 2.7414901208877565
step: 2500 loss_mean: 2.7596060752868654
step: 2550 loss_mean: 2.7538420724868775
step: 2600 loss_mean: 2.6813784074783324
step: 2650 loss_mean: 2.6618178129196166
step: 2700 loss_mean: 2.6387098360061647
step: 2750 loss_mean: 2.799698944091797
step: 2800 loss_mean: 2.753747682571411
step: 2850 loss_mean: 2.762868709564209
step: 2900 loss_mean: 2.722069334983826
step: 2950 loss_mean: 2.7184201431274415
step: 3000 loss_mean: 2.742409372329712
step: 3050 loss_mean: 2.696435408592224
step: 3100 loss_mean: 2.7480117321014403
step: 50 loss_mean: 2.7492458057403564
step: 100 loss_mean: 2.783085055351257
step: 150 loss_mean: 2.7011564445495604
step: 200 loss_mean: 2.81123893737793
step: 250 loss_mean: 2.775333561897278
step: 300 loss_mean: 2.8680561494827272
step: 350 loss_mean: 2.726820387840271
step: 400 loss_mean: 2.736993770599365
step: 450 loss_mean: 2.7256991720199584
step: 500 loss_mean: 2.7802792310714723
step: 550 loss_mean: 2.739356393814087
step: 600 loss_mean: 2.694010272026062
Epoch: 81 | Run time: 624.0 s | Train loss: 2.72 | Valid loss: 2.76
Saved checkpoint 81 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1187 s.
run_epoch
step: 50 loss_mean: 2.7517353010177614
step: 100 loss_mean: 2.6718242025375365
step: 150 loss_mean: 2.7597473859786987
step: 200 loss_mean: 2.7073091077804565
step: 250 loss_mean: 2.6563018321990968
step: 300 loss_mean: 2.711795587539673
step: 350 loss_mean: 2.721560206413269
step: 400 loss_mean: 2.7361544799804687
step: 450 loss_mean: 2.735691065788269
step: 500 loss_mean: 2.7231292724609375
step: 550 loss_mean: 2.6599720525741577
step: 600 loss_mean: 2.706006898880005
step: 650 loss_mean: 2.6800195932388307
step: 700 loss_mean: 2.7391677570343016
step: 750 loss_mean: 2.704815969467163
step: 800 loss_mean: 2.7389713096618653
step: 850 loss_mean: 2.6811695194244383
step: 900 loss_mean: 2.784658155441284
step: 950 loss_mean: 2.722310771942139
step: 1000 loss_mean: 2.7325349855422973
step: 1050 loss_mean: 2.708141713142395
step: 1100 loss_mean: 2.6855937910079954
step: 1150 loss_mean: 2.724028134346008
step: 1200 loss_mean: 2.704988369941711
step: 1250 loss_mean: 2.7452018737792967
step: 1300 loss_mean: 2.711162905693054
step: 1350 loss_mean: 2.6795968580245972
step: 1400 loss_mean: 2.7592817544937134
step: 1450 loss_mean: 2.7278893518447878
step: 1500 loss_mean: 2.6618358373641966
step: 1550 loss_mean: 2.7379492998123167
step: 1600 loss_mean: 2.7475724601745606
step: 1650 loss_mean: 2.7339946365356447
step: 1700 loss_mean: 2.755190396308899
step: 1750 loss_mean: 2.726175236701965
step: 1800 loss_mean: 2.6826472234725953
step: 1850 loss_mean: 2.7282183361053467
step: 1900 loss_mean: 2.752229166030884
step: 1950 loss_mean: 2.6968102741241453
step: 2000 loss_mean: 2.748530797958374
step: 2050 loss_mean: 2.7182944202423096
step: 2100 loss_mean: 2.703778624534607
step: 2150 loss_mean: 2.7454900407791136
step: 2200 loss_mean: 2.7277139902114866
step: 2250 loss_mean: 2.685667462348938
step: 2300 loss_mean: 2.7187293672561648
step: 2350 loss_mean: 2.6873689603805544
step: 2400 loss_mean: 2.6982259511947633
step: 2450 loss_mean: 2.6802121925354006
step: 2500 loss_mean: 2.677981390953064
step: 2550 loss_mean: 2.6707331466674806
step: 2600 loss_mean: 2.672762885093689
step: 2650 loss_mean: 2.7429656839370726
step: 2700 loss_mean: 2.692073450088501
step: 2750 loss_mean: 2.7037005710601805
step: 2800 loss_mean: 2.686693339347839
step: 2850 loss_mean: 2.73488618850708
step: 2900 loss_mean: 2.7206580448150635
step: 2950 loss_mean: 2.772932586669922
step: 3000 loss_mean: 2.6841255140304567
step: 3050 loss_mean: 2.719093990325928
step: 3100 loss_mean: 2.714572229385376
step: 50 loss_mean: 2.741055588722229
step: 100 loss_mean: 2.768782682418823
step: 150 loss_mean: 2.716197175979614
step: 200 loss_mean: 2.8196078014373778
step: 250 loss_mean: 2.770915789604187
step: 300 loss_mean: 2.8456041622161865
step: 350 loss_mean: 2.7121300888061524
step: 400 loss_mean: 2.7340940856933593
step: 450 loss_mean: 2.7269712495803833
step: 500 loss_mean: 2.7581314849853515
step: 550 loss_mean: 2.7337080955505373
step: 600 loss_mean: 2.678722472190857
Epoch: 82 | Run time: 624.0 s | Train loss: 2.71 | Valid loss: 2.75
Saved checkpoint 82 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1198 s.
run_epoch
step: 50 loss_mean: 2.7515858125686647
step: 100 loss_mean: 2.6782369136810305
step: 150 loss_mean: 2.7276780724525453
step: 200 loss_mean: 2.738193235397339
step: 250 loss_mean: 2.6844255208969114
step: 300 loss_mean: 2.717817254066467
step: 350 loss_mean: 2.699344391822815
step: 400 loss_mean: 2.737951035499573
step: 450 loss_mean: 2.687960481643677
step: 500 loss_mean: 2.6986806392669678
step: 550 loss_mean: 2.769228706359863
step: 600 loss_mean: 2.6812339878082274
step: 650 loss_mean: 2.6368409967422486
step: 700 loss_mean: 2.7255814456939698
step: 750 loss_mean: 2.712508978843689
step: 800 loss_mean: 2.7445774984359743
step: 850 loss_mean: 2.736278285980225
step: 900 loss_mean: 2.6893493509292603
step: 950 loss_mean: 2.7200000381469724
step: 1000 loss_mean: 2.6918458461761476
step: 1050 loss_mean: 2.7445145654678345
step: 1100 loss_mean: 2.7442528438568115
step: 1150 loss_mean: 2.7678458881378174
step: 1200 loss_mean: 2.7303313875198363
step: 1250 loss_mean: 2.729999804496765
step: 1300 loss_mean: 2.666712827682495
step: 1350 loss_mean: 2.6863000345230104
step: 1400 loss_mean: 2.7250475120544433
step: 1450 loss_mean: 2.7495632362365723
step: 1500 loss_mean: 2.733944444656372
step: 1550 loss_mean: 2.708793387413025
step: 1600 loss_mean: 2.7088524961471556
step: 1650 loss_mean: 2.6725519418716432
step: 1700 loss_mean: 2.7194257497787477
step: 1750 loss_mean: 2.687138543128967
step: 1800 loss_mean: 2.7611918544769285
step: 1850 loss_mean: 2.719759478569031
step: 1900 loss_mean: 2.7142496347427367
step: 1950 loss_mean: 2.6926069259643555
step: 2000 loss_mean: 2.736816825866699
step: 2050 loss_mean: 2.6993664264678956
step: 2100 loss_mean: 2.728785877227783
step: 2150 loss_mean: 2.716958260536194
step: 2200 loss_mean: 2.67042387008667
step: 2250 loss_mean: 2.770629711151123
step: 2300 loss_mean: 2.726870698928833
step: 2350 loss_mean: 2.7015678977966306
step: 2400 loss_mean: 2.7355488538742065
step: 2450 loss_mean: 2.7062638902664187
step: 2500 loss_mean: 2.708569302558899
step: 2550 loss_mean: 2.6599536609649657
step: 2600 loss_mean: 2.7207552576065064
step: 2650 loss_mean: 2.6419780015945435
step: 2700 loss_mean: 2.660534520149231
step: 2750 loss_mean: 2.7596479940414427
step: 2800 loss_mean: 2.7004881620407106
step: 2850 loss_mean: 2.75815233707428
step: 2900 loss_mean: 2.6963020896911623
step: 2950 loss_mean: 2.746210060119629
step: 3000 loss_mean: 2.6831465911865235
step: 3050 loss_mean: 2.72419038772583
step: 3100 loss_mean: 2.7437561178207397
step: 50 loss_mean: 2.7288880348205566
step: 100 loss_mean: 2.77532874584198
step: 150 loss_mean: 2.7024576759338377
step: 200 loss_mean: 2.811136608123779
step: 250 loss_mean: 2.753493838310242
step: 300 loss_mean: 2.8369472885131835
step: 350 loss_mean: 2.7117156887054445
step: 400 loss_mean: 2.744017467498779
step: 450 loss_mean: 2.7174065685272217
step: 500 loss_mean: 2.7710107278823854
step: 550 loss_mean: 2.726899290084839
step: 600 loss_mean: 2.6823837184906005
Epoch: 83 | Run time: 625.0 s | Train loss: 2.71 | Valid loss: 2.75
Saved checkpoint 83 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1224 s.
run_epoch
step: 50 loss_mean: 2.7276873922348024
step: 100 loss_mean: 2.6912899923324587
step: 150 loss_mean: 2.7157282972335817
step: 200 loss_mean: 2.7555537462234496
step: 250 loss_mean: 2.7064277744293213
step: 300 loss_mean: 2.7338218307495117
step: 350 loss_mean: 2.717194776535034
step: 400 loss_mean: 2.7297257471084593
step: 450 loss_mean: 2.7410277938842773
step: 500 loss_mean: 2.689776906967163
step: 550 loss_mean: 2.6923657274246215
step: 600 loss_mean: 2.7016769552230837
step: 650 loss_mean: 2.714129629135132
step: 700 loss_mean: 2.7215821743011475
step: 750 loss_mean: 2.7138133668899536
step: 800 loss_mean: 2.688437566757202
step: 850 loss_mean: 2.696309733390808
step: 900 loss_mean: 2.710902004241943
step: 950 loss_mean: 2.6586837816238402
step: 1000 loss_mean: 2.695608205795288
step: 1050 loss_mean: 2.714454574584961
step: 1100 loss_mean: 2.7282760190963744
step: 1150 loss_mean: 2.6847996282577515
step: 1200 loss_mean: 2.772378544807434
step: 1250 loss_mean: 2.736116762161255
step: 1300 loss_mean: 2.6875430393218993
step: 1350 loss_mean: 2.743974313735962
step: 1400 loss_mean: 2.7015897178649904
step: 1450 loss_mean: 2.678004808425903
step: 1500 loss_mean: 2.69626690864563
step: 1550 loss_mean: 2.683019733428955
step: 1600 loss_mean: 2.6580063581466673
step: 1650 loss_mean: 2.7071354389190674
step: 1700 loss_mean: 2.6720699739456175
step: 1750 loss_mean: 2.694344310760498
step: 1800 loss_mean: 2.744494752883911
step: 1850 loss_mean: 2.7409906244277953
step: 1900 loss_mean: 2.719493532180786
step: 1950 loss_mean: 2.7277557468414306
step: 2000 loss_mean: 2.752318596839905
step: 2050 loss_mean: 2.7242402982711793
step: 2100 loss_mean: 2.7041318893432615
step: 2150 loss_mean: 2.7350928926467897
step: 2200 loss_mean: 2.673109426498413
step: 2250 loss_mean: 2.690338807106018
step: 2300 loss_mean: 2.7192247104644776
step: 2350 loss_mean: 2.735237135887146
step: 2400 loss_mean: 2.7787496852874756
step: 2450 loss_mean: 2.7384167528152465
step: 2500 loss_mean: 2.7227348470687867
step: 2550 loss_mean: 2.7487934160232546
step: 2600 loss_mean: 2.7064412117004393
step: 2650 loss_mean: 2.712520523071289
step: 2700 loss_mean: 2.748992877006531
step: 2750 loss_mean: 2.72108585357666
step: 2800 loss_mean: 2.6986755466461183
step: 2850 loss_mean: 2.734428286552429
step: 2900 loss_mean: 2.7064042949676512
step: 2950 loss_mean: 2.6781990337371826
step: 3000 loss_mean: 2.707763366699219
step: 3050 loss_mean: 2.7048290252685545
step: 3100 loss_mean: 2.668617084026337
step: 50 loss_mean: 2.771407403945923
step: 100 loss_mean: 2.8129290771484374
step: 150 loss_mean: 2.7464932298660276
step: 200 loss_mean: 2.8304588413238525
step: 250 loss_mean: 2.7832816791534425
step: 300 loss_mean: 2.869170069694519
step: 350 loss_mean: 2.7503875732421874
step: 400 loss_mean: 2.7771155881881713
step: 450 loss_mean: 2.7535999393463135
step: 500 loss_mean: 2.800636787414551
step: 550 loss_mean: 2.742215781211853
step: 600 loss_mean: 2.7181567764282226
Epoch: 84 | Run time: 622.0 s | Train loss: 2.71 | Valid loss: 2.78
Saved checkpoint 84 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1222 s.
run_epoch
step: 50 loss_mean: 2.6790138339996337
step: 100 loss_mean: 2.7686188220977783
step: 150 loss_mean: 2.6462701034545897
step: 200 loss_mean: 2.743510046005249
step: 250 loss_mean: 2.7314698123931884
step: 300 loss_mean: 2.6850547695159914
step: 350 loss_mean: 2.6874735736846924
step: 400 loss_mean: 2.7274853944778443
step: 450 loss_mean: 2.7195819854736327
step: 500 loss_mean: 2.790361533164978
step: 550 loss_mean: 2.7052352476119994
step: 600 loss_mean: 2.6722756242752075
step: 650 loss_mean: 2.6697694301605224
step: 700 loss_mean: 2.7434305143356323
step: 750 loss_mean: 2.7402255868911745
step: 800 loss_mean: 2.711487956047058
step: 850 loss_mean: 2.6947182559967042
step: 900 loss_mean: 2.7106842184066773
step: 950 loss_mean: 2.6820996952056886
step: 1000 loss_mean: 2.7228852558135985
step: 1050 loss_mean: 2.6891587734222413
step: 1100 loss_mean: 2.757174882888794
step: 1150 loss_mean: 2.6948737621307375
step: 1200 loss_mean: 2.6801812410354615
step: 1250 loss_mean: 2.7565618896484376
step: 1300 loss_mean: 2.724866952896118
step: 1350 loss_mean: 2.6738736057281494
step: 1400 loss_mean: 2.737443037033081
step: 1450 loss_mean: 2.743750696182251
step: 1500 loss_mean: 2.7023746490478517
step: 1550 loss_mean: 2.742066254615784
step: 1600 loss_mean: 2.6610253620147706
step: 1650 loss_mean: 2.736618456840515
step: 1700 loss_mean: 2.676583881378174
step: 1750 loss_mean: 2.7314294290542604
step: 1800 loss_mean: 2.7208067989349365
step: 1850 loss_mean: 2.746563000679016
step: 1900 loss_mean: 2.6679939436912536
step: 1950 loss_mean: 2.6899839544296267
step: 2000 loss_mean: 2.7258441734313963
step: 2050 loss_mean: 2.703522176742554
step: 2100 loss_mean: 2.6976092004776
step: 2150 loss_mean: 2.760296692848206
step: 2200 loss_mean: 2.7451353311538695
step: 2250 loss_mean: 2.7266877031326295
step: 2300 loss_mean: 2.694232907295227
step: 2350 loss_mean: 2.6995206928253173
step: 2400 loss_mean: 2.6972892761230467
step: 2450 loss_mean: 2.7038654899597168
step: 2500 loss_mean: 2.6739747953414916
step: 2550 loss_mean: 2.7496292924880983
step: 2600 loss_mean: 2.740518288612366
step: 2650 loss_mean: 2.692320227622986
step: 2700 loss_mean: 2.6825586414337157
step: 2750 loss_mean: 2.7069492626190184
step: 2800 loss_mean: 2.6806771564483642
step: 2850 loss_mean: 2.722397737503052
step: 2900 loss_mean: 2.6802636337280275
step: 2950 loss_mean: 2.7119529581069948
step: 3000 loss_mean: 2.7366644382476806
step: 3050 loss_mean: 2.697974362373352
step: 3100 loss_mean: 2.7330130434036253
step: 50 loss_mean: 2.7592310905456543
step: 100 loss_mean: 2.805487971305847
step: 150 loss_mean: 2.710642013549805
step: 200 loss_mean: 2.809584059715271
step: 250 loss_mean: 2.781924614906311
step: 300 loss_mean: 2.8568188667297365
step: 350 loss_mean: 2.745213022232056
step: 400 loss_mean: 2.7486320781707763
step: 450 loss_mean: 2.734136724472046
step: 500 loss_mean: 2.792432370185852
step: 550 loss_mean: 2.7249481201171877
step: 600 loss_mean: 2.7027291536331175
Epoch: 85 | Run time: 624.0 s | Train loss: 2.71 | Valid loss: 2.77
Saved checkpoint 85 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1241 s.
run_epoch
step: 50 loss_mean: 2.748351001739502
step: 100 loss_mean: 2.687313656806946
step: 150 loss_mean: 2.749459481239319
step: 200 loss_mean: 2.6708264446258543
step: 250 loss_mean: 2.701743974685669
step: 300 loss_mean: 2.704005436897278
step: 350 loss_mean: 2.667968916893005
step: 400 loss_mean: 2.7239435338974
step: 450 loss_mean: 2.71918824672699
step: 500 loss_mean: 2.683112554550171
step: 550 loss_mean: 2.700129384994507
step: 600 loss_mean: 2.72416389465332
step: 650 loss_mean: 2.6945872068405152
step: 700 loss_mean: 2.745201659202576
step: 750 loss_mean: 2.709315791130066
step: 800 loss_mean: 2.7580583620071413
step: 850 loss_mean: 2.7321002960205076
step: 900 loss_mean: 2.717114658355713
step: 950 loss_mean: 2.658814778327942
step: 1000 loss_mean: 2.7357067394256593
step: 1050 loss_mean: 2.6834010791778566
step: 1100 loss_mean: 2.721365075111389
step: 1150 loss_mean: 2.7025342178344727
step: 1200 loss_mean: 2.698961043357849
step: 1250 loss_mean: 2.722707896232605
step: 1300 loss_mean: 2.7131018829345703
step: 1350 loss_mean: 2.6956800985336304
step: 1400 loss_mean: 2.6632169675827027
step: 1450 loss_mean: 2.694759750366211
step: 1500 loss_mean: 2.7100206279754637
step: 1550 loss_mean: 2.7010307025909426
step: 1600 loss_mean: 2.6871389627456663
step: 1650 loss_mean: 2.723775978088379
step: 1700 loss_mean: 2.7185506629943847
step: 1750 loss_mean: 2.6449722480773925
step: 1800 loss_mean: 2.705111379623413
step: 1850 loss_mean: 2.698878765106201
step: 1900 loss_mean: 2.7161357545852662
step: 1950 loss_mean: 2.6965195083618165
step: 2000 loss_mean: 2.7455065774917604
step: 2050 loss_mean: 2.753325719833374
step: 2100 loss_mean: 2.761298866271973
step: 2150 loss_mean: 2.6738918924331667
step: 2200 loss_mean: 2.7008667945861817
step: 2250 loss_mean: 2.7273579835891724
step: 2300 loss_mean: 2.680243144035339
step: 2350 loss_mean: 2.7291035509109496
step: 2400 loss_mean: 2.749721522331238
step: 2450 loss_mean: 2.6768418884277345
step: 2500 loss_mean: 2.721193618774414
step: 2550 loss_mean: 2.691411700248718
step: 2600 loss_mean: 2.6968224334716795
step: 2650 loss_mean: 2.722576313018799
step: 2700 loss_mean: 2.704217162132263
step: 2750 loss_mean: 2.7397733116149903
step: 2800 loss_mean: 2.7266294288635256
step: 2850 loss_mean: 2.701400108337402
step: 2900 loss_mean: 2.718426465988159
step: 2950 loss_mean: 2.739110016822815
step: 3000 loss_mean: 2.6930362129211427
step: 3050 loss_mean: 2.7491012620925903
step: 3100 loss_mean: 2.732383232116699
step: 50 loss_mean: 2.746558151245117
step: 100 loss_mean: 2.7947945404052734
step: 150 loss_mean: 2.7084824514389036
step: 200 loss_mean: 2.8053852844238283
step: 250 loss_mean: 2.755962972640991
step: 300 loss_mean: 2.854167342185974
step: 350 loss_mean: 2.7135253667831423
step: 400 loss_mean: 2.721975202560425
step: 450 loss_mean: 2.7221849584579467
step: 500 loss_mean: 2.7676138639450074
step: 550 loss_mean: 2.717304816246033
step: 600 loss_mean: 2.6797445106506346
Epoch: 86 | Run time: 622.0 s | Train loss: 2.71 | Valid loss: 2.75
Saved checkpoint 86 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1249 s.
run_epoch
step: 50 loss_mean: 2.7138484859466554
step: 100 loss_mean: 2.7021687984466554
step: 150 loss_mean: 2.7496853113174438
step: 200 loss_mean: 2.8123571062088013
step: 250 loss_mean: 2.7407960939407348
step: 300 loss_mean: 2.7048297262191774
step: 350 loss_mean: 2.7268343591690063
step: 400 loss_mean: 2.7191039657592775
step: 450 loss_mean: 2.6795678186416625
step: 500 loss_mean: 2.7192140293121336
step: 550 loss_mean: 2.7131435585021975
step: 600 loss_mean: 2.764382734298706
step: 650 loss_mean: 2.7611605644226076
step: 700 loss_mean: 2.6961326503753664
step: 750 loss_mean: 2.7123465061187746
step: 800 loss_mean: 2.6401440715789795
step: 850 loss_mean: 2.704010467529297
step: 900 loss_mean: 2.675146632194519
step: 950 loss_mean: 2.6807963037490845
step: 1000 loss_mean: 2.73621515750885
step: 1050 loss_mean: 2.704661445617676
step: 1100 loss_mean: 2.705244550704956
step: 1150 loss_mean: 2.6890503406524657
step: 1200 loss_mean: 2.7098886585235595
step: 1250 loss_mean: 2.776384129524231
step: 1300 loss_mean: 2.6565612983703613
step: 1350 loss_mean: 2.747868719100952
step: 1400 loss_mean: 2.7581194067001342
step: 1450 loss_mean: 2.657808723449707
step: 1500 loss_mean: 2.7002599811553956
step: 1550 loss_mean: 2.6936293125152586
step: 1600 loss_mean: 2.7516830730438233
step: 1650 loss_mean: 2.6834125089645386
step: 1700 loss_mean: 2.720980052947998
step: 1750 loss_mean: 2.6856700563430786
step: 1800 loss_mean: 2.696141095161438
step: 1850 loss_mean: 2.747112889289856
step: 1900 loss_mean: 2.737294912338257
step: 1950 loss_mean: 2.7310674238204955
step: 2000 loss_mean: 2.7070383501052855
step: 2050 loss_mean: 2.6908162784576417
step: 2100 loss_mean: 2.672840943336487
step: 2150 loss_mean: 2.6910341072082518
step: 2200 loss_mean: 2.7067746925354004
step: 2250 loss_mean: 2.7259887409210206
step: 2300 loss_mean: 2.6804938077926637
step: 2350 loss_mean: 2.6830856370925904
step: 2400 loss_mean: 2.679017343521118
step: 2450 loss_mean: 2.7495666408538817
step: 2500 loss_mean: 2.7448528528213503
step: 2550 loss_mean: 2.7046042490005493
step: 2600 loss_mean: 2.710355911254883
step: 2650 loss_mean: 2.690261278152466
step: 2700 loss_mean: 2.707395658493042
step: 2750 loss_mean: 2.671815347671509
step: 2800 loss_mean: 2.7506843996047974
step: 2850 loss_mean: 2.713083257675171
step: 2900 loss_mean: 2.6965153408050537
step: 2950 loss_mean: 2.7200344514846804
step: 3000 loss_mean: 2.692136678695679
step: 3050 loss_mean: 2.7040497875213623
step: 3100 loss_mean: 2.7028666830062864
step: 50 loss_mean: 2.737618818283081
step: 100 loss_mean: 2.7762201023101807
step: 150 loss_mean: 2.7167002725601197
step: 200 loss_mean: 2.7933430528640746
step: 250 loss_mean: 2.7600542163848876
step: 300 loss_mean: 2.8435961198806763
step: 350 loss_mean: 2.7000119066238404
step: 400 loss_mean: 2.7306314277648926
step: 450 loss_mean: 2.7215675497055054
step: 500 loss_mean: 2.765708317756653
step: 550 loss_mean: 2.72539776802063
step: 600 loss_mean: 2.687493953704834
Epoch: 87 | Run time: 623.0 s | Train loss: 2.71 | Valid loss: 2.75
Saved checkpoint 87 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1264 s.
run_epoch
step: 50 loss_mean: 2.701548147201538
step: 100 loss_mean: 2.68899893283844
step: 150 loss_mean: 2.7751280736923216
step: 200 loss_mean: 2.664660177230835
step: 250 loss_mean: 2.6924176931381227
step: 300 loss_mean: 2.7469198942184447
step: 350 loss_mean: 2.7120280647277832
step: 400 loss_mean: 2.6744150733947754
step: 450 loss_mean: 2.717989025115967
step: 500 loss_mean: 2.7282896852493286
step: 550 loss_mean: 2.675868520736694
step: 600 loss_mean: 2.7432155084609984
step: 650 loss_mean: 2.697540645599365
step: 700 loss_mean: 2.746055679321289
step: 750 loss_mean: 2.683775372505188
step: 800 loss_mean: 2.6628209590911864
step: 850 loss_mean: 2.74297221660614
step: 900 loss_mean: 2.7963068103790283
step: 950 loss_mean: 2.7392365980148314
step: 1000 loss_mean: 2.7269346046447756
step: 1050 loss_mean: 2.6945460653305053
step: 1100 loss_mean: 2.7385265827178955
step: 1150 loss_mean: 2.7120223951339724
step: 1200 loss_mean: 2.683913269042969
step: 1250 loss_mean: 2.6711469078063965
step: 1300 loss_mean: 2.7500194311141968
step: 1350 loss_mean: 2.731663689613342
step: 1400 loss_mean: 2.7068671798706054
step: 1450 loss_mean: 2.663711404800415
step: 1500 loss_mean: 2.7305828619003294
step: 1550 loss_mean: 2.7460152435302736
step: 1600 loss_mean: 2.690061721801758
step: 1650 loss_mean: 2.7188392972946165
step: 1700 loss_mean: 2.684179844856262
step: 1750 loss_mean: 2.7313558197021486
step: 1800 loss_mean: 2.7557528257369994
step: 1850 loss_mean: 2.6930939865112307
step: 1900 loss_mean: 2.7256620264053346
step: 1950 loss_mean: 2.741974425315857
step: 2000 loss_mean: 2.6882584857940675
step: 2050 loss_mean: 2.7046236610412597
step: 2100 loss_mean: 2.710185809135437
step: 2150 loss_mean: 2.7161760759353637
step: 2200 loss_mean: 2.700262260437012
step: 2250 loss_mean: 2.704945216178894
step: 2300 loss_mean: 2.694702410697937
step: 2350 loss_mean: 2.7272276878356934
step: 2400 loss_mean: 2.712046432495117
step: 2450 loss_mean: 2.6615470457077026
step: 2500 loss_mean: 2.7468158388137818
step: 2550 loss_mean: 2.716187791824341
step: 2600 loss_mean: 2.669457030296326
step: 2650 loss_mean: 2.6910472297668457
step: 2700 loss_mean: 2.6994817304611205
step: 2750 loss_mean: 2.718249979019165
step: 2800 loss_mean: 2.6466452646255494
step: 2850 loss_mean: 2.728719925880432
step: 2900 loss_mean: 2.7287697219848632
step: 2950 loss_mean: 2.7123216342926026
step: 3000 loss_mean: 2.720455193519592
step: 3050 loss_mean: 2.681766095161438
step: 3100 loss_mean: 2.6973442125320433
step: 50 loss_mean: 2.749653763771057
step: 100 loss_mean: 2.7816015005111696
step: 150 loss_mean: 2.7192658185958862
step: 200 loss_mean: 2.8093345832824705
step: 250 loss_mean: 2.7563431024551392
step: 300 loss_mean: 2.8474861860275267
step: 350 loss_mean: 2.7357268857955934
step: 400 loss_mean: 2.7315127944946287
step: 450 loss_mean: 2.724886474609375
step: 500 loss_mean: 2.794908242225647
step: 550 loss_mean: 2.733049750328064
step: 600 loss_mean: 2.6959283256530764
Epoch: 88 | Run time: 624.0 s | Train loss: 2.71 | Valid loss: 2.76
Saved checkpoint 88 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1300 s.
run_epoch
step: 50 loss_mean: 2.7213855314254762
step: 100 loss_mean: 2.6940418338775634
step: 150 loss_mean: 2.6978826379776
step: 200 loss_mean: 2.6817220497131347
step: 250 loss_mean: 2.6825943756103516
step: 300 loss_mean: 2.7014321231842042
step: 350 loss_mean: 2.723417892456055
step: 400 loss_mean: 2.7503342247009277
step: 450 loss_mean: 2.7221227264404297
step: 500 loss_mean: 2.6750792598724367
step: 550 loss_mean: 2.771110701560974
step: 600 loss_mean: 2.704312496185303
step: 650 loss_mean: 2.6655667066574096
step: 700 loss_mean: 2.7373980569839476
step: 750 loss_mean: 2.7296431732177733
step: 800 loss_mean: 2.6957205724716187
step: 850 loss_mean: 2.7000766944885255
step: 900 loss_mean: 2.7711044359207153
step: 950 loss_mean: 2.7227698707580568
step: 1000 loss_mean: 2.775359263420105
step: 1050 loss_mean: 2.699227647781372
step: 1100 loss_mean: 2.681557745933533
step: 1150 loss_mean: 2.6735557746887206
step: 1200 loss_mean: 2.7285516977310182
step: 1250 loss_mean: 2.767446966171265
step: 1300 loss_mean: 2.652237572669983
step: 1350 loss_mean: 2.6949972820281984
step: 1400 loss_mean: 2.6721037101745604
step: 1450 loss_mean: 2.7056363010406494
step: 1500 loss_mean: 2.706302580833435
step: 1550 loss_mean: 2.6954464673995973
step: 1600 loss_mean: 2.7336152172088624
step: 1650 loss_mean: 2.7042257404327392
step: 1700 loss_mean: 2.7598273277282717
step: 1750 loss_mean: 2.6924080944061277
step: 1800 loss_mean: 2.6961010599136355
step: 1850 loss_mean: 2.6810715055465697
step: 1900 loss_mean: 2.727451114654541
step: 1950 loss_mean: 2.7324007987976073
step: 2000 loss_mean: 2.755046510696411
step: 2050 loss_mean: 2.7448157453536988
step: 2100 loss_mean: 2.720457730293274
step: 2150 loss_mean: 2.6941480159759523
step: 2200 loss_mean: 2.6822538328170777
step: 2250 loss_mean: 2.700925488471985
step: 2300 loss_mean: 2.6917391300201414
step: 2350 loss_mean: 2.637587351799011
step: 2400 loss_mean: 2.713224720954895
step: 2450 loss_mean: 2.774537000656128
step: 2500 loss_mean: 2.6776898813247683
step: 2550 loss_mean: 2.683594241142273
step: 2600 loss_mean: 2.6870603466033938
step: 2650 loss_mean: 2.6621418237686156
step: 2700 loss_mean: 2.7122711658477785
step: 2750 loss_mean: 2.718664288520813
step: 2800 loss_mean: 2.7440050888061522
step: 2850 loss_mean: 2.709553294181824
step: 2900 loss_mean: 2.7083220052719117
step: 2950 loss_mean: 2.7594015073776244
step: 3000 loss_mean: 2.6483397340774535
step: 3050 loss_mean: 2.634275722503662
step: 3100 loss_mean: 2.7118747997283936
step: 50 loss_mean: 2.7513650369644167
step: 100 loss_mean: 2.787930850982666
step: 150 loss_mean: 2.7255067110061644
step: 200 loss_mean: 2.808816075325012
step: 250 loss_mean: 2.7613716459274293
step: 300 loss_mean: 2.8683549642562864
step: 350 loss_mean: 2.6995355558395384
step: 400 loss_mean: 2.716986308097839
step: 450 loss_mean: 2.7272606229782106
step: 500 loss_mean: 2.7648707962036134
step: 550 loss_mean: 2.7402139854431153
step: 600 loss_mean: 2.68728955745697
Epoch: 89 | Run time: 620.0 s | Train loss: 2.71 | Valid loss: 2.75
Saved checkpoint 89 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1296 s.
run_epoch
step: 50 loss_mean: 2.658092999458313
step: 100 loss_mean: 2.6243088483810424
step: 150 loss_mean: 2.733748426437378
step: 200 loss_mean: 2.745883040428162
step: 250 loss_mean: 2.738631844520569
step: 300 loss_mean: 2.6979978227615358
step: 350 loss_mean: 2.7175509309768677
step: 400 loss_mean: 2.697322120666504
step: 450 loss_mean: 2.712871870994568
step: 500 loss_mean: 2.7090194702148436
step: 550 loss_mean: 2.680685076713562
step: 600 loss_mean: 2.7476322937011717
step: 650 loss_mean: 2.7295387315750124
step: 700 loss_mean: 2.689325604438782
step: 750 loss_mean: 2.7396731901168825
step: 800 loss_mean: 2.71150399684906
step: 850 loss_mean: 2.7394097423553467
step: 900 loss_mean: 2.6860247898101806
step: 950 loss_mean: 2.705474462509155
step: 1000 loss_mean: 2.7040107679367065
step: 1050 loss_mean: 2.7229024887084963
step: 1100 loss_mean: 2.727774076461792
step: 1150 loss_mean: 2.694549870491028
step: 1200 loss_mean: 2.7013992476463318
step: 1250 loss_mean: 2.7271170139312746
step: 1300 loss_mean: 2.729379553794861
step: 1350 loss_mean: 2.7279241943359374
step: 1400 loss_mean: 2.732416281700134
step: 1450 loss_mean: 2.700500388145447
step: 1500 loss_mean: 2.714013485908508
step: 1550 loss_mean: 2.7288130950927734
step: 1600 loss_mean: 2.741915726661682
step: 1650 loss_mean: 2.7029427766799925
step: 1700 loss_mean: 2.766650915145874
step: 1750 loss_mean: 2.7569165515899656
step: 1800 loss_mean: 2.660777363777161
step: 1850 loss_mean: 2.6599753856658936
step: 1900 loss_mean: 2.677417769432068
step: 1950 loss_mean: 2.7387477207183837
step: 2000 loss_mean: 2.751931529045105
step: 2050 loss_mean: 2.692240333557129
step: 2100 loss_mean: 2.6592480373382568
step: 2150 loss_mean: 2.665436043739319
step: 2200 loss_mean: 2.69388973236084
step: 2250 loss_mean: 2.7113036823272707
step: 2300 loss_mean: 2.674330644607544
step: 2350 loss_mean: 2.683808698654175
step: 2400 loss_mean: 2.681364974975586
step: 2450 loss_mean: 2.6797804260253906
step: 2500 loss_mean: 2.719394497871399
step: 2550 loss_mean: 2.721295561790466
step: 2600 loss_mean: 2.7241481828689573
step: 2650 loss_mean: 2.6908614253997802
step: 2700 loss_mean: 2.7315846490859985
step: 2750 loss_mean: 2.6861991548538207
step: 2800 loss_mean: 2.7081604957580567
step: 2850 loss_mean: 2.725243511199951
step: 2900 loss_mean: 2.7057628202438355
step: 2950 loss_mean: 2.667148747444153
step: 3000 loss_mean: 2.7084655141830445
step: 3050 loss_mean: 2.6755149126052857
step: 3100 loss_mean: 2.7293052577972414
step: 50 loss_mean: 2.735931091308594
step: 100 loss_mean: 2.767591118812561
step: 150 loss_mean: 2.7067951822280882
step: 200 loss_mean: 2.8088176918029784
step: 250 loss_mean: 2.774307703971863
step: 300 loss_mean: 2.858320074081421
step: 350 loss_mean: 2.7062903165817263
step: 400 loss_mean: 2.7308146142959595
step: 450 loss_mean: 2.7154983949661253
step: 500 loss_mean: 2.7703043937683107
step: 550 loss_mean: 2.7246221351623534
step: 600 loss_mean: 2.683132586479187
Epoch: 90 | Run time: 623.0 s | Train loss: 2.71 | Valid loss: 2.75
Saved checkpoint 90 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1307 s.
run_epoch
step: 50 loss_mean: 2.716214904785156
step: 100 loss_mean: 2.643492798805237
step: 150 loss_mean: 2.6979455852508547
step: 200 loss_mean: 2.7099821949005127
step: 250 loss_mean: 2.7405010986328127
step: 300 loss_mean: 2.7414005041122436
step: 350 loss_mean: 2.6951759099960326
step: 400 loss_mean: 2.714518208503723
step: 450 loss_mean: 2.713109793663025
step: 500 loss_mean: 2.664128441810608
step: 550 loss_mean: 2.713901524543762
step: 600 loss_mean: 2.684132008552551
step: 650 loss_mean: 2.757431735992432
step: 700 loss_mean: 2.6916289949417114
step: 750 loss_mean: 2.767157640457153
step: 800 loss_mean: 2.679083757400513
step: 850 loss_mean: 2.7190841245651245
step: 900 loss_mean: 2.7135125637054442
step: 950 loss_mean: 2.713937463760376
step: 1000 loss_mean: 2.6704490900039675
step: 1050 loss_mean: 2.7195566082000733
step: 1100 loss_mean: 2.6520149183273314
step: 1150 loss_mean: 2.6776684141159057
step: 1200 loss_mean: 2.6791562461853027
step: 1250 loss_mean: 2.7000694417953492
step: 1300 loss_mean: 2.7236932277679444
step: 1350 loss_mean: 2.7086568689346313
step: 1400 loss_mean: 2.70078022480011
step: 1450 loss_mean: 2.6901279306411743
step: 1500 loss_mean: 2.708777928352356
step: 1550 loss_mean: 2.704941806793213
step: 1600 loss_mean: 2.7083700466156007
step: 1650 loss_mean: 2.7159166860580446
step: 1700 loss_mean: 2.6962110424041748
step: 1750 loss_mean: 2.692314863204956
step: 1800 loss_mean: 2.6618640184402467
step: 1850 loss_mean: 2.715845618247986
step: 1900 loss_mean: 2.7424968099594116
step: 1950 loss_mean: 2.707703223228455
step: 2000 loss_mean: 2.7115242767333982
step: 2050 loss_mean: 2.735837526321411
step: 2100 loss_mean: 2.7190043544769287
step: 2150 loss_mean: 2.6860869693756104
step: 2200 loss_mean: 2.6913720560073853
step: 2250 loss_mean: 2.6758231592178343
step: 2300 loss_mean: 2.7445838403701783
step: 2350 loss_mean: 2.671725721359253
step: 2400 loss_mean: 2.705299663543701
step: 2450 loss_mean: 2.7546506977081298
step: 2500 loss_mean: 2.6713718366622925
step: 2550 loss_mean: 2.6697555685043337
step: 2600 loss_mean: 2.6844534826278688
step: 2650 loss_mean: 2.656869468688965
step: 2700 loss_mean: 2.732721824645996
step: 2750 loss_mean: 2.74498131275177
step: 2800 loss_mean: 2.7612398052215577
step: 2850 loss_mean: 2.732135281562805
step: 2900 loss_mean: 2.752721071243286
step: 2950 loss_mean: 2.6791305923461914
step: 3000 loss_mean: 2.7274711418151854
step: 3050 loss_mean: 2.6893803167343138
step: 3100 loss_mean: 2.671105136871338
step: 50 loss_mean: 2.7295731496810913
step: 100 loss_mean: 2.76959912776947
step: 150 loss_mean: 2.6907105827331543
step: 200 loss_mean: 2.8083679628372193
step: 250 loss_mean: 2.7589247274398803
step: 300 loss_mean: 2.8489235973358156
step: 350 loss_mean: 2.7178005170822144
step: 400 loss_mean: 2.743062767982483
step: 450 loss_mean: 2.711669692993164
step: 500 loss_mean: 2.7642655324935914
step: 550 loss_mean: 2.719645161628723
step: 600 loss_mean: 2.676163425445557
Epoch: 91 | Run time: 623.0 s | Train loss: 2.71 | Valid loss: 2.75
Saved checkpoint 91 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1337 s.
run_epoch
step: 50 loss_mean: 2.744571862220764
step: 100 loss_mean: 2.72894211769104
step: 150 loss_mean: 2.7633264780044557
step: 200 loss_mean: 2.7515749979019164
step: 250 loss_mean: 2.6841840839385984
step: 300 loss_mean: 2.7124271154403687
step: 350 loss_mean: 2.7005255937576296
step: 400 loss_mean: 2.7237834692001344
step: 450 loss_mean: 2.6898451471328735
step: 500 loss_mean: 2.735612678527832
step: 550 loss_mean: 2.706253547668457
step: 600 loss_mean: 2.7223535490036013
step: 650 loss_mean: 2.6789578866958617
step: 700 loss_mean: 2.656915879249573
step: 750 loss_mean: 2.6943934965133667
step: 800 loss_mean: 2.690987982749939
step: 850 loss_mean: 2.7219850397109986
step: 900 loss_mean: 2.708684120178223
step: 950 loss_mean: 2.711107840538025
step: 1000 loss_mean: 2.7564962196350096
step: 1050 loss_mean: 2.709990372657776
step: 1100 loss_mean: 2.6851028871536253
step: 1150 loss_mean: 2.628869411945343
step: 1200 loss_mean: 2.6907874870300295
step: 1250 loss_mean: 2.72064001083374
step: 1300 loss_mean: 2.708441252708435
step: 1350 loss_mean: 2.7019499826431272
step: 1400 loss_mean: 2.742979474067688
step: 1450 loss_mean: 2.7469509983062745
step: 1500 loss_mean: 2.6906764459609986
step: 1550 loss_mean: 2.6713072872161865
step: 1600 loss_mean: 2.6613869094848632
step: 1650 loss_mean: 2.6642447090148926
step: 1700 loss_mean: 2.759975643157959
step: 1750 loss_mean: 2.7205292797088623
step: 1800 loss_mean: 2.6938912057876587
step: 1850 loss_mean: 2.7314222955703737
step: 1900 loss_mean: 2.7291993856430055
step: 1950 loss_mean: 2.6806517362594606
step: 2000 loss_mean: 2.7044037532806398
step: 2050 loss_mean: 2.6553791761398315
step: 2100 loss_mean: 2.6824833154678345
step: 2150 loss_mean: 2.673918881416321
step: 2200 loss_mean: 2.685914692878723
step: 2250 loss_mean: 2.697976508140564
step: 2300 loss_mean: 2.7207365894317626
step: 2350 loss_mean: 2.6594753098487853
step: 2400 loss_mean: 2.7237427043914795
step: 2450 loss_mean: 2.6772850561141968
step: 2500 loss_mean: 2.747457594871521
step: 2550 loss_mean: 2.7370283603668213
step: 2600 loss_mean: 2.71370379447937
step: 2650 loss_mean: 2.685116629600525
step: 2700 loss_mean: 2.669280605316162
step: 2750 loss_mean: 2.727863240242004
step: 2800 loss_mean: 2.741175570487976
step: 2850 loss_mean: 2.657796230316162
step: 2900 loss_mean: 2.7180047845840454
step: 2950 loss_mean: 2.6799674606323243
step: 3000 loss_mean: 2.688547797203064
step: 3050 loss_mean: 2.7645297336578367
step: 3100 loss_mean: 2.704321928024292
step: 50 loss_mean: 2.7728105640411376
step: 100 loss_mean: 2.8019908761978147
step: 150 loss_mean: 2.7281988048553467
step: 200 loss_mean: 2.8155474853515625
step: 250 loss_mean: 2.7978008794784546
step: 300 loss_mean: 2.851206088066101
step: 350 loss_mean: 2.765154037475586
step: 400 loss_mean: 2.76152316570282
step: 450 loss_mean: 2.737969632148743
step: 500 loss_mean: 2.7938298559188843
step: 550 loss_mean: 2.734827699661255
step: 600 loss_mean: 2.6918438196182253
Epoch: 92 | Run time: 622.0 s | Train loss: 2.71 | Valid loss: 2.77
Saved checkpoint 92 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1336 s.
run_epoch
step: 50 loss_mean: 2.6789487838745116
step: 100 loss_mean: 2.6509951066970827
step: 150 loss_mean: 2.7182318496704103
step: 200 loss_mean: 2.6867346572875976
step: 250 loss_mean: 2.6907782888412477
step: 300 loss_mean: 2.702159914970398
step: 350 loss_mean: 2.6833642148971557
step: 400 loss_mean: 2.725697913169861
step: 450 loss_mean: 2.6843998336791994
step: 500 loss_mean: 2.69140017747879
step: 550 loss_mean: 2.718529534339905
step: 600 loss_mean: 2.7501544427871703
step: 650 loss_mean: 2.679203815460205
step: 700 loss_mean: 2.6995120906829833
step: 750 loss_mean: 2.6838307762145996
step: 800 loss_mean: 2.690244436264038
step: 850 loss_mean: 2.741488256454468
step: 900 loss_mean: 2.7305630111694335
step: 950 loss_mean: 2.718257031440735
step: 1000 loss_mean: 2.6942982721328734
step: 1050 loss_mean: 2.657734122276306
step: 1100 loss_mean: 2.704820599555969
step: 1150 loss_mean: 2.727259612083435
step: 1200 loss_mean: 2.6494393396377562
step: 1250 loss_mean: 2.7281132793426512
step: 1300 loss_mean: 2.7228563022613526
step: 1350 loss_mean: 2.6884490346908567
step: 1400 loss_mean: 2.6981622791290283
step: 1450 loss_mean: 2.6751416730880737
step: 1500 loss_mean: 2.7451140308380126
step: 1550 loss_mean: 2.6986432552337645
step: 1600 loss_mean: 2.6670549535751342
step: 1650 loss_mean: 2.7121094465255737
step: 1700 loss_mean: 2.7146868181228636
step: 1750 loss_mean: 2.7347397136688234
step: 1800 loss_mean: 2.6683501195907593
step: 1850 loss_mean: 2.651366534233093
step: 1900 loss_mean: 2.7228391122817994
step: 1950 loss_mean: 2.7262541389465333
step: 2000 loss_mean: 2.68495662689209
step: 2050 loss_mean: 2.6888726711273194
step: 2100 loss_mean: 2.6752035093307494
step: 2150 loss_mean: 2.712592115402222
step: 2200 loss_mean: 2.722361125946045
step: 2250 loss_mean: 2.6967722177505493
step: 2300 loss_mean: 2.7197801446914673
step: 2350 loss_mean: 2.6457093286514284
step: 2400 loss_mean: 2.727650957107544
step: 2450 loss_mean: 2.7527015304565428
step: 2500 loss_mean: 2.711451086997986
step: 2550 loss_mean: 2.760485777854919
step: 2600 loss_mean: 2.747038221359253
step: 2650 loss_mean: 2.712348599433899
step: 2700 loss_mean: 2.7259260749816896
step: 2750 loss_mean: 2.654910559654236
step: 2800 loss_mean: 2.669262113571167
step: 2850 loss_mean: 2.661302809715271
step: 2900 loss_mean: 2.7644221067428587
step: 2950 loss_mean: 2.7215847635269164
step: 3000 loss_mean: 2.6914148139953613
step: 3050 loss_mean: 2.7065782880783082
step: 3100 loss_mean: 2.730948257446289
step: 50 loss_mean: 2.7954324865341187
step: 100 loss_mean: 2.8176738309860228
step: 150 loss_mean: 2.754629693031311
step: 200 loss_mean: 2.8811521339416504
step: 250 loss_mean: 2.8090702724456786
step: 300 loss_mean: 2.8796029806137087
step: 350 loss_mean: 2.7556816291809083
step: 400 loss_mean: 2.7991360998153687
step: 450 loss_mean: 2.7466165256500243
step: 500 loss_mean: 2.8116609477996826
step: 550 loss_mean: 2.7810116815567016
step: 600 loss_mean: 2.7220537662506104
Epoch: 93 | Run time: 622.0 s | Train loss: 2.7 | Valid loss: 2.8
Saved checkpoint 93 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1350 s.
run_epoch
step: 50 loss_mean: 2.688362550735474
step: 100 loss_mean: 2.736865119934082
step: 150 loss_mean: 2.7325077629089356
step: 200 loss_mean: 2.760703649520874
step: 250 loss_mean: 2.66562837600708
step: 300 loss_mean: 2.717815890312195
step: 350 loss_mean: 2.67839777469635
step: 400 loss_mean: 2.7439357614517212
step: 450 loss_mean: 2.749025821685791
step: 500 loss_mean: 2.683549461364746
step: 550 loss_mean: 2.685452127456665
step: 600 loss_mean: 2.7465108394622804
step: 650 loss_mean: 2.6703819036483765
step: 700 loss_mean: 2.7000198316574098
step: 750 loss_mean: 2.7074569845199585
step: 800 loss_mean: 2.7062653255462648
step: 850 loss_mean: 2.682251453399658
step: 900 loss_mean: 2.7322394275665283
step: 950 loss_mean: 2.749511742591858
step: 1000 loss_mean: 2.6868403339385987
step: 1050 loss_mean: 2.6951374626159668
step: 1100 loss_mean: 2.698132286071777
step: 1150 loss_mean: 2.6699349880218506
step: 1200 loss_mean: 2.7053501415252685
step: 1250 loss_mean: 2.699231481552124
step: 1300 loss_mean: 2.692419562339783
step: 1350 loss_mean: 2.7076984071731567
step: 1400 loss_mean: 2.787458415031433
step: 1450 loss_mean: 2.6800637578964235
step: 1500 loss_mean: 2.65520387172699
step: 1550 loss_mean: 2.6307896614074706
step: 1600 loss_mean: 2.6526307678222656
step: 1650 loss_mean: 2.7159727716445925
step: 1700 loss_mean: 2.699741415977478
step: 1750 loss_mean: 2.7098884773254395
step: 1800 loss_mean: 2.6940029525756835
step: 1850 loss_mean: 2.680404896736145
step: 1900 loss_mean: 2.6982733535766603
step: 1950 loss_mean: 2.673823843002319
step: 2000 loss_mean: 2.713890438079834
step: 2050 loss_mean: 2.7182218503952025
step: 2100 loss_mean: 2.6725897121429445
step: 2150 loss_mean: 2.7036722755432128
step: 2200 loss_mean: 2.676005277633667
step: 2250 loss_mean: 2.707173476219177
step: 2300 loss_mean: 2.7124096632003782
step: 2350 loss_mean: 2.719984140396118
step: 2400 loss_mean: 2.653943099975586
step: 2450 loss_mean: 2.783199791908264
step: 2500 loss_mean: 2.7851016616821287
step: 2550 loss_mean: 2.7173734664916993
step: 2600 loss_mean: 2.6537520933151244
step: 2650 loss_mean: 2.731318030357361
step: 2700 loss_mean: 2.7290319919586183
step: 2750 loss_mean: 2.7052296733856203
step: 2800 loss_mean: 2.6572115135192873
step: 2850 loss_mean: 2.716923484802246
step: 2900 loss_mean: 2.7042212677001953
step: 2950 loss_mean: 2.68861701965332
step: 3000 loss_mean: 2.6774170446395873
step: 3050 loss_mean: 2.643806595802307
step: 3100 loss_mean: 2.690254545211792
step: 50 loss_mean: 2.738622851371765
step: 100 loss_mean: 2.7780954122543333
step: 150 loss_mean: 2.724244780540466
step: 200 loss_mean: 2.804044723510742
step: 250 loss_mean: 2.760509490966797
step: 300 loss_mean: 2.8470791244506835
step: 350 loss_mean: 2.7148367738723755
step: 400 loss_mean: 2.7438593816757204
step: 450 loss_mean: 2.7186124992370604
step: 500 loss_mean: 2.7775199222564697
step: 550 loss_mean: 2.72891366481781
step: 600 loss_mean: 2.681513538360596
Epoch: 94 | Run time: 625.0 s | Train loss: 2.7 | Valid loss: 2.75
Saved checkpoint 94 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1360 s.
run_epoch
step: 50 loss_mean: 2.7329917526245118
step: 100 loss_mean: 2.7570752620697023
step: 150 loss_mean: 2.7197751092910765
step: 200 loss_mean: 2.7028418827056884
step: 250 loss_mean: 2.663752403259277
step: 300 loss_mean: 2.6393788051605225
step: 350 loss_mean: 2.6820406913757324
step: 400 loss_mean: 2.7185476922988894
step: 450 loss_mean: 2.720937976837158
step: 500 loss_mean: 2.6844434213638304
step: 550 loss_mean: 2.7219053602218626
step: 600 loss_mean: 2.6939579534530638
step: 650 loss_mean: 2.699732346534729
step: 700 loss_mean: 2.723271951675415
step: 750 loss_mean: 2.706501450538635
step: 800 loss_mean: 2.7333581495285033
step: 850 loss_mean: 2.660479712486267
step: 900 loss_mean: 2.666696238517761
step: 950 loss_mean: 2.706533932685852
step: 1000 loss_mean: 2.729140210151672
step: 1050 loss_mean: 2.712687301635742
step: 1100 loss_mean: 2.7281179237365722
step: 1150 loss_mean: 2.7301392698287965
step: 1200 loss_mean: 2.765203108787537
step: 1250 loss_mean: 2.67045220375061
step: 1300 loss_mean: 2.7103878355026243
step: 1350 loss_mean: 2.694855046272278
step: 1400 loss_mean: 2.6891075372695923
step: 1450 loss_mean: 2.6459838438034056
step: 1500 loss_mean: 2.6884488010406495
step: 1550 loss_mean: 2.7638542461395263
step: 1600 loss_mean: 2.6863045740127562
step: 1650 loss_mean: 2.6636254739761354
step: 1700 loss_mean: 2.666846525669098
step: 1750 loss_mean: 2.678344798088074
step: 1800 loss_mean: 2.718338379859924
step: 1850 loss_mean: 2.72247670173645
step: 1900 loss_mean: 2.7022991132736207
step: 1950 loss_mean: 2.7089287519454954
step: 2000 loss_mean: 2.720994863510132
step: 2050 loss_mean: 2.6720895910263063
step: 2100 loss_mean: 2.6626998805999755
step: 2150 loss_mean: 2.7080496978759765
step: 2200 loss_mean: 2.686407413482666
step: 2250 loss_mean: 2.7198003005981444
step: 2300 loss_mean: 2.7427654123306273
step: 2350 loss_mean: 2.6744829273223876
step: 2400 loss_mean: 2.748945212364197
step: 2450 loss_mean: 2.6904559469223024
step: 2500 loss_mean: 2.7685255575180054
step: 2550 loss_mean: 2.7161540842056273
step: 2600 loss_mean: 2.714355993270874
step: 2650 loss_mean: 2.6810431051254273
step: 2700 loss_mean: 2.7622568511962893
step: 2750 loss_mean: 2.6631681966781615
step: 2800 loss_mean: 2.7033309030532835
step: 2850 loss_mean: 2.6973687553405763
step: 2900 loss_mean: 2.7123642635345457
step: 2950 loss_mean: 2.696639747619629
step: 3000 loss_mean: 2.728372735977173
step: 3050 loss_mean: 2.6736441373825075
step: 3100 loss_mean: 2.702789783477783
step: 50 loss_mean: 2.758477420806885
step: 100 loss_mean: 2.792939043045044
step: 150 loss_mean: 2.7275933265686034
step: 200 loss_mean: 2.810106782913208
step: 250 loss_mean: 2.7804476737976076
step: 300 loss_mean: 2.8520869255065917
step: 350 loss_mean: 2.7541592121124268
step: 400 loss_mean: 2.758086223602295
step: 450 loss_mean: 2.7451071453094484
step: 500 loss_mean: 2.7912368106842043
step: 550 loss_mean: 2.7332177639007567
step: 600 loss_mean: 2.7018742895126344
Epoch: 95 | Run time: 620.0 s | Train loss: 2.7 | Valid loss: 2.77
Saved checkpoint 95 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1380 s.
run_epoch
step: 50 loss_mean: 2.705370478630066
step: 100 loss_mean: 2.741589512825012
step: 150 loss_mean: 2.686271824836731
step: 200 loss_mean: 2.7288930654525756
step: 250 loss_mean: 2.703688006401062
step: 300 loss_mean: 2.6610319900512693
step: 350 loss_mean: 2.666814365386963
step: 400 loss_mean: 2.681106538772583
step: 450 loss_mean: 2.732176365852356
step: 500 loss_mean: 2.735384855270386
step: 550 loss_mean: 2.693249249458313
step: 600 loss_mean: 2.6703963756561278
step: 650 loss_mean: 2.719797353744507
step: 700 loss_mean: 2.662826919555664
step: 750 loss_mean: 2.672601227760315
step: 800 loss_mean: 2.6772086429595947
step: 850 loss_mean: 2.746237835884094
step: 900 loss_mean: 2.7283899641036986
step: 950 loss_mean: 2.7608746480941773
step: 1000 loss_mean: 2.7076247215270994
step: 1050 loss_mean: 2.7216447019577026
step: 1100 loss_mean: 2.7230004262924195
step: 1150 loss_mean: 2.6851117992401123
step: 1200 loss_mean: 2.6446391105651856
step: 1250 loss_mean: 2.7048549270629882
step: 1300 loss_mean: 2.6383595633506776
step: 1350 loss_mean: 2.6717754220962524
step: 1400 loss_mean: 2.739646348953247
step: 1450 loss_mean: 2.7098730564117433
step: 1500 loss_mean: 2.723989734649658
step: 1550 loss_mean: 2.673269624710083
step: 1600 loss_mean: 2.674496126174927
step: 1650 loss_mean: 2.730271506309509
step: 1700 loss_mean: 2.6813438081741334
step: 1750 loss_mean: 2.7121788692474365
step: 1800 loss_mean: 2.7158917951583863
step: 1850 loss_mean: 2.712848176956177
step: 1900 loss_mean: 2.672531566619873
step: 1950 loss_mean: 2.7165691804885865
step: 2000 loss_mean: 2.695316181182861
step: 2050 loss_mean: 2.6466922092437746
step: 2100 loss_mean: 2.6921831226348876
step: 2150 loss_mean: 2.6980252504348754
step: 2200 loss_mean: 2.614152092933655
step: 2250 loss_mean: 2.7209468936920165
step: 2300 loss_mean: 2.7179779767990113
step: 2350 loss_mean: 2.715644750595093
step: 2400 loss_mean: 2.6845939350128174
step: 2450 loss_mean: 2.722898688316345
step: 2500 loss_mean: 2.6897121047973633
step: 2550 loss_mean: 2.7278710126876833
step: 2600 loss_mean: 2.6674404907226563
step: 2650 loss_mean: 2.7358816862106323
step: 2700 loss_mean: 2.7377361536026
step: 2750 loss_mean: 2.7114259719848635
step: 2800 loss_mean: 2.7091462659835814
step: 2850 loss_mean: 2.6922560691833497
step: 2900 loss_mean: 2.688679060935974
step: 2950 loss_mean: 2.725306158065796
step: 3000 loss_mean: 2.745907030105591
step: 3050 loss_mean: 2.7007527446746824
step: 3100 loss_mean: 2.7470814275741575
step: 50 loss_mean: 2.747934422492981
step: 100 loss_mean: 2.784091672897339
step: 150 loss_mean: 2.7201989889144897
step: 200 loss_mean: 2.8037363290786743
step: 250 loss_mean: 2.7539741086959837
step: 300 loss_mean: 2.854451313018799
step: 350 loss_mean: 2.7185596942901613
step: 400 loss_mean: 2.7361183023452758
step: 450 loss_mean: 2.7224936437606813
step: 500 loss_mean: 2.7779189348220825
step: 550 loss_mean: 2.729259419441223
step: 600 loss_mean: 2.6905845260620116
Epoch: 96 | Run time: 625.0 s | Train loss: 2.7 | Valid loss: 2.75
Saved checkpoint 96 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1390 s.
run_epoch
step: 50 loss_mean: 2.7117673778533935
step: 100 loss_mean: 2.6582122087478637
step: 150 loss_mean: 2.6921997117996215
step: 200 loss_mean: 2.7251726627349853
step: 250 loss_mean: 2.678425335884094
step: 300 loss_mean: 2.7275423765182496
step: 350 loss_mean: 2.68524893283844
step: 400 loss_mean: 2.7334987831115725
step: 450 loss_mean: 2.757913365364075
step: 500 loss_mean: 2.744244737625122
step: 550 loss_mean: 2.7872693729400635
step: 600 loss_mean: 2.7929653215408323
step: 650 loss_mean: 2.677941012382507
step: 700 loss_mean: 2.672271366119385
step: 750 loss_mean: 2.701430125236511
step: 800 loss_mean: 2.7569782447814943
step: 850 loss_mean: 2.698519940376282
step: 900 loss_mean: 2.6880962896347045
step: 950 loss_mean: 2.690511145591736
step: 1000 loss_mean: 2.6894939804077147
step: 1050 loss_mean: 2.727554802894592
step: 1100 loss_mean: 2.6878786849975587
step: 1150 loss_mean: 2.7376909828186036
step: 1200 loss_mean: 2.658722801208496
step: 1250 loss_mean: 2.658193335533142
step: 1300 loss_mean: 2.7361770915985106
step: 1350 loss_mean: 2.730435600280762
step: 1400 loss_mean: 2.702931046485901
step: 1450 loss_mean: 2.6606982946395874
step: 1500 loss_mean: 2.7268822383880615
step: 1550 loss_mean: 2.7103446769714354
step: 1600 loss_mean: 2.6954160356521606
step: 1650 loss_mean: 2.722938027381897
step: 1700 loss_mean: 2.6862860107421875
step: 1750 loss_mean: 2.6850991916656493
step: 1800 loss_mean: 2.710493941307068
step: 1850 loss_mean: 2.7116279888153074
step: 1900 loss_mean: 2.672785086631775
step: 1950 loss_mean: 2.7045332098007204
step: 2000 loss_mean: 2.709211058616638
step: 2050 loss_mean: 2.742035541534424
step: 2100 loss_mean: 2.663060622215271
step: 2150 loss_mean: 2.66082444190979
step: 2200 loss_mean: 2.7096542072296144
step: 2250 loss_mean: 2.6494427156448364
step: 2300 loss_mean: 2.748971405029297
step: 2350 loss_mean: 2.7435552740097044
step: 2400 loss_mean: 2.630907082557678
step: 2450 loss_mean: 2.7153756046295165
step: 2500 loss_mean: 2.7417034101486206
step: 2550 loss_mean: 2.6416412019729614
step: 2600 loss_mean: 2.6090572690963745
step: 2650 loss_mean: 2.6854098987579347
step: 2700 loss_mean: 2.6725654792785645
step: 2750 loss_mean: 2.6929825592041015
step: 2800 loss_mean: 2.689871282577515
step: 2850 loss_mean: 2.693528776168823
step: 2900 loss_mean: 2.7115909385681154
step: 2950 loss_mean: 2.7433843564987184
step: 3000 loss_mean: 2.6664833068847655
step: 3050 loss_mean: 2.6721698379516603
step: 3100 loss_mean: 2.7350743198394776
step: 50 loss_mean: 2.7355097007751463
step: 100 loss_mean: 2.784921088218689
step: 150 loss_mean: 2.71322368144989
step: 200 loss_mean: 2.8041856908798217
step: 250 loss_mean: 2.7738072443008424
step: 300 loss_mean: 2.861631226539612
step: 350 loss_mean: 2.706180143356323
step: 400 loss_mean: 2.720775499343872
step: 450 loss_mean: 2.7159065866470335
step: 500 loss_mean: 2.7811172246932983
step: 550 loss_mean: 2.7275091409683228
step: 600 loss_mean: 2.690734348297119
Epoch: 97 | Run time: 625.0 s | Train loss: 2.7 | Valid loss: 2.75
Saved checkpoint 97 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1404 s.
run_epoch
step: 50 loss_mean: 2.7032579374313355
step: 100 loss_mean: 2.693821597099304
step: 150 loss_mean: 2.7198293590545655
step: 200 loss_mean: 2.6957376861572264
step: 250 loss_mean: 2.716044478416443
step: 300 loss_mean: 2.6820960569381715
step: 350 loss_mean: 2.7163764810562134
step: 400 loss_mean: 2.669916577339172
step: 450 loss_mean: 2.7669705295562745
step: 500 loss_mean: 2.68950963973999
step: 550 loss_mean: 2.697981638908386
step: 600 loss_mean: 2.7117869901657103
step: 650 loss_mean: 2.7070398712158203
step: 700 loss_mean: 2.7423179244995115
step: 750 loss_mean: 2.701991958618164
step: 800 loss_mean: 2.688904891014099
step: 850 loss_mean: 2.6832538652420044
step: 900 loss_mean: 2.60311475276947
step: 950 loss_mean: 2.703484559059143
step: 1000 loss_mean: 2.7327422285079956
step: 1050 loss_mean: 2.6601816320419314
step: 1100 loss_mean: 2.7043849563598634
step: 1150 loss_mean: 2.6757930660247804
step: 1200 loss_mean: 2.6668576860427855
step: 1250 loss_mean: 2.6853254652023315
step: 1300 loss_mean: 2.731947073936462
step: 1350 loss_mean: 2.6865986299514772
step: 1400 loss_mean: 2.6909634494781494
step: 1450 loss_mean: 2.734434051513672
step: 1500 loss_mean: 2.6799706506729124
step: 1550 loss_mean: 2.7632181549072268
step: 1600 loss_mean: 2.692458701133728
step: 1650 loss_mean: 2.7253064727783203
step: 1700 loss_mean: 2.71055823802948
step: 1750 loss_mean: 2.6731385135650636
step: 1800 loss_mean: 2.660788908004761
step: 1850 loss_mean: 2.6999673080444335
step: 1900 loss_mean: 2.7090988779067993
step: 1950 loss_mean: 2.7191997003555297
step: 2000 loss_mean: 2.6542869186401368
step: 2050 loss_mean: 2.714458932876587
step: 2100 loss_mean: 2.6534343814849852
step: 2150 loss_mean: 2.7520327520370484
step: 2200 loss_mean: 2.7206550645828247
step: 2250 loss_mean: 2.7004686307907106
step: 2300 loss_mean: 2.7212035703659057
step: 2350 loss_mean: 2.7363163328170774
step: 2400 loss_mean: 2.6987315368652345
step: 2450 loss_mean: 2.7439760875701906
step: 2500 loss_mean: 2.7276211071014402
step: 2550 loss_mean: 2.683404903411865
step: 2600 loss_mean: 2.62686363697052
step: 2650 loss_mean: 2.7368034601211546
step: 2700 loss_mean: 2.678186950683594
step: 2750 loss_mean: 2.71648485660553
step: 2800 loss_mean: 2.702245488166809
step: 2850 loss_mean: 2.70692608833313
step: 2900 loss_mean: 2.7339305686950683
step: 2950 loss_mean: 2.727721304893494
step: 3000 loss_mean: 2.6983933019638062
step: 3050 loss_mean: 2.686267728805542
step: 3100 loss_mean: 2.7130429553985596
step: 50 loss_mean: 2.7590003204345703
step: 100 loss_mean: 2.817854948043823
step: 150 loss_mean: 2.7345209980010985
step: 200 loss_mean: 2.831843638420105
step: 250 loss_mean: 2.782953796386719
step: 300 loss_mean: 2.8799950551986693
step: 350 loss_mean: 2.7249786281585693
step: 400 loss_mean: 2.7510241413116456
step: 450 loss_mean: 2.7542740154266356
step: 500 loss_mean: 2.7977948141098024
step: 550 loss_mean: 2.7445446634292603
step: 600 loss_mean: 2.704957437515259
Epoch: 98 | Run time: 622.0 s | Train loss: 2.7 | Valid loss: 2.78
Saved checkpoint 98 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1421 s.
run_epoch
step: 50 loss_mean: 2.687575588226318
step: 100 loss_mean: 2.6850007343292237
step: 150 loss_mean: 2.7096140289306643
step: 200 loss_mean: 2.689983158111572
step: 250 loss_mean: 2.662850122451782
step: 300 loss_mean: 2.678282904624939
step: 350 loss_mean: 2.6931473302841185
step: 400 loss_mean: 2.7556912088394165
step: 450 loss_mean: 2.674377408027649
step: 500 loss_mean: 2.7276469469070435
step: 550 loss_mean: 2.6835104751586916
step: 600 loss_mean: 2.725839066505432
step: 650 loss_mean: 2.71382239818573
step: 700 loss_mean: 2.6592779302597047
step: 750 loss_mean: 2.682152814865112
step: 800 loss_mean: 2.777213940620422
step: 850 loss_mean: 2.6760111618041993
step: 900 loss_mean: 2.694876027107239
step: 950 loss_mean: 2.7434581089019776
step: 1000 loss_mean: 2.718875107765198
step: 1050 loss_mean: 2.6482066679000855
step: 1100 loss_mean: 2.7599849128723144
step: 1150 loss_mean: 2.740309953689575
step: 1200 loss_mean: 2.6832613563537597
step: 1250 loss_mean: 2.6928461313247682
step: 1300 loss_mean: 2.730959873199463
step: 1350 loss_mean: 2.764568610191345
step: 1400 loss_mean: 2.719110984802246
step: 1450 loss_mean: 2.6310738801956175
step: 1500 loss_mean: 2.6898681259155275
step: 1550 loss_mean: 2.689156718254089
step: 1600 loss_mean: 2.7094503021240235
step: 1650 loss_mean: 2.736001434326172
step: 1700 loss_mean: 2.6508742141723634
step: 1750 loss_mean: 2.7084198236465453
step: 1800 loss_mean: 2.7318720388412476
step: 1850 loss_mean: 2.7133800888061526
step: 1900 loss_mean: 2.6885492992401123
step: 1950 loss_mean: 2.71310781955719
step: 2000 loss_mean: 2.697781147956848
step: 2050 loss_mean: 2.671732521057129
step: 2100 loss_mean: 2.773948926925659
step: 2150 loss_mean: 2.711144332885742
step: 2200 loss_mean: 2.724574327468872
step: 2250 loss_mean: 2.7039226484298706
step: 2300 loss_mean: 2.716608557701111
step: 2350 loss_mean: 2.680493793487549
step: 2400 loss_mean: 2.699271140098572
step: 2450 loss_mean: 2.6890107870101927
step: 2500 loss_mean: 2.6685547685623168
step: 2550 loss_mean: 2.684219398498535
step: 2600 loss_mean: 2.7070341444015504
step: 2650 loss_mean: 2.733154602050781
step: 2700 loss_mean: 2.6900065898895265
step: 2750 loss_mean: 2.6630131292343138
step: 2800 loss_mean: 2.6870850419998167
step: 2850 loss_mean: 2.645950450897217
step: 2900 loss_mean: 2.6896458673477173
step: 2950 loss_mean: 2.7040787649154665
step: 3000 loss_mean: 2.708831777572632
step: 3050 loss_mean: 2.6718079948425295
step: 3100 loss_mean: 2.6674867486953735
step: 50 loss_mean: 2.7495835065841674
step: 100 loss_mean: 2.792894010543823
step: 150 loss_mean: 2.7142542123794557
step: 200 loss_mean: 2.8262474584579467
step: 250 loss_mean: 2.7794377183914185
step: 300 loss_mean: 2.8722951507568357
step: 350 loss_mean: 2.70350510597229
step: 400 loss_mean: 2.7438683795928953
step: 450 loss_mean: 2.718688979148865
step: 500 loss_mean: 2.7684007263183594
step: 550 loss_mean: 2.7565967416763306
step: 600 loss_mean: 2.6672095823287965
Epoch: 99 | Run time: 615.0 s | Train loss: 2.7 | Valid loss: 2.76
Saved checkpoint 99 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1430 s.
run_epoch
step: 50 loss_mean: 2.6618442964553832
step: 100 loss_mean: 2.698375816345215
step: 150 loss_mean: 2.7091721200942995
step: 200 loss_mean: 2.7609583234786985
step: 250 loss_mean: 2.669169716835022
step: 300 loss_mean: 2.721263041496277
step: 350 loss_mean: 2.7152878856658935
step: 400 loss_mean: 2.70509801864624
step: 450 loss_mean: 2.756347584724426
step: 500 loss_mean: 2.7080170917510986
step: 550 loss_mean: 2.7392735195159914
step: 600 loss_mean: 2.6932875537872314
step: 650 loss_mean: 2.6679552459716795
step: 700 loss_mean: 2.7231357431411745
step: 750 loss_mean: 2.707179832458496
step: 800 loss_mean: 2.6733278369903566
step: 850 loss_mean: 2.692483148574829
step: 900 loss_mean: 2.70154013633728
step: 950 loss_mean: 2.7109257698059084
step: 1000 loss_mean: 2.7048195219039917
step: 1050 loss_mean: 2.7311464595794677
step: 1100 loss_mean: 2.6811719226837156
step: 1150 loss_mean: 2.6863798904418945
step: 1200 loss_mean: 2.6801954460144044
step: 1250 loss_mean: 2.6693289566040037
step: 1300 loss_mean: 2.735399451255798
step: 1350 loss_mean: 2.770079560279846
step: 1400 loss_mean: 2.7485116147994995
step: 1450 loss_mean: 2.635871801376343
step: 1500 loss_mean: 2.689022979736328
step: 1550 loss_mean: 2.710096197128296
step: 1600 loss_mean: 2.6706251764297484
step: 1650 loss_mean: 2.709390525817871
step: 1700 loss_mean: 2.6691091680526733
step: 1750 loss_mean: 2.6374281549453737
step: 1800 loss_mean: 2.712757501602173
step: 1850 loss_mean: 2.702622299194336
step: 1900 loss_mean: 2.711750922203064
step: 1950 loss_mean: 2.7502270793914794
step: 2000 loss_mean: 2.680734715461731
step: 2050 loss_mean: 2.719832010269165
step: 2100 loss_mean: 2.7147333240509033
step: 2150 loss_mean: 2.703878140449524
step: 2200 loss_mean: 2.7059452390670775
step: 2250 loss_mean: 2.7134813261032105
step: 2300 loss_mean: 2.6913611268997193
step: 2350 loss_mean: 2.695968680381775
step: 2400 loss_mean: 2.736742420196533
step: 2450 loss_mean: 2.6717848920822145
step: 2500 loss_mean: 2.691881327629089
step: 2550 loss_mean: 2.7077821683883667
step: 2600 loss_mean: 2.7048134326934816
step: 2650 loss_mean: 2.6788293600082396
step: 2700 loss_mean: 2.7157138919830324
step: 2750 loss_mean: 2.7010255527496336
step: 2800 loss_mean: 2.6801329374313356
step: 2850 loss_mean: 2.691859173774719
step: 2900 loss_mean: 2.681836013793945
step: 2950 loss_mean: 2.618960461616516
step: 3000 loss_mean: 2.7379484081268313
step: 3050 loss_mean: 2.68427743434906
step: 3100 loss_mean: 2.6823195028305054
step: 50 loss_mean: 2.7315635108947753
step: 100 loss_mean: 2.7822016525268554
step: 150 loss_mean: 2.715277285575867
step: 200 loss_mean: 2.8077755546569825
step: 250 loss_mean: 2.757043480873108
step: 300 loss_mean: 2.8530162954330445
step: 350 loss_mean: 2.7120866179466248
step: 400 loss_mean: 2.7532293510437014
step: 450 loss_mean: 2.702475657463074
step: 500 loss_mean: 2.785575909614563
step: 550 loss_mean: 2.733914475440979
step: 600 loss_mean: 2.682592763900757
Epoch: 100 | Run time: 624.0 s | Train loss: 2.7 | Valid loss: 2.75
Saved checkpoint 100 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1441 s.
run_epoch
step: 50 loss_mean: 2.6932835245132445
step: 100 loss_mean: 2.7399706745147707
step: 150 loss_mean: 2.6399025964736937
step: 200 loss_mean: 2.741757674217224
step: 250 loss_mean: 2.7268790578842164
step: 300 loss_mean: 2.6489084815979003
step: 350 loss_mean: 2.7492877674102782
step: 400 loss_mean: 2.663841791152954
step: 450 loss_mean: 2.72704243183136
step: 500 loss_mean: 2.6839441537857054
step: 550 loss_mean: 2.6693399715423585
step: 600 loss_mean: 2.6918675470352174
step: 650 loss_mean: 2.6824914169311525
step: 700 loss_mean: 2.694370412826538
step: 750 loss_mean: 2.6533305931091307
step: 800 loss_mean: 2.6978567314147948
step: 850 loss_mean: 2.6878019618988036
step: 900 loss_mean: 2.7015163326263427
step: 950 loss_mean: 2.7064249753952025
step: 1000 loss_mean: 2.701622796058655
step: 1050 loss_mean: 2.7370220470428466
step: 1100 loss_mean: 2.7103589391708374
step: 1150 loss_mean: 2.711102886199951
step: 1200 loss_mean: 2.7228552150726317
step: 1250 loss_mean: 2.7161847162246704
step: 1300 loss_mean: 2.732493815422058
step: 1350 loss_mean: 2.6845747375488282
step: 1400 loss_mean: 2.6639310836791994
step: 1450 loss_mean: 2.6473605489730834
step: 1500 loss_mean: 2.7312516164779663
step: 1550 loss_mean: 2.7609820795059203
step: 1600 loss_mean: 2.753592081069946
step: 1650 loss_mean: 2.6715253496170046
step: 1700 loss_mean: 2.6648261547088623
step: 1750 loss_mean: 2.7053113174438477
step: 1800 loss_mean: 2.7210951232910157
step: 1850 loss_mean: 2.7032169246673585
step: 1900 loss_mean: 2.7120876121520996
step: 1950 loss_mean: 2.696819500923157
step: 2000 loss_mean: 2.7124259853363037
step: 2050 loss_mean: 2.6935242366790773
step: 2100 loss_mean: 2.6634381008148194
step: 2150 loss_mean: 2.6793770360946656
step: 2200 loss_mean: 2.6814598989486695
step: 2250 loss_mean: 2.704342632293701
step: 2300 loss_mean: 2.716005859375
step: 2350 loss_mean: 2.740785222053528
step: 2400 loss_mean: 2.7004748964309693
step: 2450 loss_mean: 2.7087658739089964
step: 2500 loss_mean: 2.64065345287323
step: 2550 loss_mean: 2.772871255874634
step: 2600 loss_mean: 2.7073870944976806
step: 2650 loss_mean: 2.643669991493225
step: 2700 loss_mean: 2.723653483390808
step: 2750 loss_mean: 2.6435330295562744
step: 2800 loss_mean: 2.7205905723571777
step: 2850 loss_mean: 2.7021411991119386
step: 2900 loss_mean: 2.6920453977584837
step: 2950 loss_mean: 2.6754861736297606
step: 3000 loss_mean: 2.733561186790466
step: 3050 loss_mean: 2.7177303218841553
step: 3100 loss_mean: 2.6779833698272704
step: 50 loss_mean: 2.7379491567611693
step: 100 loss_mean: 2.7790373086929323
step: 150 loss_mean: 2.715107173919678
step: 200 loss_mean: 2.800095715522766
step: 250 loss_mean: 2.773471121788025
step: 300 loss_mean: 2.8517221879959105
step: 350 loss_mean: 2.7028865575790406
step: 400 loss_mean: 2.7609284257888795
step: 450 loss_mean: 2.7189057302474975
step: 500 loss_mean: 2.7736096715927125
step: 550 loss_mean: 2.726513957977295
step: 600 loss_mean: 2.678046369552612
Epoch: 101 | Run time: 613.0 s | Train loss: 2.7 | Valid loss: 2.75
Saved checkpoint 101 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1461 s.
run_epoch
step: 50 loss_mean: 2.7117109632492067
step: 100 loss_mean: 2.6840575313568116
step: 150 loss_mean: 2.719295172691345
step: 200 loss_mean: 2.749939203262329
step: 250 loss_mean: 2.730222635269165
step: 300 loss_mean: 2.729141206741333
step: 350 loss_mean: 2.6949108028411866
step: 400 loss_mean: 2.65588436126709
step: 450 loss_mean: 2.6876566553115846
step: 500 loss_mean: 2.6847975206375123
step: 550 loss_mean: 2.7463470888137818
step: 600 loss_mean: 2.7236040353775026
step: 650 loss_mean: 2.665329895019531
step: 700 loss_mean: 2.6731304121017456
step: 750 loss_mean: 2.6549695491790772
step: 800 loss_mean: 2.6523602485656737
step: 850 loss_mean: 2.703277201652527
step: 900 loss_mean: 2.6889398002624514
step: 950 loss_mean: 2.6876758098602296
step: 1000 loss_mean: 2.665208878517151
step: 1050 loss_mean: 2.685942506790161
step: 1100 loss_mean: 2.739829144477844
step: 1150 loss_mean: 2.7163983678817747
step: 1200 loss_mean: 2.670870532989502
step: 1250 loss_mean: 2.7435476350784302
step: 1300 loss_mean: 2.721569986343384
step: 1350 loss_mean: 2.646671633720398
step: 1400 loss_mean: 2.6374117660522463
step: 1450 loss_mean: 2.7237518358230592
step: 1500 loss_mean: 2.723730845451355
step: 1550 loss_mean: 2.7363427829742433
step: 1600 loss_mean: 2.66540922164917
step: 1650 loss_mean: 2.699045190811157
step: 1700 loss_mean: 2.70071195602417
step: 1750 loss_mean: 2.704488043785095
step: 1800 loss_mean: 2.7288432550430297
step: 1850 loss_mean: 2.7430682611465453
step: 1900 loss_mean: 2.717000432014465
step: 1950 loss_mean: 2.6956341600418092
step: 2000 loss_mean: 2.717803268432617
step: 2050 loss_mean: 2.7412394666671753
step: 2100 loss_mean: 2.618599202632904
step: 2150 loss_mean: 2.6890554428100586
step: 2200 loss_mean: 2.6501206588745116
step: 2250 loss_mean: 2.656089367866516
step: 2300 loss_mean: 2.6931608390808104
step: 2350 loss_mean: 2.7648082447052
step: 2400 loss_mean: 2.689508762359619
step: 2450 loss_mean: 2.6942559909820556
step: 2500 loss_mean: 2.678015661239624
step: 2550 loss_mean: 2.72584716796875
step: 2600 loss_mean: 2.631183261871338
step: 2650 loss_mean: 2.773967213630676
step: 2700 loss_mean: 2.6370480680465698
step: 2750 loss_mean: 2.6483398723602294
step: 2800 loss_mean: 2.7128133821487426
step: 2850 loss_mean: 2.7089888048171997
step: 2900 loss_mean: 2.6853848266601563
step: 2950 loss_mean: 2.776160454750061
step: 3000 loss_mean: 2.709521131515503
step: 3050 loss_mean: 2.6968670511245727
step: 3100 loss_mean: 2.6813991594314577
step: 50 loss_mean: 2.7370578145980833
step: 100 loss_mean: 2.780512185096741
step: 150 loss_mean: 2.6983735084533693
step: 200 loss_mean: 2.7988217210769655
step: 250 loss_mean: 2.7522354698181153
step: 300 loss_mean: 2.843484091758728
step: 350 loss_mean: 2.7132006454467774
step: 400 loss_mean: 2.721311101913452
step: 450 loss_mean: 2.699189853668213
step: 500 loss_mean: 2.7738646125793456
step: 550 loss_mean: 2.713383321762085
step: 600 loss_mean: 2.6720684337615968
Epoch: 102 | Run time: 621.0 s | Train loss: 2.7 | Valid loss: 2.74
Saved checkpoint 102 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1471 s.
run_epoch
step: 50 loss_mean: 2.6567423725128174
step: 100 loss_mean: 2.702125840187073
step: 150 loss_mean: 2.7256934118270872
step: 200 loss_mean: 2.6672959971427916
step: 250 loss_mean: 2.742409448623657
step: 300 loss_mean: 2.659255690574646
step: 350 loss_mean: 2.7350609874725342
step: 400 loss_mean: 2.6465517902374267
step: 450 loss_mean: 2.6822714233398437
step: 500 loss_mean: 2.6626561737060546
step: 550 loss_mean: 2.7465519809722903
step: 600 loss_mean: 2.730913586616516
step: 650 loss_mean: 2.64514705657959
step: 700 loss_mean: 2.7065512418746946
step: 750 loss_mean: 2.6640683603286743
step: 800 loss_mean: 2.688198347091675
step: 850 loss_mean: 2.684071493148804
step: 900 loss_mean: 2.635440354347229
step: 950 loss_mean: 2.663623433113098
step: 1000 loss_mean: 2.701925859451294
step: 1050 loss_mean: 2.6846161103248596
step: 1100 loss_mean: 2.7257969617843627
step: 1150 loss_mean: 2.6791860032081605
step: 1200 loss_mean: 2.7274054050445558
step: 1250 loss_mean: 2.668079347610474
step: 1300 loss_mean: 2.6968494033813477
step: 1350 loss_mean: 2.72521993637085
step: 1400 loss_mean: 2.6705805969238283
step: 1450 loss_mean: 2.690968589782715
step: 1500 loss_mean: 2.7212225246429442
step: 1550 loss_mean: 2.730166974067688
step: 1600 loss_mean: 2.7183851432800292
step: 1650 loss_mean: 2.7033591985702516
step: 1700 loss_mean: 2.669275164604187
step: 1750 loss_mean: 2.738832197189331
step: 1800 loss_mean: 2.6818095445632935
step: 1850 loss_mean: 2.6499734497070313
step: 1900 loss_mean: 2.692182297706604
step: 1950 loss_mean: 2.7478919315338133
step: 2000 loss_mean: 2.6878589010238647
step: 2050 loss_mean: 2.6751252460479735
step: 2100 loss_mean: 2.75830762386322
step: 2150 loss_mean: 2.714785032272339
step: 2200 loss_mean: 2.7181393003463743
step: 2250 loss_mean: 2.6732574272155762
step: 2300 loss_mean: 2.750307569503784
step: 2350 loss_mean: 2.65814079284668
step: 2400 loss_mean: 2.6546494483947756
step: 2450 loss_mean: 2.6859975147247312
step: 2500 loss_mean: 2.7136951684951782
step: 2550 loss_mean: 2.777157716751099
step: 2600 loss_mean: 2.700764846801758
step: 2650 loss_mean: 2.6988786268234253
step: 2700 loss_mean: 2.7006886529922487
step: 2750 loss_mean: 2.6761531019210816
step: 2800 loss_mean: 2.65568968296051
step: 2850 loss_mean: 2.686772313117981
step: 2900 loss_mean: 2.697732071876526
step: 2950 loss_mean: 2.6697136878967287
step: 3000 loss_mean: 2.660301661491394
step: 3050 loss_mean: 2.745475535392761
step: 3100 loss_mean: 2.6876337194442748
step: 50 loss_mean: 2.7252430057525636
step: 100 loss_mean: 2.7707616567611693
step: 150 loss_mean: 2.705469431877136
step: 200 loss_mean: 2.8079851627349854
step: 250 loss_mean: 2.766086149215698
step: 300 loss_mean: 2.8372897911071777
step: 350 loss_mean: 2.7080694246292114
step: 400 loss_mean: 2.7466881895065307
step: 450 loss_mean: 2.709318175315857
step: 500 loss_mean: 2.7669160985946655
step: 550 loss_mean: 2.729688563346863
step: 600 loss_mean: 2.676774411201477
Epoch: 103 | Run time: 624.0 s | Train loss: 2.7 | Valid loss: 2.75
Saved checkpoint 103 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1484 s.
run_epoch
step: 50 loss_mean: 2.6671324968338013
step: 100 loss_mean: 2.6444285535812377
step: 150 loss_mean: 2.700229172706604
step: 200 loss_mean: 2.669757571220398
step: 250 loss_mean: 2.730593638420105
step: 300 loss_mean: 2.6696852922439573
step: 350 loss_mean: 2.7083838033676146
step: 400 loss_mean: 2.7168384552001954
step: 450 loss_mean: 2.657576332092285
step: 500 loss_mean: 2.6968012714385985
step: 550 loss_mean: 2.7239217710494996
step: 600 loss_mean: 2.722680492401123
step: 650 loss_mean: 2.63284499168396
step: 700 loss_mean: 2.631643786430359
step: 750 loss_mean: 2.6989397859573363
step: 800 loss_mean: 2.6439705419540407
step: 850 loss_mean: 2.7649797010421753
step: 900 loss_mean: 2.707450633049011
step: 950 loss_mean: 2.6162537145614624
step: 1000 loss_mean: 2.693157515525818
step: 1050 loss_mean: 2.732007246017456
step: 1100 loss_mean: 2.6703018140792847
step: 1150 loss_mean: 2.7535159301757814
step: 1200 loss_mean: 2.718367924690247
step: 1250 loss_mean: 2.646775507926941
step: 1300 loss_mean: 2.706176619529724
step: 1350 loss_mean: 2.735153956413269
step: 1400 loss_mean: 2.69339111328125
step: 1450 loss_mean: 2.701352834701538
step: 1500 loss_mean: 2.6983841037750245
step: 1550 loss_mean: 2.731037402153015
step: 1600 loss_mean: 2.696607971191406
step: 1650 loss_mean: 2.705010828971863
step: 1700 loss_mean: 2.731073651313782
step: 1750 loss_mean: 2.720288634300232
step: 1800 loss_mean: 2.6594155550003054
step: 1850 loss_mean: 2.683514232635498
step: 1900 loss_mean: 2.737717561721802
step: 1950 loss_mean: 2.7219594812393186
step: 2000 loss_mean: 2.742555980682373
step: 2050 loss_mean: 2.6822703647613526
step: 2100 loss_mean: 2.6755231189727784
step: 2150 loss_mean: 2.6961221265792847
step: 2200 loss_mean: 2.70441077709198
step: 2250 loss_mean: 2.687481641769409
step: 2300 loss_mean: 2.7384684896469116
step: 2350 loss_mean: 2.7371605253219604
step: 2400 loss_mean: 2.721361174583435
step: 2450 loss_mean: 2.6960983085632324
step: 2500 loss_mean: 2.6704001426696777
step: 2550 loss_mean: 2.697506055831909
step: 2600 loss_mean: 2.6642942094802855
step: 2650 loss_mean: 2.6823033571243284
step: 2700 loss_mean: 2.711748523712158
step: 2750 loss_mean: 2.6688985204696656
step: 2800 loss_mean: 2.683052644729614
step: 2850 loss_mean: 2.706275157928467
step: 2900 loss_mean: 2.6655342817306518
step: 2950 loss_mean: 2.6920187139511107
step: 3000 loss_mean: 2.703088059425354
step: 3050 loss_mean: 2.6902834081649782
step: 3100 loss_mean: 2.6833829402923586
step: 50 loss_mean: 2.7376313257217406
step: 100 loss_mean: 2.7897022914886476
step: 150 loss_mean: 2.707606806755066
step: 200 loss_mean: 2.79960205078125
step: 250 loss_mean: 2.7642393207550047
step: 300 loss_mean: 2.8532826471328736
step: 350 loss_mean: 2.7251696014404296
step: 400 loss_mean: 2.7452411317825316
step: 450 loss_mean: 2.721407380104065
step: 500 loss_mean: 2.7766801595687864
step: 550 loss_mean: 2.730228853225708
step: 600 loss_mean: 2.677902326583862
Epoch: 104 | Run time: 622.0 s | Train loss: 2.7 | Valid loss: 2.75
Saved checkpoint 104 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1501 s.
run_epoch
step: 50 loss_mean: 2.6639079093933105
step: 100 loss_mean: 2.677618832588196
step: 150 loss_mean: 2.713982968330383
step: 200 loss_mean: 2.6455101346969605
step: 250 loss_mean: 2.662168850898743
step: 300 loss_mean: 2.697100086212158
step: 350 loss_mean: 2.6855648612976073
step: 400 loss_mean: 2.6758335065841674
step: 450 loss_mean: 2.714139947891235
step: 500 loss_mean: 2.675371217727661
step: 550 loss_mean: 2.7186794662475586
step: 600 loss_mean: 2.703227648735046
step: 650 loss_mean: 2.6373953580856324
step: 700 loss_mean: 2.6914158916473387
step: 750 loss_mean: 2.684587936401367
step: 800 loss_mean: 2.6805504369735718
step: 850 loss_mean: 2.657626791000366
step: 900 loss_mean: 2.7120683908462526
step: 950 loss_mean: 2.7297180891036987
step: 1000 loss_mean: 2.716788263320923
step: 1050 loss_mean: 2.660083155632019
step: 1100 loss_mean: 2.681630973815918
step: 1150 loss_mean: 2.7099476051330567
step: 1200 loss_mean: 2.731312494277954
step: 1250 loss_mean: 2.720875506401062
step: 1300 loss_mean: 2.6828508710861207
step: 1350 loss_mean: 2.712485737800598
step: 1400 loss_mean: 2.733543076515198
step: 1450 loss_mean: 2.6896860647201537
step: 1500 loss_mean: 2.6764246082305907
step: 1550 loss_mean: 2.7289148950576783
step: 1600 loss_mean: 2.749903564453125
step: 1650 loss_mean: 2.7223917865753173
step: 1700 loss_mean: 2.6916263580322264
step: 1750 loss_mean: 2.6740676259994505
step: 1800 loss_mean: 2.7118485879898073
step: 1850 loss_mean: 2.7004558086395263
step: 1900 loss_mean: 2.6985054492950438
step: 1950 loss_mean: 2.68998592376709
step: 2000 loss_mean: 2.721465048789978
step: 2050 loss_mean: 2.651517558097839
step: 2100 loss_mean: 2.6913991594314575
step: 2150 loss_mean: 2.708179135322571
step: 2200 loss_mean: 2.698338761329651
step: 2250 loss_mean: 2.6877863931655885
step: 2300 loss_mean: 2.6792675495147704
step: 2350 loss_mean: 2.6620274639129637
step: 2400 loss_mean: 2.706042323112488
step: 2450 loss_mean: 2.678540644645691
step: 2500 loss_mean: 2.7329501628875734
step: 2550 loss_mean: 2.698174543380737
step: 2600 loss_mean: 2.6974273920059204
step: 2650 loss_mean: 2.7524690008163453
step: 2700 loss_mean: 2.7482367086410524
step: 2750 loss_mean: 2.7168145275115965
step: 2800 loss_mean: 2.7166848373413086
step: 2850 loss_mean: 2.6902559185028077
step: 2900 loss_mean: 2.640860710144043
step: 2950 loss_mean: 2.6913922452926635
step: 3000 loss_mean: 2.6671820783615114
step: 3050 loss_mean: 2.7653469896316527
step: 3100 loss_mean: 2.706865873336792
step: 50 loss_mean: 2.734993996620178
step: 100 loss_mean: 2.783292980194092
step: 150 loss_mean: 2.7131961679458616
step: 200 loss_mean: 2.809091873168945
step: 250 loss_mean: 2.7533811283111573
step: 300 loss_mean: 2.851676731109619
step: 350 loss_mean: 2.710998151302338
step: 400 loss_mean: 2.7212787532806395
step: 450 loss_mean: 2.718011527061462
step: 500 loss_mean: 2.766657156944275
step: 550 loss_mean: 2.7233961963653566
step: 600 loss_mean: 2.675081205368042
Epoch: 105 | Run time: 624.0 s | Train loss: 2.7 | Valid loss: 2.75
Saved checkpoint 105 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1522 s.
run_epoch
step: 50 loss_mean: 2.6867178678512573
step: 100 loss_mean: 2.694443883895874
step: 150 loss_mean: 2.6658148336410523
step: 200 loss_mean: 2.7110647201538085
step: 250 loss_mean: 2.710830717086792
step: 300 loss_mean: 2.685279550552368
step: 350 loss_mean: 2.662663049697876
step: 400 loss_mean: 2.741018600463867
step: 450 loss_mean: 2.6758571100234985
step: 500 loss_mean: 2.7038515186309815
step: 550 loss_mean: 2.6863817501068117
step: 600 loss_mean: 2.696390085220337
step: 650 loss_mean: 2.6655406045913694
step: 700 loss_mean: 2.692690577507019
step: 750 loss_mean: 2.6995946884155275
step: 800 loss_mean: 2.6434043979644777
step: 850 loss_mean: 2.653569474220276
step: 900 loss_mean: 2.699780750274658
step: 950 loss_mean: 2.7178678846359254
step: 1000 loss_mean: 2.678153533935547
step: 1050 loss_mean: 2.727976875305176
step: 1100 loss_mean: 2.693942332267761
step: 1150 loss_mean: 2.681263651847839
step: 1200 loss_mean: 2.7103732299804686
step: 1250 loss_mean: 2.694805874824524
step: 1300 loss_mean: 2.745562629699707
step: 1350 loss_mean: 2.7352162647247313
step: 1400 loss_mean: 2.695157585144043
step: 1450 loss_mean: 2.6922271156311037
step: 1500 loss_mean: 2.6266016864776613
step: 1550 loss_mean: 2.6452937173843383
step: 1600 loss_mean: 2.6685626792907713
step: 1650 loss_mean: 2.7033817386627197
step: 1700 loss_mean: 2.705755171775818
step: 1750 loss_mean: 2.6689635372161864
step: 1800 loss_mean: 2.720066146850586
step: 1850 loss_mean: 2.7462021350860595
step: 1900 loss_mean: 2.696743383407593
step: 1950 loss_mean: 2.6551185989379884
step: 2000 loss_mean: 2.671388177871704
step: 2050 loss_mean: 2.7205265283584597
step: 2100 loss_mean: 2.7437925148010254
step: 2150 loss_mean: 2.6938876628875734
step: 2200 loss_mean: 2.6934496545791626
step: 2250 loss_mean: 2.671832571029663
step: 2300 loss_mean: 2.7021837425231934
step: 2350 loss_mean: 2.725038151741028
step: 2400 loss_mean: 2.6985553550720214
step: 2450 loss_mean: 2.718683738708496
step: 2500 loss_mean: 2.681620955467224
step: 2550 loss_mean: 2.6434573459625246
step: 2600 loss_mean: 2.7550460290908814
step: 2650 loss_mean: 2.6873582553863526
step: 2700 loss_mean: 2.6163421773910525
step: 2750 loss_mean: 2.727580609321594
step: 2800 loss_mean: 2.7233276081085207
step: 2850 loss_mean: 2.7104993724822997
step: 2900 loss_mean: 2.6535295820236207
step: 2950 loss_mean: 2.712997169494629
step: 3000 loss_mean: 2.731954550743103
step: 3050 loss_mean: 2.7302979040145874
step: 3100 loss_mean: 2.6381968212127687
step: 50 loss_mean: 2.733647723197937
step: 100 loss_mean: 2.7900037431716918
step: 150 loss_mean: 2.6916245937347414
step: 200 loss_mean: 2.796272020339966
step: 250 loss_mean: 2.766006655693054
step: 300 loss_mean: 2.8638564443588255
step: 350 loss_mean: 2.7180555200576784
step: 400 loss_mean: 2.7307606887817384
step: 450 loss_mean: 2.709857029914856
step: 500 loss_mean: 2.773264055252075
step: 550 loss_mean: 2.719985218048096
step: 600 loss_mean: 2.6745208501815796
Epoch: 106 | Run time: 623.0 s | Train loss: 2.69 | Valid loss: 2.75
Saved checkpoint 106 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1525 s.
run_epoch
step: 50 loss_mean: 2.70579975605011
step: 100 loss_mean: 2.651995530128479
step: 150 loss_mean: 2.6710674381256103
step: 200 loss_mean: 2.674905195236206
step: 250 loss_mean: 2.7484800481796263
step: 300 loss_mean: 2.669140830039978
step: 350 loss_mean: 2.6490478038787844
step: 400 loss_mean: 2.6722837018966676
step: 450 loss_mean: 2.716674165725708
step: 500 loss_mean: 2.749945921897888
step: 550 loss_mean: 2.726751055717468
step: 600 loss_mean: 2.6871627950668335
step: 650 loss_mean: 2.6907079458236693
step: 700 loss_mean: 2.6719247484207154
step: 750 loss_mean: 2.695500316619873
step: 800 loss_mean: 2.689793138504028
step: 850 loss_mean: 2.651011629104614
step: 900 loss_mean: 2.671479959487915
step: 950 loss_mean: 2.745913891792297
step: 1000 loss_mean: 2.7685295391082763
step: 1050 loss_mean: 2.6694534969329835
step: 1100 loss_mean: 2.7467076778411865
step: 1150 loss_mean: 2.6674604272842406
step: 1200 loss_mean: 2.7044773292541504
step: 1250 loss_mean: 2.641109809875488
step: 1300 loss_mean: 2.715103516578674
step: 1350 loss_mean: 2.7089566564559937
step: 1400 loss_mean: 2.696478610038757
step: 1450 loss_mean: 2.668469729423523
step: 1500 loss_mean: 2.637689161300659
step: 1550 loss_mean: 2.665675368309021
step: 1600 loss_mean: 2.6900041055679322
step: 1650 loss_mean: 2.669630513191223
step: 1700 loss_mean: 2.697768921852112
step: 1750 loss_mean: 2.7134934186935427
step: 1800 loss_mean: 2.688003125190735
step: 1850 loss_mean: 2.6929919004440306
step: 1900 loss_mean: 2.669956431388855
step: 1950 loss_mean: 2.701210031509399
step: 2000 loss_mean: 2.6099963760375977
step: 2050 loss_mean: 2.730396199226379
step: 2100 loss_mean: 2.734163384437561
step: 2150 loss_mean: 2.643041045665741
step: 2200 loss_mean: 2.685412802696228
step: 2250 loss_mean: 2.7175693988800047
step: 2300 loss_mean: 2.6887612628936766
step: 2350 loss_mean: 2.7426125240325927
step: 2400 loss_mean: 2.7131246185302733
step: 2450 loss_mean: 2.687912950515747
step: 2500 loss_mean: 2.7026112842559815
step: 2550 loss_mean: 2.674600372314453
step: 2600 loss_mean: 2.6818495988845825
step: 2650 loss_mean: 2.6096044778823853
step: 2700 loss_mean: 2.6618824100494383
step: 2750 loss_mean: 2.7612560892105105
step: 2800 loss_mean: 2.685715684890747
step: 2850 loss_mean: 2.6802816867828367
step: 2900 loss_mean: 2.741516046524048
step: 2950 loss_mean: 2.728115577697754
step: 3000 loss_mean: 2.6793761396408082
step: 3050 loss_mean: 2.7153069448471068
step: 3100 loss_mean: 2.6771598720550536
step: 50 loss_mean: 2.7397379779815676
step: 100 loss_mean: 2.78266788482666
step: 150 loss_mean: 2.702618842124939
step: 200 loss_mean: 2.7862209367752073
step: 250 loss_mean: 2.7663784646987915
step: 300 loss_mean: 2.8441693782806396
step: 350 loss_mean: 2.727392201423645
step: 400 loss_mean: 2.7391084671020507
step: 450 loss_mean: 2.7126996231079104
step: 500 loss_mean: 2.785693335533142
step: 550 loss_mean: 2.7234285831451417
step: 600 loss_mean: 2.6837850332260134
Epoch: 107 | Run time: 624.0 s | Train loss: 2.69 | Valid loss: 2.75
Saved checkpoint 107 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1545 s.
run_epoch
step: 50 loss_mean: 2.7182480192184446
step: 100 loss_mean: 2.6672321462631228
step: 150 loss_mean: 2.7363889455795287
step: 200 loss_mean: 2.678971920013428
step: 250 loss_mean: 2.6462921333312988
step: 300 loss_mean: 2.689556350708008
step: 350 loss_mean: 2.6419263076782227
step: 400 loss_mean: 2.674151782989502
step: 450 loss_mean: 2.6551315498352053
step: 500 loss_mean: 2.665618886947632
step: 550 loss_mean: 2.7477006196975706
step: 600 loss_mean: 2.719177556037903
step: 650 loss_mean: 2.6610481119155884
step: 700 loss_mean: 2.677860794067383
step: 750 loss_mean: 2.672164158821106
step: 800 loss_mean: 2.7251806545257566
step: 850 loss_mean: 2.6680562782287596
step: 900 loss_mean: 2.6787304496765136
step: 950 loss_mean: 2.7163711071014403
step: 1000 loss_mean: 2.6995523262023924
step: 1050 loss_mean: 2.716082992553711
step: 1100 loss_mean: 2.6471795558929445
step: 1150 loss_mean: 2.6560710644721985
step: 1200 loss_mean: 2.723910398483276
step: 1250 loss_mean: 2.735769772529602
step: 1300 loss_mean: 2.7487057304382323
step: 1350 loss_mean: 2.711936526298523
step: 1400 loss_mean: 2.6722876119613646
step: 1450 loss_mean: 2.666744198799133
step: 1500 loss_mean: 2.7315961503982544
step: 1550 loss_mean: 2.6972224521636963
step: 1600 loss_mean: 2.74227472782135
step: 1650 loss_mean: 2.680803823471069
step: 1700 loss_mean: 2.6892452001571656
step: 1750 loss_mean: 2.725204472541809
step: 1800 loss_mean: 2.615804991722107
step: 1850 loss_mean: 2.7343354511260984
step: 1900 loss_mean: 2.713848786354065
step: 1950 loss_mean: 2.699438605308533
step: 2000 loss_mean: 2.671910629272461
step: 2050 loss_mean: 2.7203880739212036
step: 2100 loss_mean: 2.6880101442337034
step: 2150 loss_mean: 2.6915005373954775
step: 2200 loss_mean: 2.7422154140472412
step: 2250 loss_mean: 2.726627063751221
step: 2300 loss_mean: 2.701736078262329
step: 2350 loss_mean: 2.700413908958435
step: 2400 loss_mean: 2.6594670677185057
step: 2450 loss_mean: 2.679417595863342
step: 2500 loss_mean: 2.67306911945343
step: 2550 loss_mean: 2.703994007110596
step: 2600 loss_mean: 2.6846515321731568
step: 2650 loss_mean: 2.6416891384124757
step: 2700 loss_mean: 2.720102906227112
step: 2750 loss_mean: 2.710347280502319
step: 2800 loss_mean: 2.652597541809082
step: 2850 loss_mean: 2.691506199836731
step: 2900 loss_mean: 2.7057949352264403
step: 2950 loss_mean: 2.6999285078048705
step: 3000 loss_mean: 2.6885397243499756
step: 3050 loss_mean: 2.639161829948425
step: 3100 loss_mean: 2.6644420051574706
step: 50 loss_mean: 2.7331240367889404
step: 100 loss_mean: 2.777356595993042
step: 150 loss_mean: 2.7025848817825318
step: 200 loss_mean: 2.791875
step: 250 loss_mean: 2.747499623298645
step: 300 loss_mean: 2.852445650100708
step: 350 loss_mean: 2.7232148265838623
step: 400 loss_mean: 2.729754648208618
step: 450 loss_mean: 2.7032829236984255
step: 500 loss_mean: 2.7816797924041747
step: 550 loss_mean: 2.721161413192749
step: 600 loss_mean: 2.6667841625213624
Epoch: 108 | Run time: 624.0 s | Train loss: 2.69 | Valid loss: 2.75
Saved checkpoint 108 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1565 s.
run_epoch
step: 50 loss_mean: 2.728951826095581
step: 100 loss_mean: 2.659094867706299
step: 150 loss_mean: 2.6873122787475587
step: 200 loss_mean: 2.6603878688812257
step: 250 loss_mean: 2.6953570318222044
step: 300 loss_mean: 2.6947349882125855
step: 350 loss_mean: 2.6724599504470827
step: 400 loss_mean: 2.6642774391174315
step: 450 loss_mean: 2.6778062200546264
step: 500 loss_mean: 2.7281103420257566
step: 550 loss_mean: 2.696327395439148
step: 600 loss_mean: 2.7203741598129274
step: 650 loss_mean: 2.665774607658386
step: 700 loss_mean: 2.6895909214019778
step: 750 loss_mean: 2.629722752571106
step: 800 loss_mean: 2.665570607185364
step: 850 loss_mean: 2.6960412216186525
step: 900 loss_mean: 2.7185265827178955
step: 950 loss_mean: 2.6838126850128172
step: 1000 loss_mean: 2.706342740058899
step: 1050 loss_mean: 2.6783925819396974
step: 1100 loss_mean: 2.706959648132324
step: 1150 loss_mean: 2.681416392326355
step: 1200 loss_mean: 2.686862745285034
step: 1250 loss_mean: 2.6846093273162843
step: 1300 loss_mean: 2.6819458770751954
step: 1350 loss_mean: 2.6964237213134767
step: 1400 loss_mean: 2.6707165479660033
step: 1450 loss_mean: 2.7182355451583864
step: 1500 loss_mean: 2.6901126432418825
step: 1550 loss_mean: 2.681344094276428
step: 1600 loss_mean: 2.6432389879226683
step: 1650 loss_mean: 2.682013473510742
step: 1700 loss_mean: 2.7035348558425905
step: 1750 loss_mean: 2.7278242015838625
step: 1800 loss_mean: 2.7530142641067505
step: 1850 loss_mean: 2.699828391075134
step: 1900 loss_mean: 2.6954915714263916
step: 1950 loss_mean: 2.685667314529419
step: 2000 loss_mean: 2.6945926094055177
step: 2050 loss_mean: 2.7227221155166625
step: 2100 loss_mean: 2.6929646730422974
step: 2150 loss_mean: 2.635671548843384
step: 2200 loss_mean: 2.6872038984298707
step: 2250 loss_mean: 2.713093709945679
step: 2300 loss_mean: 2.6381442737579346
step: 2350 loss_mean: 2.681076102256775
step: 2400 loss_mean: 2.6573189306259155
step: 2450 loss_mean: 2.6687497758865355
step: 2500 loss_mean: 2.722213850021362
step: 2550 loss_mean: 2.675810737609863
step: 2600 loss_mean: 2.743349256515503
step: 2650 loss_mean: 2.7260000991821287
step: 2700 loss_mean: 2.667533473968506
step: 2750 loss_mean: 2.70721519947052
step: 2800 loss_mean: 2.675084047317505
step: 2850 loss_mean: 2.692675790786743
step: 2900 loss_mean: 2.72339421749115
step: 2950 loss_mean: 2.7148626756668093
step: 3000 loss_mean: 2.6394508695602417
step: 3050 loss_mean: 2.7067070817947387
step: 3100 loss_mean: 2.693175072669983
step: 50 loss_mean: 2.732175121307373
step: 100 loss_mean: 2.7713500452041626
step: 150 loss_mean: 2.701568832397461
step: 200 loss_mean: 2.7822297954559327
step: 250 loss_mean: 2.7582014083862303
step: 300 loss_mean: 2.8376654100418093
step: 350 loss_mean: 2.733493185043335
step: 400 loss_mean: 2.6957051849365232
step: 450 loss_mean: 2.6984517002105712
step: 500 loss_mean: 2.7687148523330687
step: 550 loss_mean: 2.7161564731597903
step: 600 loss_mean: 2.6462639713287355
Epoch: 109 | Run time: 625.0 s | Train loss: 2.69 | Valid loss: 2.74
Saved checkpoint 109 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1577 s.
run_epoch
step: 50 loss_mean: 2.662309160232544
step: 100 loss_mean: 2.6476721477508547
step: 150 loss_mean: 2.6650524234771726
step: 200 loss_mean: 2.723098316192627
step: 250 loss_mean: 2.688356652259827
step: 300 loss_mean: 2.687503933906555
step: 350 loss_mean: 2.6467522239685057
step: 400 loss_mean: 2.696219048500061
step: 450 loss_mean: 2.6766740226745607
step: 500 loss_mean: 2.725280647277832
step: 550 loss_mean: 2.629349913597107
step: 600 loss_mean: 2.722886061668396
step: 650 loss_mean: 2.7895392990112304
step: 700 loss_mean: 2.6914316606521607
step: 750 loss_mean: 2.7266012907028196
step: 800 loss_mean: 2.6507237100601198
step: 850 loss_mean: 2.6644149589538575
step: 900 loss_mean: 2.701191864013672
step: 950 loss_mean: 2.7083731746673583
step: 1000 loss_mean: 2.664154839515686
step: 1050 loss_mean: 2.6182745409011843
step: 1100 loss_mean: 2.7311123323440554
step: 1150 loss_mean: 2.665672197341919
step: 1200 loss_mean: 2.63515097618103
step: 1250 loss_mean: 2.7267557334899903
step: 1300 loss_mean: 2.7102363681793213
step: 1350 loss_mean: 2.6232391595840454
step: 1400 loss_mean: 2.697040491104126
step: 1450 loss_mean: 2.7278026437759397
step: 1500 loss_mean: 2.7008240461349486
step: 1550 loss_mean: 2.681663975715637
step: 1600 loss_mean: 2.667765250205994
step: 1650 loss_mean: 2.7009620094299316
step: 1700 loss_mean: 2.7097161340713503
step: 1750 loss_mean: 2.68971565246582
step: 1800 loss_mean: 2.7390497303009034
step: 1850 loss_mean: 2.7014418697357176
step: 1900 loss_mean: 2.783771448135376
step: 1950 loss_mean: 2.701574835777283
step: 2000 loss_mean: 2.735966067314148
step: 2050 loss_mean: 2.727342972755432
step: 2100 loss_mean: 2.688485097885132
step: 2150 loss_mean: 2.6470026636123656
step: 2200 loss_mean: 2.706706027984619
step: 2250 loss_mean: 2.6981173515319825
step: 2300 loss_mean: 2.6935288381576536
step: 2350 loss_mean: 2.7135638570785523
step: 2400 loss_mean: 2.6971043062210085
step: 2450 loss_mean: 2.6491470623016355
step: 2500 loss_mean: 2.683159227371216
step: 2550 loss_mean: 2.6725401401519777
step: 2600 loss_mean: 2.7376781034469606
step: 2650 loss_mean: 2.6691620683670045
step: 2700 loss_mean: 2.683259382247925
step: 2750 loss_mean: 2.7142213869094847
step: 2800 loss_mean: 2.6775308895111083
step: 2850 loss_mean: 2.739030795097351
step: 2900 loss_mean: 2.678900899887085
step: 2950 loss_mean: 2.6211130523681643
step: 3000 loss_mean: 2.6838926029205323
step: 3050 loss_mean: 2.661709861755371
step: 3100 loss_mean: 2.672643027305603
step: 50 loss_mean: 2.749885892868042
step: 100 loss_mean: 2.800595645904541
step: 150 loss_mean: 2.724638524055481
step: 200 loss_mean: 2.8228898859024047
step: 250 loss_mean: 2.796626844406128
step: 300 loss_mean: 2.8585583209991454
step: 350 loss_mean: 2.763366470336914
step: 400 loss_mean: 2.757299165725708
step: 450 loss_mean: 2.7270076417922975
step: 500 loss_mean: 2.800146255493164
step: 550 loss_mean: 2.74060112953186
step: 600 loss_mean: 2.6940152263641357
Epoch: 110 | Run time: 623.0 s | Train loss: 2.69 | Valid loss: 2.77
Saved checkpoint 110 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1614 s.
run_epoch
step: 50 loss_mean: 2.7463887977600097
step: 100 loss_mean: 2.689215888977051
step: 150 loss_mean: 2.717473015785217
step: 200 loss_mean: 2.6301360750198364
step: 250 loss_mean: 2.7149225902557372
step: 300 loss_mean: 2.6457772302627562
step: 350 loss_mean: 2.6804559421539307
step: 400 loss_mean: 2.7112332487106325
step: 450 loss_mean: 2.66757089138031
step: 500 loss_mean: 2.6803513765335083
step: 550 loss_mean: 2.69719304561615
step: 600 loss_mean: 2.7769056940078736
step: 650 loss_mean: 2.7199642133712767
step: 700 loss_mean: 2.6357157707214354
step: 750 loss_mean: 2.68770947933197
step: 800 loss_mean: 2.7002563667297363
step: 850 loss_mean: 2.6838702154159546
step: 900 loss_mean: 2.6888115072250365
step: 950 loss_mean: 2.7053129863739014
step: 1000 loss_mean: 2.690742506980896
step: 1050 loss_mean: 2.722999348640442
step: 1100 loss_mean: 2.7308725690841675
step: 1150 loss_mean: 2.705773777961731
step: 1200 loss_mean: 2.603493595123291
step: 1250 loss_mean: 2.6826007747650147
step: 1300 loss_mean: 2.75132905960083
step: 1350 loss_mean: 2.6988024520874023
step: 1400 loss_mean: 2.7093829917907715
step: 1450 loss_mean: 2.662554225921631
step: 1500 loss_mean: 2.695630578994751
step: 1550 loss_mean: 2.683727502822876
step: 1600 loss_mean: 2.706620807647705
step: 1650 loss_mean: 2.6967582178115843
step: 1700 loss_mean: 2.623755569458008
step: 1750 loss_mean: 2.71314950466156
step: 1800 loss_mean: 2.72194100856781
step: 1850 loss_mean: 2.6698148536682127
step: 1900 loss_mean: 2.5959098505973817
step: 1950 loss_mean: 2.7305783796310426
step: 2000 loss_mean: 2.6950914525985716
step: 2050 loss_mean: 2.707156205177307
step: 2100 loss_mean: 2.666968183517456
step: 2150 loss_mean: 2.724612760543823
step: 2200 loss_mean: 2.674401240348816
step: 2250 loss_mean: 2.7325474977493287
step: 2300 loss_mean: 2.69781400680542
step: 2350 loss_mean: 2.6808804368972776
step: 2400 loss_mean: 2.7120960474014284
step: 2450 loss_mean: 2.6744183921813964
step: 2500 loss_mean: 2.6989279317855837
step: 2550 loss_mean: 2.670144944190979
step: 2600 loss_mean: 2.649066758155823
step: 2650 loss_mean: 2.7043883657455443
step: 2700 loss_mean: 2.74893208026886
step: 2750 loss_mean: 2.6710970783233643
step: 2800 loss_mean: 2.6572934913635256
step: 2850 loss_mean: 2.658455295562744
step: 2900 loss_mean: 2.6754442644119263
step: 2950 loss_mean: 2.675166597366333
step: 3000 loss_mean: 2.6804196500778197
step: 3050 loss_mean: 2.66799852848053
step: 3100 loss_mean: 2.7179499435424805
step: 50 loss_mean: 2.733859043121338
step: 100 loss_mean: 2.788127121925354
step: 150 loss_mean: 2.708989853858948
step: 200 loss_mean: 2.795882053375244
step: 250 loss_mean: 2.7556134414672853
step: 300 loss_mean: 2.8602891254425047
step: 350 loss_mean: 2.724639449119568
step: 400 loss_mean: 2.7214126873016355
step: 450 loss_mean: 2.7231968784332277
step: 500 loss_mean: 2.8029875421524046
step: 550 loss_mean: 2.7358352851867678
step: 600 loss_mean: 2.675074715614319
Epoch: 111 | Run time: 624.0 s | Train loss: 2.69 | Valid loss: 2.75
Saved checkpoint 111 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1597 s.
run_epoch
step: 50 loss_mean: 2.686516456604004
step: 100 loss_mean: 2.716679630279541
step: 150 loss_mean: 2.7130207777023316
step: 200 loss_mean: 2.695801739692688
step: 250 loss_mean: 2.665437216758728
step: 300 loss_mean: 2.6864437818527223
step: 350 loss_mean: 2.7072983312606813
step: 400 loss_mean: 2.7198361921310426
step: 450 loss_mean: 2.7635101175308225
step: 500 loss_mean: 2.732861571311951
step: 550 loss_mean: 2.6560574340820313
step: 600 loss_mean: 2.6604718732833863
step: 650 loss_mean: 2.673421998023987
step: 700 loss_mean: 2.7193429040908814
step: 750 loss_mean: 2.67441556930542
step: 800 loss_mean: 2.671378574371338
step: 850 loss_mean: 2.7073158407211304
step: 900 loss_mean: 2.7035110855102538
step: 950 loss_mean: 2.6890552043914795
step: 1000 loss_mean: 2.685934314727783
step: 1050 loss_mean: 2.6338527822494506
step: 1100 loss_mean: 2.717549195289612
step: 1150 loss_mean: 2.772380075454712
step: 1200 loss_mean: 2.7205195951461794
step: 1250 loss_mean: 2.690042142868042
step: 1300 loss_mean: 2.6641218852996826
step: 1350 loss_mean: 2.668748683929443
step: 1400 loss_mean: 2.695932264328003
step: 1450 loss_mean: 2.6787926721572877
step: 1500 loss_mean: 2.7357764768600465
step: 1550 loss_mean: 2.6312245512008667
step: 1600 loss_mean: 2.71240647315979
step: 1650 loss_mean: 2.5445865201950073
step: 1700 loss_mean: 2.690231690406799
step: 1750 loss_mean: 2.681995921134949
step: 1800 loss_mean: 2.681421365737915
step: 1850 loss_mean: 2.6332481908798218
step: 1900 loss_mean: 2.7352023935317993
step: 1950 loss_mean: 2.6808847522735597
step: 2000 loss_mean: 2.6952070331573488
step: 2050 loss_mean: 2.7381733894348144
step: 2100 loss_mean: 2.6625276803970337
step: 2150 loss_mean: 2.6594984006881712
step: 2200 loss_mean: 2.70529598236084
step: 2250 loss_mean: 2.6693655395507814
step: 2300 loss_mean: 2.679655451774597
step: 2350 loss_mean: 2.657656359672546
step: 2400 loss_mean: 2.721853094100952
step: 2450 loss_mean: 2.6620155906677248
step: 2500 loss_mean: 2.676895956993103
step: 2550 loss_mean: 2.6613648176193236
step: 2600 loss_mean: 2.648669834136963
step: 2650 loss_mean: 2.711762504577637
step: 2700 loss_mean: 2.701039490699768
step: 2750 loss_mean: 2.7500573205947876
step: 2800 loss_mean: 2.697536506652832
step: 2850 loss_mean: 2.705852918624878
step: 2900 loss_mean: 2.6970109224319456
step: 2950 loss_mean: 2.684577536582947
step: 3000 loss_mean: 2.6428601694107057
step: 3050 loss_mean: 2.6333577966690065
step: 3100 loss_mean: 2.752444725036621
step: 50 loss_mean: 2.733099503517151
step: 100 loss_mean: 2.760897994041443
step: 150 loss_mean: 2.693635439872742
step: 200 loss_mean: 2.796296029090881
step: 250 loss_mean: 2.773255648612976
step: 300 loss_mean: 2.856837787628174
step: 350 loss_mean: 2.721509938240051
step: 400 loss_mean: 2.7234759187698363
step: 450 loss_mean: 2.7088850021362303
step: 500 loss_mean: 2.7787925243377685
step: 550 loss_mean: 2.735937271118164
step: 600 loss_mean: 2.668729476928711
Epoch: 112 | Run time: 624.0 s | Train loss: 2.69 | Valid loss: 2.75
Saved checkpoint 112 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1621 s.
run_epoch
step: 50 loss_mean: 2.654238166809082
step: 100 loss_mean: 2.693022918701172
step: 150 loss_mean: 2.715274772644043
step: 200 loss_mean: 2.690788998603821
step: 250 loss_mean: 2.6691743898391724
step: 300 loss_mean: 2.689219536781311
step: 350 loss_mean: 2.6887412881851196
step: 400 loss_mean: 2.705658779144287
step: 450 loss_mean: 2.6639866065979003
step: 500 loss_mean: 2.6938148164749145
step: 550 loss_mean: 2.6466004133224486
step: 600 loss_mean: 2.702489261627197
step: 650 loss_mean: 2.641099143028259
step: 700 loss_mean: 2.6795388054847717
step: 750 loss_mean: 2.701858925819397
step: 800 loss_mean: 2.6366757011413573
step: 850 loss_mean: 2.6724656295776366
step: 900 loss_mean: 2.663535432815552
step: 950 loss_mean: 2.6711318111419677
step: 1000 loss_mean: 2.6546890687942506
step: 1050 loss_mean: 2.7472013998031617
step: 1100 loss_mean: 2.6337803411483764
step: 1150 loss_mean: 2.71321213722229
step: 1200 loss_mean: 2.703970046043396
step: 1250 loss_mean: 2.686851954460144
step: 1300 loss_mean: 2.6770590782165526
step: 1350 loss_mean: 2.6987436628341674
step: 1400 loss_mean: 2.68238920211792
step: 1450 loss_mean: 2.738533544540405
step: 1500 loss_mean: 2.690661063194275
step: 1550 loss_mean: 2.650198130607605
step: 1600 loss_mean: 2.718600730895996
step: 1650 loss_mean: 2.7216154766082763
step: 1700 loss_mean: 2.729570598602295
step: 1750 loss_mean: 2.659978551864624
step: 1800 loss_mean: 2.638606135845184
step: 1850 loss_mean: 2.685121216773987
step: 1900 loss_mean: 2.6659779596328734
step: 1950 loss_mean: 2.6854019927978516
step: 2000 loss_mean: 2.6842115259170534
step: 2050 loss_mean: 2.7306929874420165
step: 2100 loss_mean: 2.671148223876953
step: 2150 loss_mean: 2.6788833951950073
step: 2200 loss_mean: 2.7181793689727782
step: 2250 loss_mean: 2.7272186946868895
step: 2300 loss_mean: 2.7086390256881714
step: 2350 loss_mean: 2.727847924232483
step: 2400 loss_mean: 2.724655170440674
step: 2450 loss_mean: 2.695865817070007
step: 2500 loss_mean: 2.6775269985198973
step: 2550 loss_mean: 2.712780027389526
step: 2600 loss_mean: 2.7321207618713377
step: 2650 loss_mean: 2.7195665526390074
step: 2700 loss_mean: 2.6882254028320314
step: 2750 loss_mean: 2.677258219718933
step: 2800 loss_mean: 2.6731297302246095
step: 2850 loss_mean: 2.700011315345764
step: 2900 loss_mean: 2.6773900842666625
step: 2950 loss_mean: 2.6833815431594847
step: 3000 loss_mean: 2.6434000587463378
step: 3050 loss_mean: 2.693684878349304
step: 3100 loss_mean: 2.6345258235931395
step: 50 loss_mean: 2.753101472854614
step: 100 loss_mean: 2.7882967519760133
step: 150 loss_mean: 2.695363335609436
step: 200 loss_mean: 2.8071762704849244
step: 250 loss_mean: 2.7605128049850465
step: 300 loss_mean: 2.8606934785842895
step: 350 loss_mean: 2.7068220138549806
step: 400 loss_mean: 2.7209974336624145
step: 450 loss_mean: 2.717707176208496
step: 500 loss_mean: 2.78797963142395
step: 550 loss_mean: 2.727373704910278
step: 600 loss_mean: 2.673703417778015
Epoch: 113 | Run time: 622.0 s | Train loss: 2.69 | Valid loss: 2.75
Saved checkpoint 113 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1628 s.
run_epoch
step: 50 loss_mean: 2.6693288898468017
step: 100 loss_mean: 2.6792499446868896
step: 150 loss_mean: 2.6316055727005003
step: 200 loss_mean: 2.701221089363098
step: 250 loss_mean: 2.7237429237365722
step: 300 loss_mean: 2.6956511449813845
step: 350 loss_mean: 2.6693807315826414
step: 400 loss_mean: 2.6397703552246092
step: 450 loss_mean: 2.671967887878418
step: 500 loss_mean: 2.619465832710266
step: 550 loss_mean: 2.663687481880188
step: 600 loss_mean: 2.7084912109375
step: 650 loss_mean: 2.7443347549438477
step: 700 loss_mean: 2.661117601394653
step: 750 loss_mean: 2.720617723464966
step: 800 loss_mean: 2.7023691749572754
step: 850 loss_mean: 2.6765987873077393
step: 900 loss_mean: 2.676869068145752
step: 950 loss_mean: 2.7040186977386473
step: 1000 loss_mean: 2.687547135353088
step: 1050 loss_mean: 2.7034250593185423
step: 1100 loss_mean: 2.685102300643921
step: 1150 loss_mean: 2.6970868730545043
step: 1200 loss_mean: 2.6700495672225952
step: 1250 loss_mean: 2.7130623483657836
step: 1300 loss_mean: 2.740766806602478
step: 1350 loss_mean: 2.748311381340027
step: 1400 loss_mean: 2.722423405647278
step: 1450 loss_mean: 2.657415442466736
step: 1500 loss_mean: 2.673081521987915
step: 1550 loss_mean: 2.6626049852371216
step: 1600 loss_mean: 2.711086573600769
step: 1650 loss_mean: 2.6252221727371214
step: 1700 loss_mean: 2.6715344905853273
step: 1750 loss_mean: 2.656066436767578
step: 1800 loss_mean: 2.633201403617859
step: 1850 loss_mean: 2.705398235321045
step: 1900 loss_mean: 2.706139621734619
step: 1950 loss_mean: 2.6401181173324586
step: 2000 loss_mean: 2.6806927633285524
step: 2050 loss_mean: 2.694564309120178
step: 2100 loss_mean: 2.7322429513931272
step: 2150 loss_mean: 2.706960530281067
step: 2200 loss_mean: 2.6913925552368165
step: 2250 loss_mean: 2.7308479833602903
step: 2300 loss_mean: 2.658098564147949
step: 2350 loss_mean: 2.7306452226638793
step: 2400 loss_mean: 2.745070524215698
step: 2450 loss_mean: 2.687084016799927
step: 2500 loss_mean: 2.6905013275146485
step: 2550 loss_mean: 2.642560544013977
step: 2600 loss_mean: 2.716550016403198
step: 2650 loss_mean: 2.6662876844406127
step: 2700 loss_mean: 2.688182759284973
step: 2750 loss_mean: 2.729267063140869
step: 2800 loss_mean: 2.6585796880722046
step: 2850 loss_mean: 2.6617700815200807
step: 2900 loss_mean: 2.6676045274734497
step: 2950 loss_mean: 2.683084559440613
step: 3000 loss_mean: 2.7405672883987426
step: 3050 loss_mean: 2.69633442401886
step: 3100 loss_mean: 2.7219483947753904
step: 50 loss_mean: 2.7173454189300537
step: 100 loss_mean: 2.772150368690491
step: 150 loss_mean: 2.6836018228530882
step: 200 loss_mean: 2.794852685928345
step: 250 loss_mean: 2.742883772850037
step: 300 loss_mean: 2.832120780944824
step: 350 loss_mean: 2.70247766494751
step: 400 loss_mean: 2.7258393383026123
step: 450 loss_mean: 2.6998588705062865
step: 500 loss_mean: 2.7786440563201906
step: 550 loss_mean: 2.712152991294861
step: 600 loss_mean: 2.656387462615967
Epoch: 114 | Run time: 620.0 s | Train loss: 2.69 | Valid loss: 2.74
Saved checkpoint 114 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1639 s.
run_epoch
step: 50 loss_mean: 2.646142053604126
step: 100 loss_mean: 2.665344867706299
step: 150 loss_mean: 2.653277626037598
step: 200 loss_mean: 2.6853366184234617
step: 250 loss_mean: 2.68805100440979
step: 300 loss_mean: 2.6745976972579957
step: 350 loss_mean: 2.7041822624206544
step: 400 loss_mean: 2.6935230255126954
step: 450 loss_mean: 2.6777581548690796
step: 500 loss_mean: 2.745420398712158
step: 550 loss_mean: 2.7120328092575074
step: 600 loss_mean: 2.698977494239807
step: 650 loss_mean: 2.737713990211487
step: 700 loss_mean: 2.672619647979736
step: 750 loss_mean: 2.695290150642395
step: 800 loss_mean: 2.6770036125183108
step: 850 loss_mean: 2.7548060703277586
step: 900 loss_mean: 2.703768482208252
step: 950 loss_mean: 2.6878610324859618
step: 1000 loss_mean: 2.694740962982178
step: 1050 loss_mean: 2.6607554292678834
step: 1100 loss_mean: 2.6416415119171144
step: 1150 loss_mean: 2.6633321380615236
step: 1200 loss_mean: 2.647703380584717
step: 1250 loss_mean: 2.7173625946044924
step: 1300 loss_mean: 2.7199923753738404
step: 1350 loss_mean: 2.7542079639434816
step: 1400 loss_mean: 2.6780360889434816
step: 1450 loss_mean: 2.5996552896499634
step: 1500 loss_mean: 2.6484225273132322
step: 1550 loss_mean: 2.7054462242126465
step: 1600 loss_mean: 2.7373071050643922
step: 1650 loss_mean: 2.6852947950363157
step: 1700 loss_mean: 2.7096859550476076
step: 1750 loss_mean: 2.722420072555542
step: 1800 loss_mean: 2.6366852569580077
step: 1850 loss_mean: 2.646477379798889
step: 1900 loss_mean: 2.662423920631409
step: 1950 loss_mean: 2.7360108518600463
step: 2000 loss_mean: 2.646308102607727
step: 2050 loss_mean: 2.7064707565307615
step: 2100 loss_mean: 2.6981412506103517
step: 2150 loss_mean: 2.703225402832031
step: 2200 loss_mean: 2.7168158864974976
step: 2250 loss_mean: 2.7014138174057005
step: 2300 loss_mean: 2.6790068435668943
step: 2350 loss_mean: 2.655049982070923
step: 2400 loss_mean: 2.716384811401367
step: 2450 loss_mean: 2.6741427755355835
step: 2500 loss_mean: 2.6491002321243284
step: 2550 loss_mean: 2.6934126949310304
step: 2600 loss_mean: 2.6957156443595887
step: 2650 loss_mean: 2.6351050901412965
step: 2700 loss_mean: 2.6813935470581054
step: 2750 loss_mean: 2.677612533569336
step: 2800 loss_mean: 2.7000325441360475
step: 2850 loss_mean: 2.69930784702301
step: 2900 loss_mean: 2.663213856220245
step: 2950 loss_mean: 2.700956287384033
step: 3000 loss_mean: 2.606088123321533
step: 3050 loss_mean: 2.6799252367019655
step: 3100 loss_mean: 2.678626732826233
step: 50 loss_mean: 2.7228876161575317
step: 100 loss_mean: 2.777218708992004
step: 150 loss_mean: 2.693743553161621
step: 200 loss_mean: 2.785232038497925
step: 250 loss_mean: 2.752128872871399
step: 300 loss_mean: 2.834354057312012
step: 350 loss_mean: 2.7088255786895754
step: 400 loss_mean: 2.7292634010314942
step: 450 loss_mean: 2.692467622756958
step: 500 loss_mean: 2.7761418628692627
step: 550 loss_mean: 2.7168759155273436
step: 600 loss_mean: 2.6657285690307617
Epoch: 115 | Run time: 622.0 s | Train loss: 2.69 | Valid loss: 2.74
Saved checkpoint 115 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1655 s.
run_epoch
step: 50 loss_mean: 2.739140329360962
step: 100 loss_mean: 2.709807996749878
step: 150 loss_mean: 2.6752261447906496
step: 200 loss_mean: 2.6356591606140136
step: 250 loss_mean: 2.742882890701294
step: 300 loss_mean: 2.625927767753601
step: 350 loss_mean: 2.638092613220215
step: 400 loss_mean: 2.733402066230774
step: 450 loss_mean: 2.611233375072479
step: 500 loss_mean: 2.626017632484436
step: 550 loss_mean: 2.6852123165130615
step: 600 loss_mean: 2.6787315464019774
step: 650 loss_mean: 2.671268873214722
step: 700 loss_mean: 2.7084071683883666
step: 750 loss_mean: 2.717214674949646
step: 800 loss_mean: 2.7158648347854615
step: 850 loss_mean: 2.6675745105743407
step: 900 loss_mean: 2.6475328969955445
step: 950 loss_mean: 2.694901075363159
step: 1000 loss_mean: 2.6772355222702027
step: 1050 loss_mean: 2.715829539299011
step: 1100 loss_mean: 2.6491610193252564
step: 1150 loss_mean: 2.755019087791443
step: 1200 loss_mean: 2.662594165802002
step: 1250 loss_mean: 2.7541433000564575
step: 1300 loss_mean: 2.7463336753845216
step: 1350 loss_mean: 2.672367339134216
step: 1400 loss_mean: 2.6856644916534425
step: 1450 loss_mean: 2.6422293281555174
step: 1500 loss_mean: 2.626965980529785
step: 1550 loss_mean: 2.6342333984375
step: 1600 loss_mean: 2.730859622955322
step: 1650 loss_mean: 2.7267653846740725
step: 1700 loss_mean: 2.6364500093460084
step: 1750 loss_mean: 2.6616976690292358
step: 1800 loss_mean: 2.6513728046417238
step: 1850 loss_mean: 2.6982426595687867
step: 1900 loss_mean: 2.7092390871047973
step: 1950 loss_mean: 2.688361253738403
step: 2000 loss_mean: 2.677327871322632
step: 2050 loss_mean: 2.656920247077942
step: 2100 loss_mean: 2.6937512159347534
step: 2150 loss_mean: 2.6635934925079345
step: 2200 loss_mean: 2.683142313957214
step: 2250 loss_mean: 2.684984745979309
step: 2300 loss_mean: 2.7052240228652953
step: 2350 loss_mean: 2.7436853408813477
step: 2400 loss_mean: 2.698564486503601
step: 2450 loss_mean: 2.675879273414612
step: 2500 loss_mean: 2.6284774684906007
step: 2550 loss_mean: 2.6865348291397093
step: 2600 loss_mean: 2.6892114877700806
step: 2650 loss_mean: 2.729658908843994
step: 2700 loss_mean: 2.6692582035064696
step: 2750 loss_mean: 2.760996904373169
step: 2800 loss_mean: 2.705342411994934
step: 2850 loss_mean: 2.6891126012802125
step: 2900 loss_mean: 2.6958028268814087
step: 2950 loss_mean: 2.6515309476852416
step: 3000 loss_mean: 2.69516902923584
step: 3050 loss_mean: 2.6700066757202148
step: 3100 loss_mean: 2.6483635568618773
step: 50 loss_mean: 2.7378661680221557
step: 100 loss_mean: 2.7649856996536255
step: 150 loss_mean: 2.7161639642715456
step: 200 loss_mean: 2.7889295339584352
step: 250 loss_mean: 2.757625298500061
step: 300 loss_mean: 2.836539740562439
step: 350 loss_mean: 2.714569444656372
step: 400 loss_mean: 2.71822838306427
step: 450 loss_mean: 2.7169276094436645
step: 500 loss_mean: 2.7832201957702636
step: 550 loss_mean: 2.720152974128723
step: 600 loss_mean: 2.670170478820801
Epoch: 116 | Run time: 625.0 s | Train loss: 2.69 | Valid loss: 2.74
Saved checkpoint 116 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1672 s.
run_epoch
step: 50 loss_mean: 2.6495661783218383
step: 100 loss_mean: 2.7132863807678222
step: 150 loss_mean: 2.7050089836120605
step: 200 loss_mean: 2.597065262794495
step: 250 loss_mean: 2.6833040380477904
step: 300 loss_mean: 2.6837955999374388
step: 350 loss_mean: 2.6999800395965576
step: 400 loss_mean: 2.6845646047592164
step: 450 loss_mean: 2.6226069784164427
step: 500 loss_mean: 2.7021465587615965
step: 550 loss_mean: 2.7114154434204103
step: 600 loss_mean: 2.6661798095703126
step: 650 loss_mean: 2.679142656326294
step: 700 loss_mean: 2.6521987676620484
step: 750 loss_mean: 2.6478186225891114
step: 800 loss_mean: 2.656657519340515
step: 850 loss_mean: 2.664971022605896
step: 900 loss_mean: 2.6933060264587403
step: 950 loss_mean: 2.6716443729400634
step: 1000 loss_mean: 2.6371335315704347
step: 1050 loss_mean: 2.700229287147522
step: 1100 loss_mean: 2.715643243789673
step: 1150 loss_mean: 2.679885215759277
step: 1200 loss_mean: 2.714213571548462
step: 1250 loss_mean: 2.6364609956741334
step: 1300 loss_mean: 2.687776470184326
step: 1350 loss_mean: 2.7013465309143068
step: 1400 loss_mean: 2.7186884689331055
step: 1450 loss_mean: 2.6759690713882445
step: 1500 loss_mean: 2.6983767986297607
step: 1550 loss_mean: 2.672935619354248
step: 1600 loss_mean: 2.6897695112228392
step: 1650 loss_mean: 2.614416489601135
step: 1700 loss_mean: 2.6583741903305054
step: 1750 loss_mean: 2.732680444717407
step: 1800 loss_mean: 2.6615793085098267
step: 1850 loss_mean: 2.693541030883789
step: 1900 loss_mean: 2.6756994247436525
step: 1950 loss_mean: 2.7181051445007323
step: 2000 loss_mean: 2.65713707447052
step: 2050 loss_mean: 2.6601312565803528
step: 2100 loss_mean: 2.676146607398987
step: 2150 loss_mean: 2.690401802062988
step: 2200 loss_mean: 2.665995979309082
step: 2250 loss_mean: 2.7498058891296386
step: 2300 loss_mean: 2.7380777549743653
step: 2350 loss_mean: 2.6625091886520384
step: 2400 loss_mean: 2.675602149963379
step: 2450 loss_mean: 2.7078554153442385
step: 2500 loss_mean: 2.61619571685791
step: 2550 loss_mean: 2.6767860746383665
step: 2600 loss_mean: 2.689493775367737
step: 2650 loss_mean: 2.6941493701934816
step: 2700 loss_mean: 2.682784457206726
step: 2750 loss_mean: 2.6827661561965943
step: 2800 loss_mean: 2.677033848762512
step: 2850 loss_mean: 2.725904402732849
step: 2900 loss_mean: 2.646802453994751
step: 2950 loss_mean: 2.698150091171265
step: 3000 loss_mean: 2.7479338455200195
step: 3050 loss_mean: 2.7115135908126833
step: 3100 loss_mean: 2.7484627962112427
step: 50 loss_mean: 2.7378186321258546
step: 100 loss_mean: 2.791023139953613
step: 150 loss_mean: 2.707982792854309
step: 200 loss_mean: 2.8009999799728393
step: 250 loss_mean: 2.738312439918518
step: 300 loss_mean: 2.8663069152832032
step: 350 loss_mean: 2.7415407037734987
step: 400 loss_mean: 2.729925584793091
step: 450 loss_mean: 2.7115240573883055
step: 500 loss_mean: 2.799856071472168
step: 550 loss_mean: 2.72617693901062
step: 600 loss_mean: 2.677237377166748
Epoch: 117 | Run time: 623.0 s | Train loss: 2.68 | Valid loss: 2.75
Saved checkpoint 117 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1691 s.
run_epoch
step: 50 loss_mean: 2.7174740409851075
step: 100 loss_mean: 2.670263867378235
step: 150 loss_mean: 2.6300943660736085
step: 200 loss_mean: 2.6345443058013918
step: 250 loss_mean: 2.7162062549591064
step: 300 loss_mean: 2.655002498626709
step: 350 loss_mean: 2.6580650329589846
step: 400 loss_mean: 2.7692553091049192
step: 450 loss_mean: 2.6867428493499754
step: 500 loss_mean: 2.708588547706604
step: 550 loss_mean: 2.6251975154876708
step: 600 loss_mean: 2.6636169576644897
step: 650 loss_mean: 2.651077599525452
step: 700 loss_mean: 2.7283016633987427
step: 750 loss_mean: 2.6491250801086426
step: 800 loss_mean: 2.617102084159851
step: 850 loss_mean: 2.669315371513367
step: 900 loss_mean: 2.600885548591614
step: 950 loss_mean: 2.688386640548706
step: 1000 loss_mean: 2.691387357711792
step: 1050 loss_mean: 2.667440962791443
step: 1100 loss_mean: 2.7004519367218016
step: 1150 loss_mean: 2.6791191339492797
step: 1200 loss_mean: 2.6769054317474366
step: 1250 loss_mean: 2.6537082099914553
step: 1300 loss_mean: 2.6525500869750975
step: 1350 loss_mean: 2.655774998664856
step: 1400 loss_mean: 2.661986150741577
step: 1450 loss_mean: 2.7231846475601196
step: 1500 loss_mean: 2.73068145275116
step: 1550 loss_mean: 2.6774335527420043
step: 1600 loss_mean: 2.7036977577209473
step: 1650 loss_mean: 2.751931610107422
step: 1700 loss_mean: 2.734168891906738
step: 1750 loss_mean: 2.70778573513031
step: 1800 loss_mean: 2.705499086380005
step: 1850 loss_mean: 2.722796096801758
step: 1900 loss_mean: 2.6009384870529173
step: 1950 loss_mean: 2.6791603708267213
step: 2000 loss_mean: 2.6725687313079836
step: 2050 loss_mean: 2.6956661128997803
step: 2100 loss_mean: 2.6434516096115113
step: 2150 loss_mean: 2.726003017425537
step: 2200 loss_mean: 2.713998098373413
step: 2250 loss_mean: 2.7440628957748414
step: 2300 loss_mean: 2.6826366186141968
step: 2350 loss_mean: 2.6776642179489136
step: 2400 loss_mean: 2.7334281873703
step: 2450 loss_mean: 2.6624180364608763
step: 2500 loss_mean: 2.7117721796035767
step: 2550 loss_mean: 2.71262508392334
step: 2600 loss_mean: 2.7260045051574706
step: 2650 loss_mean: 2.708707513809204
step: 2700 loss_mean: 2.688294186592102
step: 2750 loss_mean: 2.639377431869507
step: 2800 loss_mean: 2.7262238836288453
step: 2850 loss_mean: 2.731549048423767
step: 2900 loss_mean: 2.6786058139801026
step: 2950 loss_mean: 2.6541364908218386
step: 3000 loss_mean: 2.70778546333313
step: 3050 loss_mean: 2.6600251388549805
step: 3100 loss_mean: 2.652166905403137
step: 50 loss_mean: 2.727362027168274
step: 100 loss_mean: 2.785076742172241
step: 150 loss_mean: 2.7006255102157595
step: 200 loss_mean: 2.7894251632690428
step: 250 loss_mean: 2.749596242904663
step: 300 loss_mean: 2.8469807577133177
step: 350 loss_mean: 2.7223382902145388
step: 400 loss_mean: 2.7578888082504274
step: 450 loss_mean: 2.6991233158111574
step: 500 loss_mean: 2.821289038658142
step: 550 loss_mean: 2.725826392173767
step: 600 loss_mean: 2.6774473667144774
Epoch: 118 | Run time: 620.0 s | Train loss: 2.69 | Valid loss: 2.75
Saved checkpoint 118 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1732 s.
run_epoch
step: 50 loss_mean: 2.7400899839401247
step: 100 loss_mean: 2.6986989259719847
step: 150 loss_mean: 2.661577591896057
step: 200 loss_mean: 2.694347128868103
step: 250 loss_mean: 2.714120192527771
step: 300 loss_mean: 2.651483664512634
step: 350 loss_mean: 2.708090691566467
step: 400 loss_mean: 2.7002072858810426
step: 450 loss_mean: 2.6918702268600465
step: 500 loss_mean: 2.6489015722274782
step: 550 loss_mean: 2.6907070779800417
step: 600 loss_mean: 2.6479903078079223
step: 650 loss_mean: 2.63294572353363
step: 700 loss_mean: 2.675086193084717
step: 750 loss_mean: 2.686055889129639
step: 800 loss_mean: 2.6461836957931517
step: 850 loss_mean: 2.6222455596923826
step: 900 loss_mean: 2.6908149909973145
step: 950 loss_mean: 2.7125721073150633
step: 1000 loss_mean: 2.7103324937820434
step: 1050 loss_mean: 2.7079316425323485
step: 1100 loss_mean: 2.7088465881347656
step: 1150 loss_mean: 2.671183018684387
step: 1200 loss_mean: 2.690686411857605
step: 1250 loss_mean: 2.689393186569214
step: 1300 loss_mean: 2.688021674156189
step: 1350 loss_mean: 2.7083879947662353
step: 1400 loss_mean: 2.7044714498519897
step: 1450 loss_mean: 2.715376672744751
step: 1500 loss_mean: 2.71086398601532
step: 1550 loss_mean: 2.67628568649292
step: 1600 loss_mean: 2.650697875022888
step: 1650 loss_mean: 2.7140270137786864
step: 1700 loss_mean: 2.7046246004104613
step: 1750 loss_mean: 2.7086082887649536
step: 1800 loss_mean: 2.73078369140625
step: 1850 loss_mean: 2.66285050868988
step: 1900 loss_mean: 2.6719664907455445
step: 1950 loss_mean: 2.6987084674835207
step: 2000 loss_mean: 2.693280806541443
step: 2050 loss_mean: 2.711210970878601
step: 2100 loss_mean: 2.6683790159225462
step: 2150 loss_mean: 2.637556691169739
step: 2200 loss_mean: 2.6851201009750367
step: 2250 loss_mean: 2.6872381448745726
step: 2300 loss_mean: 2.679886932373047
step: 2350 loss_mean: 2.6091736125946046
step: 2400 loss_mean: 2.664567856788635
step: 2450 loss_mean: 2.7339329719543457
step: 2500 loss_mean: 2.686086196899414
step: 2550 loss_mean: 2.681414313316345
step: 2600 loss_mean: 2.7291506004333494
step: 2650 loss_mean: 2.682988753318787
step: 2700 loss_mean: 2.686836042404175
step: 2750 loss_mean: 2.664468193054199
step: 2800 loss_mean: 2.680455083847046
step: 2850 loss_mean: 2.644271245002747
step: 2900 loss_mean: 2.6575484919548034
step: 2950 loss_mean: 2.68284068107605
step: 3000 loss_mean: 2.635585527420044
step: 3050 loss_mean: 2.681025972366333
step: 3100 loss_mean: 2.680436582565308
step: 50 loss_mean: 2.7359626483917237
step: 100 loss_mean: 2.8014984035491945
step: 150 loss_mean: 2.703714542388916
step: 200 loss_mean: 2.817483386993408
step: 250 loss_mean: 2.7829661417007445
step: 300 loss_mean: 2.8773232889175415
step: 350 loss_mean: 2.7029706263542175
step: 400 loss_mean: 2.7506614542007446
step: 450 loss_mean: 2.710914177894592
step: 500 loss_mean: 2.800319972038269
step: 550 loss_mean: 2.75208860874176
step: 600 loss_mean: 2.682838978767395
Epoch: 119 | Run time: 618.0 s | Train loss: 2.68 | Valid loss: 2.76
Saved checkpoint 119 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1713 s.
run_epoch
step: 50 loss_mean: 2.6364668941497804
step: 100 loss_mean: 2.6430052089691163
step: 150 loss_mean: 2.682471303939819
step: 200 loss_mean: 2.660716972351074
step: 250 loss_mean: 2.7133485221862794
step: 300 loss_mean: 2.7146960067749024
step: 350 loss_mean: 2.6590003967285156
step: 400 loss_mean: 2.650065064430237
step: 450 loss_mean: 2.6735199737548827
step: 500 loss_mean: 2.7143631744384766
step: 550 loss_mean: 2.6758159732818605
step: 600 loss_mean: 2.6960118293762205
step: 650 loss_mean: 2.659988102912903
step: 700 loss_mean: 2.690680184364319
step: 750 loss_mean: 2.676078543663025
step: 800 loss_mean: 2.7287311601638793
step: 850 loss_mean: 2.675862183570862
step: 900 loss_mean: 2.6977247619628906
step: 950 loss_mean: 2.6664441347122194
step: 1000 loss_mean: 2.7023964595794676
step: 1050 loss_mean: 2.698314790725708
step: 1100 loss_mean: 2.734275255203247
step: 1150 loss_mean: 2.7366870260238647
step: 1200 loss_mean: 2.6814366292953493
step: 1250 loss_mean: 2.7322529315948487
step: 1300 loss_mean: 2.6550893354415894
step: 1350 loss_mean: 2.706652159690857
step: 1400 loss_mean: 2.6429831647872923
step: 1450 loss_mean: 2.6450020551681517
step: 1500 loss_mean: 2.673297600746155
step: 1550 loss_mean: 2.6908195638656616
step: 1600 loss_mean: 2.6837270307540892
step: 1650 loss_mean: 2.632435441017151
step: 1700 loss_mean: 2.6847259187698365
step: 1750 loss_mean: 2.714223651885986
step: 1800 loss_mean: 2.649122366905212
step: 1850 loss_mean: 2.672081503868103
step: 1900 loss_mean: 2.6991866064071655
step: 1950 loss_mean: 2.634627242088318
step: 2000 loss_mean: 2.6924592018127442
step: 2050 loss_mean: 2.6757513952255247
step: 2100 loss_mean: 2.617509865760803
step: 2150 loss_mean: 2.679912486076355
step: 2200 loss_mean: 2.721541953086853
step: 2250 loss_mean: 2.693792324066162
step: 2300 loss_mean: 2.7031589698791505
step: 2350 loss_mean: 2.6862505149841307
step: 2400 loss_mean: 2.7099934339523317
step: 2450 loss_mean: 2.6813165187835692
step: 2500 loss_mean: 2.6393088483810425
step: 2550 loss_mean: 2.6941242170333863
step: 2600 loss_mean: 2.6943123292922975
step: 2650 loss_mean: 2.6729773473739624
step: 2700 loss_mean: 2.67588285446167
step: 2750 loss_mean: 2.7068508434295655
step: 2800 loss_mean: 2.7046365690231324
step: 2850 loss_mean: 2.7320157432556154
step: 2900 loss_mean: 2.680786967277527
step: 2950 loss_mean: 2.619287915229797
step: 3000 loss_mean: 2.6352919006347655
step: 3050 loss_mean: 2.6885890531539918
step: 3100 loss_mean: 2.6952372360229493
step: 50 loss_mean: 2.724574589729309
step: 100 loss_mean: 2.7790167045593264
step: 150 loss_mean: 2.6930103302001953
step: 200 loss_mean: 2.7935588455200193
step: 250 loss_mean: 2.747312517166138
step: 300 loss_mean: 2.850898714065552
step: 350 loss_mean: 2.6906220293045044
step: 400 loss_mean: 2.7410978412628175
step: 450 loss_mean: 2.704717268943787
step: 500 loss_mean: 2.7879218244552613
step: 550 loss_mean: 2.724574761390686
step: 600 loss_mean: 2.670275535583496
Epoch: 120 | Run time: 622.0 s | Train loss: 2.68 | Valid loss: 2.74
Saved checkpoint 120 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1718 s.
run_epoch
step: 50 loss_mean: 2.6755011510849
step: 100 loss_mean: 2.668055548667908
step: 150 loss_mean: 2.6596713876724243
step: 200 loss_mean: 2.6627746534347536
step: 250 loss_mean: 2.672006449699402
step: 300 loss_mean: 2.663659510612488
step: 350 loss_mean: 2.7157382106781007
step: 400 loss_mean: 2.680754976272583
step: 450 loss_mean: 2.6849461650848387
step: 500 loss_mean: 2.6670503759384157
step: 550 loss_mean: 2.684789638519287
step: 600 loss_mean: 2.7042179107666016
step: 650 loss_mean: 2.6821823215484617
step: 700 loss_mean: 2.6557795095443724
step: 750 loss_mean: 2.684078254699707
step: 800 loss_mean: 2.693082346916199
step: 850 loss_mean: 2.712122344970703
step: 900 loss_mean: 2.6520152711868286
step: 950 loss_mean: 2.641361515522003
step: 1000 loss_mean: 2.6938024091720583
step: 1050 loss_mean: 2.6787716245651243
step: 1100 loss_mean: 2.7003252363204955
step: 1150 loss_mean: 2.679457550048828
step: 1200 loss_mean: 2.6790002965927124
step: 1250 loss_mean: 2.6682234525680544
step: 1300 loss_mean: 2.6755683708190916
step: 1350 loss_mean: 2.67903160572052
step: 1400 loss_mean: 2.694210419654846
step: 1450 loss_mean: 2.6778293895721434
step: 1500 loss_mean: 2.6859663677215577
step: 1550 loss_mean: 2.686557722091675
step: 1600 loss_mean: 2.6990041875839235
step: 1650 loss_mean: 2.6791857624053956
step: 1700 loss_mean: 2.644619460105896
step: 1750 loss_mean: 2.654722547531128
step: 1800 loss_mean: 2.694538588523865
step: 1850 loss_mean: 2.66566312789917
step: 1900 loss_mean: 2.685009789466858
step: 1950 loss_mean: 2.682712588310242
step: 2000 loss_mean: 2.661011233329773
step: 2050 loss_mean: 2.715150442123413
step: 2100 loss_mean: 2.7162351322174074
step: 2150 loss_mean: 2.664492716789246
step: 2200 loss_mean: 2.6662405586242675
step: 2250 loss_mean: 2.659521708488464
step: 2300 loss_mean: 2.6664873361587524
step: 2350 loss_mean: 2.716593174934387
step: 2400 loss_mean: 2.6913530254364013
step: 2450 loss_mean: 2.711478362083435
step: 2500 loss_mean: 2.68624267578125
step: 2550 loss_mean: 2.6527518606185914
step: 2600 loss_mean: 2.677554750442505
step: 2650 loss_mean: 2.69595299243927
step: 2700 loss_mean: 2.6917809295654296
step: 2750 loss_mean: 2.688671579360962
step: 2800 loss_mean: 2.619908819198608
step: 2850 loss_mean: 2.6752010583877563
step: 2900 loss_mean: 2.680868625640869
step: 2950 loss_mean: 2.733240032196045
step: 3000 loss_mean: 2.6956403398513795
step: 3050 loss_mean: 2.730874819755554
step: 3100 loss_mean: 2.716848306655884
step: 50 loss_mean: 2.7068938541412355
step: 100 loss_mean: 2.766030902862549
step: 150 loss_mean: 2.6982940912246702
step: 200 loss_mean: 2.7886049222946165
step: 250 loss_mean: 2.7324168920516967
step: 300 loss_mean: 2.832352571487427
step: 350 loss_mean: 2.707919726371765
step: 400 loss_mean: 2.726451187133789
step: 450 loss_mean: 2.6929453945159914
step: 500 loss_mean: 2.7793457078933717
step: 550 loss_mean: 2.7210609579086302
step: 600 loss_mean: 2.652897834777832
Epoch: 121 | Run time: 624.0 s | Train loss: 2.68 | Valid loss: 2.73
Saved checkpoint 121 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1736 s.
run_epoch
step: 50 loss_mean: 2.6949890661239624
step: 100 loss_mean: 2.6055386805534364
step: 150 loss_mean: 2.6999334716796874
step: 200 loss_mean: 2.674995985031128
step: 250 loss_mean: 2.7125145769119263
step: 300 loss_mean: 2.6816733360290526
step: 350 loss_mean: 2.655102376937866
step: 400 loss_mean: 2.6272283792495728
step: 450 loss_mean: 2.6814449071884154
step: 500 loss_mean: 2.6909556770324707
step: 550 loss_mean: 2.6691702795028687
step: 600 loss_mean: 2.762352828979492
step: 650 loss_mean: 2.7267443180084228
step: 700 loss_mean: 2.699010443687439
step: 750 loss_mean: 2.707551040649414
step: 800 loss_mean: 2.6708086252212526
step: 850 loss_mean: 2.703680510520935
step: 900 loss_mean: 2.6297508525848388
step: 950 loss_mean: 2.681797456741333
step: 1000 loss_mean: 2.679504246711731
step: 1050 loss_mean: 2.6817721128463745
step: 1100 loss_mean: 2.7070511054992674
step: 1150 loss_mean: 2.665113205909729
step: 1200 loss_mean: 2.6782100343704225
step: 1250 loss_mean: 2.6432280349731445
step: 1300 loss_mean: 2.709019627571106
step: 1350 loss_mean: 2.663294072151184
step: 1400 loss_mean: 2.6582682800292967
step: 1450 loss_mean: 2.683928060531616
step: 1500 loss_mean: 2.665092821121216
step: 1550 loss_mean: 2.6743906021118162
step: 1600 loss_mean: 2.645858955383301
step: 1650 loss_mean: 2.690991425514221
step: 1700 loss_mean: 2.641592411994934
step: 1750 loss_mean: 2.6881738233566286
step: 1800 loss_mean: 2.7289000129699708
step: 1850 loss_mean: 2.745917167663574
step: 1900 loss_mean: 2.72275022983551
step: 1950 loss_mean: 2.731350688934326
step: 2000 loss_mean: 2.6778021097183227
step: 2050 loss_mean: 2.5997377681732177
step: 2100 loss_mean: 2.732757487297058
step: 2150 loss_mean: 2.6395924949645995
step: 2200 loss_mean: 2.7051186418533324
step: 2250 loss_mean: 2.660313081741333
step: 2300 loss_mean: 2.7190413331985472
step: 2350 loss_mean: 2.6952353143692016
step: 2400 loss_mean: 2.621168179512024
step: 2450 loss_mean: 2.6375237035751344
step: 2500 loss_mean: 2.6257389068603514
step: 2550 loss_mean: 2.7158145761489867
step: 2600 loss_mean: 2.6763388299942017
step: 2650 loss_mean: 2.7078120517730713
step: 2700 loss_mean: 2.712253842353821
step: 2750 loss_mean: 2.665156292915344
step: 2800 loss_mean: 2.744497494697571
step: 2850 loss_mean: 2.694402494430542
step: 2900 loss_mean: 2.6333505487442017
step: 2950 loss_mean: 2.6678803682327272
step: 3000 loss_mean: 2.6445819902420045
step: 3050 loss_mean: 2.692950849533081
step: 3100 loss_mean: 2.625156855583191
step: 50 loss_mean: 2.756979060173035
step: 100 loss_mean: 2.837134699821472
step: 150 loss_mean: 2.7279423522949218
step: 200 loss_mean: 2.8265479755401612
step: 250 loss_mean: 2.7801279735565188
step: 300 loss_mean: 2.8756254720687866
step: 350 loss_mean: 2.768044981956482
step: 400 loss_mean: 2.78246431350708
step: 450 loss_mean: 2.750874247550964
step: 500 loss_mean: 2.811649832725525
step: 550 loss_mean: 2.7491381549835205
step: 600 loss_mean: 2.725542492866516
Epoch: 122 | Run time: 625.0 s | Train loss: 2.68 | Valid loss: 2.78
Saved checkpoint 122 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1757 s.
run_epoch
step: 50 loss_mean: 2.6591654205322266
step: 100 loss_mean: 2.686246976852417
step: 150 loss_mean: 2.649420666694641
step: 200 loss_mean: 2.700460548400879
step: 250 loss_mean: 2.6674961280822753
step: 300 loss_mean: 2.7076389980316162
step: 350 loss_mean: 2.621945662498474
step: 400 loss_mean: 2.6225438594818113
step: 450 loss_mean: 2.679771590232849
step: 500 loss_mean: 2.677333035469055
step: 550 loss_mean: 2.6917883205413817
step: 600 loss_mean: 2.7199999141693114
step: 650 loss_mean: 2.6678257989883423
step: 700 loss_mean: 2.7297109270095827
step: 750 loss_mean: 2.655388579368591
step: 800 loss_mean: 2.717220330238342
step: 850 loss_mean: 2.659681134223938
step: 900 loss_mean: 2.7373144578933717
step: 950 loss_mean: 2.6537218475341797
step: 1000 loss_mean: 2.716983046531677
step: 1050 loss_mean: 2.6969213247299195
step: 1100 loss_mean: 2.6420836639404297
step: 1150 loss_mean: 2.7016617584228517
step: 1200 loss_mean: 2.6831975030899047
step: 1250 loss_mean: 2.6909837913513184
step: 1300 loss_mean: 2.723898048400879
step: 1350 loss_mean: 2.6342709732055662
step: 1400 loss_mean: 2.6739914655685424
step: 1450 loss_mean: 2.675889883041382
step: 1500 loss_mean: 2.654437551498413
step: 1550 loss_mean: 2.6959772157669066
step: 1600 loss_mean: 2.658817253112793
step: 1650 loss_mean: 2.7032522773742675
step: 1700 loss_mean: 2.720664234161377
step: 1750 loss_mean: 2.688628430366516
step: 1800 loss_mean: 2.6363899087905884
step: 1850 loss_mean: 2.7126531982421875
step: 1900 loss_mean: 2.6812954711914063
step: 1950 loss_mean: 2.672058353424072
step: 2000 loss_mean: 2.6462391805648804
step: 2050 loss_mean: 2.70836549282074
step: 2100 loss_mean: 2.7012655210494994
step: 2150 loss_mean: 2.6886639547348024
step: 2200 loss_mean: 2.6578585147857665
step: 2250 loss_mean: 2.659119987487793
step: 2300 loss_mean: 2.6722540950775144
step: 2350 loss_mean: 2.688106641769409
step: 2400 loss_mean: 2.6317223596572874
step: 2450 loss_mean: 2.681895923614502
step: 2500 loss_mean: 2.669130778312683
step: 2550 loss_mean: 2.7038525772094726
step: 2600 loss_mean: 2.622002000808716
step: 2650 loss_mean: 2.7375285148620607
step: 2700 loss_mean: 2.7096397829055787
step: 2750 loss_mean: 2.692422890663147
step: 2800 loss_mean: 2.676674041748047
step: 2850 loss_mean: 2.7440848207473754
step: 2900 loss_mean: 2.6698333978652955
step: 2950 loss_mean: 2.6268372392654418
step: 3000 loss_mean: 2.700991358757019
step: 3050 loss_mean: 2.7150713920593263
step: 3100 loss_mean: 2.7068177461624146
step: 50 loss_mean: 2.7311988162994383
step: 100 loss_mean: 2.7910344886779783
step: 150 loss_mean: 2.6974086284637453
step: 200 loss_mean: 2.8139637231826784
step: 250 loss_mean: 2.764743838310242
step: 300 loss_mean: 2.8622647762298583
step: 350 loss_mean: 2.722036681175232
step: 400 loss_mean: 2.7428794622421266
step: 450 loss_mean: 2.7113062381744384
step: 500 loss_mean: 2.796239023208618
step: 550 loss_mean: 2.7506979370117186
step: 600 loss_mean: 2.6894639205932616
Epoch: 123 | Run time: 622.0 s | Train loss: 2.68 | Valid loss: 2.76
Saved checkpoint 123 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.2133 s.
run_epoch
step: 50 loss_mean: 2.682448501586914
step: 100 loss_mean: 2.7186929178237915
step: 150 loss_mean: 2.5926109409332274
step: 200 loss_mean: 2.6871210098266602
step: 250 loss_mean: 2.741268529891968
step: 300 loss_mean: 2.625207681655884
step: 350 loss_mean: 2.7040065002441405
step: 400 loss_mean: 2.629969148635864
step: 450 loss_mean: 2.686151661872864
step: 500 loss_mean: 2.6795220613479613
step: 550 loss_mean: 2.6670862340927126
step: 600 loss_mean: 2.6797803401947022
step: 650 loss_mean: 2.6967695236206053
step: 700 loss_mean: 2.6032944893836976
step: 750 loss_mean: 2.723889036178589
step: 800 loss_mean: 2.6740987348556517
step: 850 loss_mean: 2.687335901260376
step: 900 loss_mean: 2.7322004318237303
step: 950 loss_mean: 2.6915950536727906
step: 1000 loss_mean: 2.680762948989868
step: 1050 loss_mean: 2.6935926246643067
step: 1100 loss_mean: 2.6721922016143798
step: 1150 loss_mean: 2.73695152759552
step: 1200 loss_mean: 2.696914496421814
step: 1250 loss_mean: 2.690027942657471
step: 1300 loss_mean: 2.6884199237823485
step: 1350 loss_mean: 2.680863690376282
step: 1400 loss_mean: 2.725898017883301
step: 1450 loss_mean: 2.708749866485596
step: 1500 loss_mean: 2.6718655490875243
step: 1550 loss_mean: 2.6577443361282347
step: 1600 loss_mean: 2.665161747932434
step: 1650 loss_mean: 2.7264081907272337
step: 1700 loss_mean: 2.6937212562561035
step: 1750 loss_mean: 2.695428376197815
step: 1800 loss_mean: 2.671704478263855
step: 1850 loss_mean: 2.66898585319519
step: 1900 loss_mean: 2.6479361248016358
step: 1950 loss_mean: 2.681656255722046
step: 2000 loss_mean: 2.6722815799713135
step: 2050 loss_mean: 2.6673433923721315
step: 2100 loss_mean: 2.660767550468445
step: 2150 loss_mean: 2.649208526611328
step: 2200 loss_mean: 2.6398346328735354
step: 2250 loss_mean: 2.6825611448287963
step: 2300 loss_mean: 2.68125657081604
step: 2350 loss_mean: 2.667875399589539
step: 2400 loss_mean: 2.658120574951172
step: 2450 loss_mean: 2.7360462760925293
step: 2500 loss_mean: 2.6912506294250487
step: 2550 loss_mean: 2.6374305438995362
step: 2600 loss_mean: 2.671056480407715
step: 2650 loss_mean: 2.6888031721115113
step: 2700 loss_mean: 2.679925198554993
step: 2750 loss_mean: 2.6923711824417116
step: 2800 loss_mean: 2.6746440649032595
step: 2850 loss_mean: 2.7296474742889405
step: 2900 loss_mean: 2.714669075012207
step: 2950 loss_mean: 2.65663631439209
step: 3000 loss_mean: 2.707979664802551
step: 3050 loss_mean: 2.7018504905700684
step: 3100 loss_mean: 2.690129837989807
step: 50 loss_mean: 2.7170314025878906
step: 100 loss_mean: 2.7726234579086304
step: 150 loss_mean: 2.6986880254745484
step: 200 loss_mean: 2.789419565200806
step: 250 loss_mean: 2.742881407737732
step: 300 loss_mean: 2.857131576538086
step: 350 loss_mean: 2.696511402130127
step: 400 loss_mean: 2.7267661762237547
step: 450 loss_mean: 2.707621831893921
step: 500 loss_mean: 2.7623237133026124
step: 550 loss_mean: 2.726938257217407
step: 600 loss_mean: 2.6630706691741945
Epoch: 124 | Run time: 624.0 s | Train loss: 2.68 | Valid loss: 2.74
Saved checkpoint 124 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1807 s.
run_epoch
step: 50 loss_mean: 2.699704785346985
step: 100 loss_mean: 2.638158073425293
step: 150 loss_mean: 2.6934722232818604
step: 200 loss_mean: 2.6420913219451903
step: 250 loss_mean: 2.6494116449356078
step: 300 loss_mean: 2.6546454095840453
step: 350 loss_mean: 2.711184663772583
step: 400 loss_mean: 2.6521512937545775
step: 450 loss_mean: 2.6532242631912233
step: 500 loss_mean: 2.643177661895752
step: 550 loss_mean: 2.6843222522735597
step: 600 loss_mean: 2.6855941009521485
step: 650 loss_mean: 2.6133608865737914
step: 700 loss_mean: 2.7052779054641722
step: 750 loss_mean: 2.7051119709014895
step: 800 loss_mean: 2.6222080850601195
step: 850 loss_mean: 2.6873895025253294
step: 900 loss_mean: 2.685793986320496
step: 950 loss_mean: 2.7087421083450316
step: 1000 loss_mean: 2.6048524522781373
step: 1050 loss_mean: 2.6208000659942625
step: 1100 loss_mean: 2.6616965913772583
step: 1150 loss_mean: 2.6566601371765137
step: 1200 loss_mean: 2.6978458547592163
step: 1250 loss_mean: 2.6668910360336304
step: 1300 loss_mean: 2.7070736503601074
step: 1350 loss_mean: 2.7225290441513064
step: 1400 loss_mean: 2.6170980453491213
step: 1450 loss_mean: 2.7229949712753294
step: 1500 loss_mean: 2.694854989051819
step: 1550 loss_mean: 2.6898014545440674
step: 1600 loss_mean: 2.7042577600479127
step: 1650 loss_mean: 2.6931420850753782
step: 1700 loss_mean: 2.7506176233291626
step: 1750 loss_mean: 2.61663857460022
step: 1800 loss_mean: 2.65162654876709
step: 1850 loss_mean: 2.646325125694275
step: 1900 loss_mean: 2.6807561206817625
step: 1950 loss_mean: 2.699395089149475
step: 2000 loss_mean: 2.6912054061889648
step: 2050 loss_mean: 2.72210373878479
step: 2100 loss_mean: 2.631419200897217
step: 2150 loss_mean: 2.618055205345154
step: 2200 loss_mean: 2.679227466583252
step: 2250 loss_mean: 2.712813868522644
step: 2300 loss_mean: 2.6754114532470705
step: 2350 loss_mean: 2.63807825088501
step: 2400 loss_mean: 2.735230712890625
step: 2450 loss_mean: 2.7034923219680786
step: 2500 loss_mean: 2.707095160484314
step: 2550 loss_mean: 2.7118465805053713
step: 2600 loss_mean: 2.6856197786331175
step: 2650 loss_mean: 2.6745983600616454
step: 2700 loss_mean: 2.6859333324432373
step: 2750 loss_mean: 2.688566608428955
step: 2800 loss_mean: 2.697273440361023
step: 2850 loss_mean: 2.7298048734664917
step: 2900 loss_mean: 2.6972185659408567
step: 2950 loss_mean: 2.675924563407898
step: 3000 loss_mean: 2.6785363578796386
step: 3050 loss_mean: 2.6446189785003664
step: 3100 loss_mean: 2.686322422027588
step: 50 loss_mean: 2.713011860847473
step: 100 loss_mean: 2.766842932701111
step: 150 loss_mean: 2.6995991325378417
step: 200 loss_mean: 2.7769127082824707
step: 250 loss_mean: 2.75302107334137
step: 300 loss_mean: 2.8328362274169923
step: 350 loss_mean: 2.7163448762893676
step: 400 loss_mean: 2.72911958694458
step: 450 loss_mean: 2.69053258895874
step: 500 loss_mean: 2.773132815361023
step: 550 loss_mean: 2.7219897508621216
step: 600 loss_mean: 2.6617999029159547
Epoch: 125 | Run time: 621.0 s | Train loss: 2.68 | Valid loss: 2.74
Saved checkpoint 125 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1794 s.
run_epoch
step: 50 loss_mean: 2.6640749168395996
step: 100 loss_mean: 2.6492221546173096
step: 150 loss_mean: 2.6359444761276247
step: 200 loss_mean: 2.669395351409912
step: 250 loss_mean: 2.661097388267517
step: 300 loss_mean: 2.6570106744766235
step: 350 loss_mean: 2.7131198120117186
step: 400 loss_mean: 2.7402722787857057
step: 450 loss_mean: 2.633619508743286
step: 500 loss_mean: 2.681176700592041
step: 550 loss_mean: 2.6728505897521972
step: 600 loss_mean: 2.695574178695679
step: 650 loss_mean: 2.7063321256637574
step: 700 loss_mean: 2.6782588291168214
step: 750 loss_mean: 2.6834419679641726
step: 800 loss_mean: 2.6984518861770628
step: 850 loss_mean: 2.715933084487915
step: 900 loss_mean: 2.684866676330566
step: 950 loss_mean: 2.738282279968262
step: 1000 loss_mean: 2.689149522781372
step: 1050 loss_mean: 2.674815464019775
step: 1100 loss_mean: 2.7299588203430174
step: 1150 loss_mean: 2.6609503650665283
step: 1200 loss_mean: 2.7091120195388796
step: 1250 loss_mean: 2.647071237564087
step: 1300 loss_mean: 2.591163744926453
step: 1350 loss_mean: 2.710073585510254
step: 1400 loss_mean: 2.708228807449341
step: 1450 loss_mean: 2.656690912246704
step: 1500 loss_mean: 2.6955298376083374
step: 1550 loss_mean: 2.69738383769989
step: 1600 loss_mean: 2.710034770965576
step: 1650 loss_mean: 2.663122463226318
step: 1700 loss_mean: 2.712021279335022
step: 1750 loss_mean: 2.7582930850982668
step: 1800 loss_mean: 2.6632556343078613
step: 1850 loss_mean: 2.67634934425354
step: 1900 loss_mean: 2.658153591156006
step: 1950 loss_mean: 2.663673324584961
step: 2000 loss_mean: 2.6819008111953737
step: 2050 loss_mean: 2.6867124748229982
step: 2100 loss_mean: 2.627588586807251
step: 2150 loss_mean: 2.6463570976257325
step: 2200 loss_mean: 2.7038971614837646
step: 2250 loss_mean: 2.7008047246932985
step: 2300 loss_mean: 2.7072709465026854
step: 2350 loss_mean: 2.6697267580032347
step: 2400 loss_mean: 2.641954851150513
step: 2450 loss_mean: 2.673819246292114
step: 2500 loss_mean: 2.7232882261276243
step: 2550 loss_mean: 2.670799422264099
step: 2600 loss_mean: 2.7036924028396605
step: 2650 loss_mean: 2.6232986307144164
step: 2700 loss_mean: 2.692939667701721
step: 2750 loss_mean: 2.673807921409607
step: 2800 loss_mean: 2.645951199531555
step: 2850 loss_mean: 2.6588038921356203
step: 2900 loss_mean: 2.6585252189636233
step: 2950 loss_mean: 2.6773153257369997
step: 3000 loss_mean: 2.658040027618408
step: 3050 loss_mean: 2.6619155597686768
step: 3100 loss_mean: 2.6771499633789064
step: 50 loss_mean: 2.721243267059326
step: 100 loss_mean: 2.7628244256973264
step: 150 loss_mean: 2.7001624155044555
step: 200 loss_mean: 2.7865866136550905
step: 250 loss_mean: 2.750936870574951
step: 300 loss_mean: 2.845730628967285
step: 350 loss_mean: 2.698646125793457
step: 400 loss_mean: 2.7252832508087157
step: 450 loss_mean: 2.699063444137573
step: 500 loss_mean: 2.7707630729675294
step: 550 loss_mean: 2.726387605667114
step: 600 loss_mean: 2.6622437381744386
Epoch: 126 | Run time: 625.0 s | Train loss: 2.68 | Valid loss: 2.74
Saved checkpoint 126 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1811 s.
run_epoch
step: 50 loss_mean: 2.658908414840698
step: 100 loss_mean: 2.7042751693725586
step: 150 loss_mean: 2.6381283617019653
step: 200 loss_mean: 2.652603712081909
step: 250 loss_mean: 2.6185229063034057
step: 300 loss_mean: 2.6539543581008913
step: 350 loss_mean: 2.6806083488464356
step: 400 loss_mean: 2.675643138885498
step: 450 loss_mean: 2.687491946220398
step: 500 loss_mean: 2.6701802968978883
step: 550 loss_mean: 2.6764207744598387
step: 600 loss_mean: 2.690486931800842
step: 650 loss_mean: 2.690106348991394
step: 700 loss_mean: 2.7241382026672363
step: 750 loss_mean: 2.6546663284301757
step: 800 loss_mean: 2.6706313943862914
step: 850 loss_mean: 2.689074230194092
step: 900 loss_mean: 2.711972961425781
step: 950 loss_mean: 2.697681875228882
step: 1000 loss_mean: 2.677159128189087
step: 1050 loss_mean: 2.76833101272583
step: 1100 loss_mean: 2.6985975074768067
step: 1150 loss_mean: 2.6607730388641357
step: 1200 loss_mean: 2.6640561962127687
step: 1250 loss_mean: 2.6404913473129272
step: 1300 loss_mean: 2.7451071882247926
step: 1350 loss_mean: 2.696906337738037
step: 1400 loss_mean: 2.6680700731277467
step: 1450 loss_mean: 2.674345684051514
step: 1500 loss_mean: 2.6925625371932984
step: 1550 loss_mean: 2.6686205077171326
step: 1600 loss_mean: 2.6834070110321044
step: 1650 loss_mean: 2.68293643951416
step: 1700 loss_mean: 2.676484761238098
step: 1750 loss_mean: 2.6909338998794556
step: 1800 loss_mean: 2.6809026622772216
step: 1850 loss_mean: 2.6210758972167967
step: 1900 loss_mean: 2.5855727624893188
step: 1950 loss_mean: 2.683425760269165
step: 2000 loss_mean: 2.6813317728042603
step: 2050 loss_mean: 2.660065507888794
step: 2100 loss_mean: 2.613850622177124
step: 2150 loss_mean: 2.712589268684387
step: 2200 loss_mean: 2.6683202743530274
step: 2250 loss_mean: 2.660512592792511
step: 2300 loss_mean: 2.629913511276245
step: 2350 loss_mean: 2.686729865074158
step: 2400 loss_mean: 2.687159185409546
step: 2450 loss_mean: 2.6187467432022093
step: 2500 loss_mean: 2.730479497909546
step: 2550 loss_mean: 2.6630307722091673
step: 2600 loss_mean: 2.6773298358917237
step: 2650 loss_mean: 2.7236208724975586
step: 2700 loss_mean: 2.711702084541321
step: 2750 loss_mean: 2.7049592876434327
step: 2800 loss_mean: 2.6529401874542238
step: 2850 loss_mean: 2.649795527458191
step: 2900 loss_mean: 2.6803719663619994
step: 2950 loss_mean: 2.709182662963867
step: 3000 loss_mean: 2.7011064291000366
step: 3050 loss_mean: 2.683124098777771
step: 3100 loss_mean: 2.666234030723572
step: 50 loss_mean: 2.741838893890381
step: 100 loss_mean: 2.8089888525009155
step: 150 loss_mean: 2.7046384048461913
step: 200 loss_mean: 2.7972532749176025
step: 250 loss_mean: 2.7917762279510496
step: 300 loss_mean: 2.869152264595032
step: 350 loss_mean: 2.7318164801597593
step: 400 loss_mean: 2.744733624458313
step: 450 loss_mean: 2.7292940330505373
step: 500 loss_mean: 2.8033453226089478
step: 550 loss_mean: 2.7460901403427123
step: 600 loss_mean: 2.6938446044921873
Epoch: 127 | Run time: 625.0 s | Train loss: 2.68 | Valid loss: 2.76
Saved checkpoint 127 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1845 s.
run_epoch
step: 50 loss_mean: 2.665433197021484
step: 100 loss_mean: 2.7270552349090575
step: 150 loss_mean: 2.6833798837661744
step: 200 loss_mean: 2.685943393707275
step: 250 loss_mean: 2.6934957790374754
step: 300 loss_mean: 2.667240328788757
step: 350 loss_mean: 2.6909417390823362
step: 400 loss_mean: 2.6982505559921264
step: 450 loss_mean: 2.681271667480469
step: 500 loss_mean: 2.69494482755661
step: 550 loss_mean: 2.6142932891845705
step: 600 loss_mean: 2.641916184425354
step: 650 loss_mean: 2.7001337432861328
step: 700 loss_mean: 2.7162672519683837
step: 750 loss_mean: 2.717296996116638
step: 800 loss_mean: 2.685553789138794
step: 850 loss_mean: 2.704230523109436
step: 900 loss_mean: 2.7421823835372923
step: 950 loss_mean: 2.672337532043457
step: 1000 loss_mean: 2.7104555416107177
step: 1050 loss_mean: 2.624615287780762
step: 1100 loss_mean: 2.653053059577942
step: 1150 loss_mean: 2.6768229961395265
step: 1200 loss_mean: 2.679366660118103
step: 1250 loss_mean: 2.6653939628601075
step: 1300 loss_mean: 2.7035226345062258
step: 1350 loss_mean: 2.6420879316329957
step: 1400 loss_mean: 2.743173670768738
step: 1450 loss_mean: 2.6110829162597655
step: 1500 loss_mean: 2.7507149314880373
step: 1550 loss_mean: 2.628155608177185
step: 1600 loss_mean: 2.634541049003601
step: 1650 loss_mean: 2.7027191066741945
step: 1700 loss_mean: 2.650569062232971
step: 1750 loss_mean: 2.6888261032104492
step: 1800 loss_mean: 2.6867699337005617
step: 1850 loss_mean: 2.6844242882728575
step: 1900 loss_mean: 2.643036012649536
step: 1950 loss_mean: 2.6738891220092773
step: 2000 loss_mean: 2.7096162366867067
step: 2050 loss_mean: 2.6473798370361328
step: 2100 loss_mean: 2.638612608909607
step: 2150 loss_mean: 2.6461050033569338
step: 2200 loss_mean: 2.688202738761902
step: 2250 loss_mean: 2.6254239749908446
step: 2300 loss_mean: 2.6816717052459715
step: 2350 loss_mean: 2.6801995706558226
step: 2400 loss_mean: 2.642761940956116
step: 2450 loss_mean: 2.6725497245788574
step: 2500 loss_mean: 2.687915105819702
step: 2550 loss_mean: 2.678505787849426
step: 2600 loss_mean: 2.695152316093445
step: 2650 loss_mean: 2.6884487438201905
step: 2700 loss_mean: 2.6695728302001953
step: 2750 loss_mean: 2.6638482809066772
step: 2800 loss_mean: 2.663075771331787
step: 2850 loss_mean: 2.6667534637451173
step: 2900 loss_mean: 2.703948516845703
step: 2950 loss_mean: 2.7111514139175417
step: 3000 loss_mean: 2.6680520486831667
step: 3050 loss_mean: 2.651069097518921
step: 3100 loss_mean: 2.6448123407363893
step: 50 loss_mean: 2.7187190151214597
step: 100 loss_mean: 2.78370276927948
step: 150 loss_mean: 2.694568634033203
step: 200 loss_mean: 2.7880553913116457
step: 250 loss_mean: 2.746379323005676
step: 300 loss_mean: 2.846567430496216
step: 350 loss_mean: 2.7014035391807556
step: 400 loss_mean: 2.7254806661605837
step: 450 loss_mean: 2.7012198305130006
step: 500 loss_mean: 2.7796943616867065
step: 550 loss_mean: 2.7261197423934935
step: 600 loss_mean: 2.6586364555358886
Epoch: 128 | Run time: 624.0 s | Train loss: 2.68 | Valid loss: 2.74
Saved checkpoint 128 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1839 s.
run_epoch
step: 50 loss_mean: 2.6983557891845704
step: 100 loss_mean: 2.648438820838928
step: 150 loss_mean: 2.6850894021987917
step: 200 loss_mean: 2.6990272331237795
step: 250 loss_mean: 2.6741047620773317
step: 300 loss_mean: 2.737686038017273
step: 350 loss_mean: 2.673656005859375
step: 400 loss_mean: 2.592524003982544
step: 450 loss_mean: 2.677711296081543
step: 500 loss_mean: 2.6715687704086304
step: 550 loss_mean: 2.719978132247925
step: 600 loss_mean: 2.6618690490722656
step: 650 loss_mean: 2.629417266845703
step: 700 loss_mean: 2.7293950080871583
step: 750 loss_mean: 2.6839207649230956
step: 800 loss_mean: 2.6475366830825804
step: 850 loss_mean: 2.7028106355667116
step: 900 loss_mean: 2.629676117897034
step: 950 loss_mean: 2.68043710231781
step: 1000 loss_mean: 2.6855476903915405
step: 1050 loss_mean: 2.6068437051773072
step: 1100 loss_mean: 2.6662879467010496
step: 1150 loss_mean: 2.656053328514099
step: 1200 loss_mean: 2.738761281967163
step: 1250 loss_mean: 2.658378448486328
step: 1300 loss_mean: 2.700091338157654
step: 1350 loss_mean: 2.7420500993728636
step: 1400 loss_mean: 2.664036555290222
step: 1450 loss_mean: 2.6763566589355468
step: 1500 loss_mean: 2.6740266847610474
step: 1550 loss_mean: 2.6814731550216675
step: 1600 loss_mean: 2.7084071826934815
step: 1650 loss_mean: 2.6808252429962156
step: 1700 loss_mean: 2.682069277763367
step: 1750 loss_mean: 2.6440783524513245
step: 1800 loss_mean: 2.70468656539917
step: 1850 loss_mean: 2.6712305402755736
step: 1900 loss_mean: 2.709983534812927
step: 1950 loss_mean: 2.665777115821838
step: 2000 loss_mean: 2.6784854888916017
step: 2050 loss_mean: 2.715958709716797
step: 2100 loss_mean: 2.6528932666778564
step: 2150 loss_mean: 2.6923548555374146
step: 2200 loss_mean: 2.6582588529586793
step: 2250 loss_mean: 2.7117248821258544
step: 2300 loss_mean: 2.6800217533111574
step: 2350 loss_mean: 2.6742154264450075
step: 2400 loss_mean: 2.6542289590835573
step: 2450 loss_mean: 2.6419487524032594
step: 2500 loss_mean: 2.654316396713257
step: 2550 loss_mean: 2.651882486343384
step: 2600 loss_mean: 2.680502886772156
step: 2650 loss_mean: 2.6720517826080323
step: 2700 loss_mean: 2.683912992477417
step: 2750 loss_mean: 2.7160877704620363
step: 2800 loss_mean: 2.6776880645751953
step: 2850 loss_mean: 2.6216186237335206
step: 2900 loss_mean: 2.659512858390808
step: 2950 loss_mean: 2.7314598894119264
step: 3000 loss_mean: 2.6576160526275636
step: 3050 loss_mean: 2.6527587842941283
step: 3100 loss_mean: 2.672049336433411
step: 50 loss_mean: 2.723776774406433
step: 100 loss_mean: 2.7799385166168213
step: 150 loss_mean: 2.702740488052368
step: 200 loss_mean: 2.795884094238281
step: 250 loss_mean: 2.7522834157943725
step: 300 loss_mean: 2.848679280281067
step: 350 loss_mean: 2.6847820615768434
step: 400 loss_mean: 2.7343794870376588
step: 450 loss_mean: 2.6997204494476317
step: 500 loss_mean: 2.7706427431106566
step: 550 loss_mean: 2.7223249197006227
step: 600 loss_mean: 2.663283443450928
Epoch: 129 | Run time: 625.0 s | Train loss: 2.68 | Valid loss: 2.74
Saved checkpoint 129 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1877 s.
run_epoch
step: 50 loss_mean: 2.6767388010025024
step: 100 loss_mean: 2.702267737388611
step: 150 loss_mean: 2.712155556678772
step: 200 loss_mean: 2.719045615196228
step: 250 loss_mean: 2.6647549724578856
step: 300 loss_mean: 2.67834632396698
step: 350 loss_mean: 2.683474383354187
step: 400 loss_mean: 2.7164611053466796
step: 450 loss_mean: 2.640629978179932
step: 500 loss_mean: 2.6708396339416502
step: 550 loss_mean: 2.6293647146224974
step: 600 loss_mean: 2.6637285947799683
step: 650 loss_mean: 2.689271993637085
step: 700 loss_mean: 2.6387265062332155
step: 750 loss_mean: 2.694548101425171
step: 800 loss_mean: 2.6319668292999268
step: 850 loss_mean: 2.6790850830078123
step: 900 loss_mean: 2.7007530546188354
step: 950 loss_mean: 2.678791618347168
step: 1000 loss_mean: 2.663370146751404
step: 1050 loss_mean: 2.6460593700408936
step: 1100 loss_mean: 2.7122472381591796
step: 1150 loss_mean: 2.6738213109970093
step: 1200 loss_mean: 2.7225806427001955
step: 1250 loss_mean: 2.6611702108383177
step: 1300 loss_mean: 2.726846990585327
step: 1350 loss_mean: 2.6437720680236816
step: 1400 loss_mean: 2.6783459043502806
step: 1450 loss_mean: 2.6879063034057618
step: 1500 loss_mean: 2.670011932849884
step: 1550 loss_mean: 2.6773015546798704
step: 1600 loss_mean: 2.6712477159500123
step: 1650 loss_mean: 2.6748397827148436
step: 1700 loss_mean: 2.6183264446258545
step: 1750 loss_mean: 2.676272864341736
step: 1800 loss_mean: 2.6869313383102416
step: 1850 loss_mean: 2.6402864503860473
step: 1900 loss_mean: 2.63123920917511
step: 1950 loss_mean: 2.625357985496521
step: 2000 loss_mean: 2.6705931186676026
step: 2050 loss_mean: 2.653996133804321
step: 2100 loss_mean: 2.7302165985107423
step: 2150 loss_mean: 2.6460610914230345
step: 2200 loss_mean: 2.6847988843917845
step: 2250 loss_mean: 2.686788296699524
step: 2300 loss_mean: 2.7011755990982054
step: 2350 loss_mean: 2.694455630779266
step: 2400 loss_mean: 2.702533917427063
step: 2450 loss_mean: 2.617191343307495
step: 2500 loss_mean: 2.687913374900818
step: 2550 loss_mean: 2.704825019836426
step: 2600 loss_mean: 2.673331651687622
step: 2650 loss_mean: 2.7313883209228518
step: 2700 loss_mean: 2.694810643196106
step: 2750 loss_mean: 2.689312553405762
step: 2800 loss_mean: 2.6917156744003297
step: 2850 loss_mean: 2.676190586090088
step: 2900 loss_mean: 2.7161454725265504
step: 2950 loss_mean: 2.6387526607513427
step: 3000 loss_mean: 2.6750457000732424
step: 3050 loss_mean: 2.6393962049484254
step: 3100 loss_mean: 2.700477042198181
step: 50 loss_mean: 2.7247252798080446
step: 100 loss_mean: 2.7612302112579346
step: 150 loss_mean: 2.683980259895325
step: 200 loss_mean: 2.790860695838928
step: 250 loss_mean: 2.757261862754822
step: 300 loss_mean: 2.8465989303588866
step: 350 loss_mean: 2.716541233062744
step: 400 loss_mean: 2.751163139343262
step: 450 loss_mean: 2.69141836643219
step: 500 loss_mean: 2.782858076095581
step: 550 loss_mean: 2.727466197013855
step: 600 loss_mean: 2.656025047302246
Epoch: 130 | Run time: 625.0 s | Train loss: 2.68 | Valid loss: 2.74
Saved checkpoint 130 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1869 s.
run_epoch
step: 50 loss_mean: 2.6737702798843386
step: 100 loss_mean: 2.64169517993927
step: 150 loss_mean: 2.6542486333847046
step: 200 loss_mean: 2.679209146499634
step: 250 loss_mean: 2.682657074928284
step: 300 loss_mean: 2.698147749900818
step: 350 loss_mean: 2.6404694366455077
step: 400 loss_mean: 2.7139549398422242
step: 450 loss_mean: 2.658903036117554
step: 500 loss_mean: 2.712329936027527
step: 550 loss_mean: 2.6886200284957886
step: 600 loss_mean: 2.643380584716797
step: 650 loss_mean: 2.625999302864075
step: 700 loss_mean: 2.6126970052719116
step: 750 loss_mean: 2.723567762374878
step: 800 loss_mean: 2.669849157333374
step: 850 loss_mean: 2.7180590438842773
step: 900 loss_mean: 2.6516047811508177
step: 950 loss_mean: 2.637408719062805
step: 1000 loss_mean: 2.5789800977706907
step: 1050 loss_mean: 2.664060702323914
step: 1100 loss_mean: 2.626926121711731
step: 1150 loss_mean: 2.6959558391571044
step: 1200 loss_mean: 2.594398741722107
step: 1250 loss_mean: 2.7044248008728027
step: 1300 loss_mean: 2.6720059871673585
step: 1350 loss_mean: 2.666304974555969
step: 1400 loss_mean: 2.6653387117385865
step: 1450 loss_mean: 2.715077905654907
step: 1500 loss_mean: 2.6839536094665526
step: 1550 loss_mean: 2.657325015068054
step: 1600 loss_mean: 2.662512435913086
step: 1650 loss_mean: 2.6794298934936522
step: 1700 loss_mean: 2.624135789871216
step: 1750 loss_mean: 2.6734320592880247
step: 1800 loss_mean: 2.6453878927230834
step: 1850 loss_mean: 2.6542872285842893
step: 1900 loss_mean: 2.7170337677001952
step: 1950 loss_mean: 2.668312849998474
step: 2000 loss_mean: 2.654149446487427
step: 2050 loss_mean: 2.6836153984069826
step: 2100 loss_mean: 2.6472188663482665
step: 2150 loss_mean: 2.6926563358306885
step: 2200 loss_mean: 2.6853693532943725
step: 2250 loss_mean: 2.708150782585144
step: 2300 loss_mean: 2.6766920948028563
step: 2350 loss_mean: 2.7335705041885374
step: 2400 loss_mean: 2.7303730392456056
step: 2450 loss_mean: 2.669188389778137
step: 2500 loss_mean: 2.7182271242141725
step: 2550 loss_mean: 2.6567229795455933
step: 2600 loss_mean: 2.6712828922271727
step: 2650 loss_mean: 2.685210328102112
step: 2700 loss_mean: 2.680345983505249
step: 2750 loss_mean: 2.6843849134445192
step: 2800 loss_mean: 2.6990620398521425
step: 2850 loss_mean: 2.695519208908081
step: 2900 loss_mean: 2.6806908416748048
step: 2950 loss_mean: 2.6879086685180664
step: 3000 loss_mean: 2.7548939037322997
step: 3050 loss_mean: 2.6690969371795656
step: 3100 loss_mean: 2.663584599494934
step: 50 loss_mean: 2.7227647876739502
step: 100 loss_mean: 2.776229529380798
step: 150 loss_mean: 2.708024892807007
step: 200 loss_mean: 2.7974436902999877
step: 250 loss_mean: 2.76851571559906
step: 300 loss_mean: 2.8503166246414184
step: 350 loss_mean: 2.7246163082122803
step: 400 loss_mean: 2.73645751953125
step: 450 loss_mean: 2.7121068954467775
step: 500 loss_mean: 2.7770304918289184
step: 550 loss_mean: 2.72926127910614
step: 600 loss_mean: 2.6606520223617554
Epoch: 131 | Run time: 626.0 s | Train loss: 2.67 | Valid loss: 2.75
Saved checkpoint 131 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1881 s.
run_epoch
step: 50 loss_mean: 2.66036057472229
step: 100 loss_mean: 2.6863392400741577
step: 150 loss_mean: 2.6640466260910034
step: 200 loss_mean: 2.6623056316375733
step: 250 loss_mean: 2.6475676822662355
step: 300 loss_mean: 2.6691225218772887
step: 350 loss_mean: 2.683063545227051
step: 400 loss_mean: 2.7077021741867067
step: 450 loss_mean: 2.6510983085632325
step: 500 loss_mean: 2.634437074661255
step: 550 loss_mean: 2.715596790313721
step: 600 loss_mean: 2.6896793270111083
step: 650 loss_mean: 2.668718400001526
step: 700 loss_mean: 2.67856436252594
step: 750 loss_mean: 2.6775458526611327
step: 800 loss_mean: 2.655392370223999
step: 850 loss_mean: 2.744433970451355
step: 900 loss_mean: 2.706187629699707
step: 950 loss_mean: 2.665994963645935
step: 1000 loss_mean: 2.6632508134841917
step: 1050 loss_mean: 2.692402091026306
step: 1100 loss_mean: 2.6694679355621336
step: 1150 loss_mean: 2.679702615737915
step: 1200 loss_mean: 2.674690384864807
step: 1250 loss_mean: 2.744466209411621
step: 1300 loss_mean: 2.626525363922119
step: 1350 loss_mean: 2.705028619766235
step: 1400 loss_mean: 2.6988714933395386
step: 1450 loss_mean: 2.6884825897216795
step: 1500 loss_mean: 2.674194326400757
step: 1550 loss_mean: 2.6558899593353273
step: 1600 loss_mean: 2.6885576152801516
step: 1650 loss_mean: 2.6468386888504027
step: 1700 loss_mean: 2.6784996128082277
step: 1750 loss_mean: 2.6467894458770753
step: 1800 loss_mean: 2.669861664772034
step: 1850 loss_mean: 2.6270074605941773
step: 1900 loss_mean: 2.630082983970642
step: 1950 loss_mean: 2.686686840057373
step: 2000 loss_mean: 2.6483765745162966
step: 2050 loss_mean: 2.6904200601577757
step: 2100 loss_mean: 2.660187487602234
step: 2150 loss_mean: 2.6384616136550902
step: 2200 loss_mean: 2.635304946899414
step: 2250 loss_mean: 2.6856357765197756
step: 2300 loss_mean: 2.684011011123657
step: 2350 loss_mean: 2.66567777633667
step: 2400 loss_mean: 2.63712700843811
step: 2450 loss_mean: 2.6780466270446777
step: 2500 loss_mean: 2.720642499923706
step: 2550 loss_mean: 2.689715919494629
step: 2600 loss_mean: 2.6578715419769288
step: 2650 loss_mean: 2.671260619163513
step: 2700 loss_mean: 2.707422037124634
step: 2750 loss_mean: 2.637852530479431
step: 2800 loss_mean: 2.6227199268341064
step: 2850 loss_mean: 2.6279653835296632
step: 2900 loss_mean: 2.7138370990753176
step: 2950 loss_mean: 2.716959528923035
step: 3000 loss_mean: 2.6837975072860716
step: 3050 loss_mean: 2.6980365562438964
step: 3100 loss_mean: 2.69011598110199
step: 50 loss_mean: 2.714270844459534
step: 100 loss_mean: 2.7746995878219605
step: 150 loss_mean: 2.699419798851013
step: 200 loss_mean: 2.8043980550765992
step: 250 loss_mean: 2.739091520309448
step: 300 loss_mean: 2.858005013465881
step: 350 loss_mean: 2.6975994157791137
step: 400 loss_mean: 2.7382004642486573
step: 450 loss_mean: 2.7032339763641358
step: 500 loss_mean: 2.787723698616028
step: 550 loss_mean: 2.734889988899231
step: 600 loss_mean: 2.6761152601242064
Epoch: 132 | Run time: 626.0 s | Train loss: 2.67 | Valid loss: 2.74
Saved checkpoint 132 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1895 s.
run_epoch
step: 50 loss_mean: 2.677280478477478
step: 100 loss_mean: 2.6053773736953736
step: 150 loss_mean: 2.6880009984970092
step: 200 loss_mean: 2.616122393608093
step: 250 loss_mean: 2.650188536643982
step: 300 loss_mean: 2.653316740989685
step: 350 loss_mean: 2.7488959550857546
step: 400 loss_mean: 2.688961215019226
step: 450 loss_mean: 2.6506545162200927
step: 500 loss_mean: 2.6813133478164675
step: 550 loss_mean: 2.6649630546569822
step: 600 loss_mean: 2.610081868171692
step: 650 loss_mean: 2.672523994445801
step: 700 loss_mean: 2.699607949256897
step: 750 loss_mean: 2.666140422821045
step: 800 loss_mean: 2.7580862760543825
step: 850 loss_mean: 2.6302099084854125
step: 900 loss_mean: 2.6753774738311766
step: 950 loss_mean: 2.6023029899597168
step: 1000 loss_mean: 2.6611549472808838
step: 1050 loss_mean: 2.6736030626296996
step: 1100 loss_mean: 2.6992838764190674
step: 1150 loss_mean: 2.6323134183883665
step: 1200 loss_mean: 2.738509998321533
step: 1250 loss_mean: 2.6487760972976684
step: 1300 loss_mean: 2.633367071151733
step: 1350 loss_mean: 2.648303894996643
step: 1400 loss_mean: 2.7042350578308105
step: 1450 loss_mean: 2.6969028425216677
step: 1500 loss_mean: 2.6339315080642702
step: 1550 loss_mean: 2.717078380584717
step: 1600 loss_mean: 2.688447427749634
step: 1650 loss_mean: 2.7099361848831176
step: 1700 loss_mean: 2.647773504257202
step: 1750 loss_mean: 2.6495088958740234
step: 1800 loss_mean: 2.6769419860839845
step: 1850 loss_mean: 2.6227642345428466
step: 1900 loss_mean: 2.691846833229065
step: 1950 loss_mean: 2.667800574302673
step: 2000 loss_mean: 2.644421310424805
step: 2050 loss_mean: 2.7184988689422607
step: 2100 loss_mean: 2.646200695037842
step: 2150 loss_mean: 2.622258563041687
step: 2200 loss_mean: 2.6855532550811767
step: 2250 loss_mean: 2.6832334661483763
step: 2300 loss_mean: 2.7110480499267577
step: 2350 loss_mean: 2.6591327810287475
step: 2400 loss_mean: 2.6404496002197266
step: 2450 loss_mean: 2.669953103065491
step: 2500 loss_mean: 2.6880269861221313
step: 2550 loss_mean: 2.7161348342895506
step: 2600 loss_mean: 2.6621168279647827
step: 2650 loss_mean: 2.6940247344970705
step: 2700 loss_mean: 2.7175924396514892
step: 2750 loss_mean: 2.7132422590255736
step: 2800 loss_mean: 2.690562062263489
step: 2850 loss_mean: 2.7193003368377684
step: 2900 loss_mean: 2.674341094493866
step: 2950 loss_mean: 2.6665900564193725
step: 3000 loss_mean: 2.730654528141022
step: 3050 loss_mean: 2.6711272192001343
step: 3100 loss_mean: 2.653804302215576
step: 50 loss_mean: 2.769675736427307
step: 100 loss_mean: 2.8333232307434084
step: 150 loss_mean: 2.7206784439086915
step: 200 loss_mean: 2.81951886177063
step: 250 loss_mean: 2.8060912609100344
step: 300 loss_mean: 2.885911059379578
step: 350 loss_mean: 2.7890657567977906
step: 400 loss_mean: 2.794326491355896
step: 450 loss_mean: 2.7451835966110227
step: 500 loss_mean: 2.8370711374282838
step: 550 loss_mean: 2.7661584568023683
step: 600 loss_mean: 2.731984496116638
Epoch: 133 | Run time: 625.0 s | Train loss: 2.67 | Valid loss: 2.79
Saved checkpoint 133 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1915 s.
run_epoch
step: 50 loss_mean: 2.6903653955459594
step: 100 loss_mean: 2.6487194299697876
step: 150 loss_mean: 2.6139846277236938
step: 200 loss_mean: 2.6562097311019897
step: 250 loss_mean: 2.6924229049682618
step: 300 loss_mean: 2.664107737541199
step: 350 loss_mean: 2.7365299749374388
step: 400 loss_mean: 2.701491732597351
step: 450 loss_mean: 2.637577395439148
step: 500 loss_mean: 2.668248200416565
step: 550 loss_mean: 2.598015432357788
step: 600 loss_mean: 2.626430912017822
step: 650 loss_mean: 2.668498115539551
step: 700 loss_mean: 2.6803978967666624
step: 750 loss_mean: 2.657971568107605
step: 800 loss_mean: 2.6187811040878297
step: 850 loss_mean: 2.714420027732849
step: 900 loss_mean: 2.69694073677063
step: 950 loss_mean: 2.6941039514541627
step: 1000 loss_mean: 2.6932367038726808
step: 1050 loss_mean: 2.665755867958069
step: 1100 loss_mean: 2.699927182197571
step: 1150 loss_mean: 2.6396893453598023
step: 1200 loss_mean: 2.6571534299850463
step: 1250 loss_mean: 2.6510035729408266
step: 1300 loss_mean: 2.6299886178970335
step: 1350 loss_mean: 2.7254465675354003
step: 1400 loss_mean: 2.626228094100952
step: 1450 loss_mean: 2.6831911325454714
step: 1500 loss_mean: 2.6691464233398436
step: 1550 loss_mean: 2.654221649169922
step: 1600 loss_mean: 2.681308317184448
step: 1650 loss_mean: 2.6403560733795164
step: 1700 loss_mean: 2.6917611598968505
step: 1750 loss_mean: 2.706663899421692
step: 1800 loss_mean: 2.679069700241089
step: 1850 loss_mean: 2.7099706888198853
step: 1900 loss_mean: 2.6926807165145874
step: 1950 loss_mean: 2.7016010737419127
step: 2000 loss_mean: 2.626697072982788
step: 2050 loss_mean: 2.6972736740112304
step: 2100 loss_mean: 2.6790070581436156
step: 2150 loss_mean: 2.651737246513367
step: 2200 loss_mean: 2.6427740716934203
step: 2250 loss_mean: 2.644594087600708
step: 2300 loss_mean: 2.6607648277282716
step: 2350 loss_mean: 2.642274489402771
step: 2400 loss_mean: 2.70318519115448
step: 2450 loss_mean: 2.6851047897338867
step: 2500 loss_mean: 2.6436412858963014
step: 2550 loss_mean: 2.699503917694092
step: 2600 loss_mean: 2.6476821184158323
step: 2650 loss_mean: 2.6790056943893434
step: 2700 loss_mean: 2.673535146713257
step: 2750 loss_mean: 2.670153660774231
step: 2800 loss_mean: 2.6966796922683716
step: 2850 loss_mean: 2.7050574254989623
step: 2900 loss_mean: 2.6617554664611816
step: 2950 loss_mean: 2.7366993045806884
step: 3000 loss_mean: 2.687809052467346
step: 3050 loss_mean: 2.66117301940918
step: 3100 loss_mean: 2.704523220062256
step: 50 loss_mean: 2.736317362785339
step: 100 loss_mean: 2.811353077888489
step: 150 loss_mean: 2.721783366203308
step: 200 loss_mean: 2.7976164484024046
step: 250 loss_mean: 2.7609629821777344
step: 300 loss_mean: 2.85413941860199
step: 350 loss_mean: 2.732030348777771
step: 400 loss_mean: 2.7437703037261962
step: 450 loss_mean: 2.713273253440857
step: 500 loss_mean: 2.8031534862518313
step: 550 loss_mean: 2.730933918952942
step: 600 loss_mean: 2.679065227508545
Epoch: 134 | Run time: 624.0 s | Train loss: 2.67 | Valid loss: 2.76
Saved checkpoint 134 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1926 s.
run_epoch
step: 50 loss_mean: 2.6291563081741334
step: 100 loss_mean: 2.7087709140777587
step: 150 loss_mean: 2.6743299770355224
step: 200 loss_mean: 2.685666799545288
step: 250 loss_mean: 2.668665566444397
step: 300 loss_mean: 2.6589693641662597
step: 350 loss_mean: 2.6864179754257203
step: 400 loss_mean: 2.64112464427948
step: 450 loss_mean: 2.6863274717330934
step: 500 loss_mean: 2.697410957813263
step: 550 loss_mean: 2.7139039039611816
step: 600 loss_mean: 2.6692203664779663
step: 650 loss_mean: 2.672865743637085
step: 700 loss_mean: 2.635680985450745
step: 750 loss_mean: 2.649845771789551
step: 800 loss_mean: 2.647530517578125
step: 850 loss_mean: 2.6754277753829956
step: 900 loss_mean: 2.6793779230117796
step: 950 loss_mean: 2.6820604515075686
step: 1000 loss_mean: 2.6710428857803343
step: 1050 loss_mean: 2.684096293449402
step: 1100 loss_mean: 2.695783896446228
step: 1150 loss_mean: 2.6908321952819825
step: 1200 loss_mean: 2.6741863346099852
step: 1250 loss_mean: 2.634990200996399
step: 1300 loss_mean: 2.6604847526550293
step: 1350 loss_mean: 2.6506826210021974
step: 1400 loss_mean: 2.7217275381088255
step: 1450 loss_mean: 2.6965416049957276
step: 1500 loss_mean: 2.6786172008514404
step: 1550 loss_mean: 2.646774730682373
step: 1600 loss_mean: 2.640515966415405
step: 1650 loss_mean: 2.6420767879486085
step: 1700 loss_mean: 2.660506854057312
step: 1750 loss_mean: 2.658601460456848
step: 1800 loss_mean: 2.6804110383987427
step: 1850 loss_mean: 2.6582569217681886
step: 1900 loss_mean: 2.64125111579895
step: 1950 loss_mean: 2.6887130832672117
step: 2000 loss_mean: 2.6245588159561155
step: 2050 loss_mean: 2.6917336559295655
step: 2100 loss_mean: 2.635036861896515
step: 2150 loss_mean: 2.6880016422271726
step: 2200 loss_mean: 2.6735659551620485
step: 2250 loss_mean: 2.6701695680618287
step: 2300 loss_mean: 2.687828769683838
step: 2350 loss_mean: 2.6797040557861327
step: 2400 loss_mean: 2.687978549003601
step: 2450 loss_mean: 2.7259141874313353
step: 2500 loss_mean: 2.6539434385299683
step: 2550 loss_mean: 2.713246250152588
step: 2600 loss_mean: 2.722660684585571
step: 2650 loss_mean: 2.6692767763137817
step: 2700 loss_mean: 2.6587617588043213
step: 2750 loss_mean: 2.7133738136291505
step: 2800 loss_mean: 2.6339267253875733
step: 2850 loss_mean: 2.6377826070785524
step: 2900 loss_mean: 2.7134034919738768
step: 2950 loss_mean: 2.654120512008667
step: 3000 loss_mean: 2.685861701965332
step: 3050 loss_mean: 2.6753818559646607
step: 3100 loss_mean: 2.647094283103943
step: 50 loss_mean: 2.7433001232147216
step: 100 loss_mean: 2.787302083969116
step: 150 loss_mean: 2.7004089832305906
step: 200 loss_mean: 2.7835841512680055
step: 250 loss_mean: 2.7737934017181396
step: 300 loss_mean: 2.8602465534210206
step: 350 loss_mean: 2.7078326416015623
step: 400 loss_mean: 2.7490572261810304
step: 450 loss_mean: 2.7114611434936524
step: 500 loss_mean: 2.7959280967712403
step: 550 loss_mean: 2.7292151260375976
step: 600 loss_mean: 2.6730517530441285
Epoch: 135 | Run time: 622.0 s | Train loss: 2.67 | Valid loss: 2.75
Saved checkpoint 135 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1932 s.
run_epoch
step: 50 loss_mean: 2.6631821632385253
step: 100 loss_mean: 2.680817551612854
step: 150 loss_mean: 2.6648503971099853
step: 200 loss_mean: 2.677480335235596
step: 250 loss_mean: 2.6901823043823243
step: 300 loss_mean: 2.6430662965774534
step: 350 loss_mean: 2.6097992944717405
step: 400 loss_mean: 2.6643602418899537
step: 450 loss_mean: 2.651495404243469
step: 500 loss_mean: 2.7013878202438355
step: 550 loss_mean: 2.634455223083496
step: 600 loss_mean: 2.586228709220886
step: 650 loss_mean: 2.6921409034729002
step: 700 loss_mean: 2.7095437812805176
step: 750 loss_mean: 2.6306216430664064
step: 800 loss_mean: 2.699936518669128
step: 850 loss_mean: 2.7260566663742067
step: 900 loss_mean: 2.680739974975586
step: 950 loss_mean: 2.6537628984451294
step: 1000 loss_mean: 2.661867141723633
step: 1050 loss_mean: 2.7100993394851685
step: 1100 loss_mean: 2.675422930717468
step: 1150 loss_mean: 2.633982572555542
step: 1200 loss_mean: 2.6406300592422487
step: 1250 loss_mean: 2.654706587791443
step: 1300 loss_mean: 2.6688897085189818
step: 1350 loss_mean: 2.6365164613723753
step: 1400 loss_mean: 2.6648674249649047
step: 1450 loss_mean: 2.7200496101379397
step: 1500 loss_mean: 2.7091554498672483
step: 1550 loss_mean: 2.6357886457443236
step: 1600 loss_mean: 2.6642789340019224
step: 1650 loss_mean: 2.706307888031006
step: 1700 loss_mean: 2.6064296770095825
step: 1750 loss_mean: 2.6991464042663575
step: 1800 loss_mean: 2.6692720794677736
step: 1850 loss_mean: 2.6976743793487548
step: 1900 loss_mean: 2.6840497875213623
step: 1950 loss_mean: 2.6734122800827027
step: 2000 loss_mean: 2.7084857034683227
step: 2050 loss_mean: 2.6589318180084227
step: 2100 loss_mean: 2.660008707046509
step: 2150 loss_mean: 2.656657795906067
step: 2200 loss_mean: 2.6966805553436277
step: 2250 loss_mean: 2.6351814699172973
step: 2300 loss_mean: 2.673425164222717
step: 2350 loss_mean: 2.645709767341614
step: 2400 loss_mean: 2.6295571184158324
step: 2450 loss_mean: 2.705428066253662
step: 2500 loss_mean: 2.649389400482178
step: 2550 loss_mean: 2.7273025465011598
step: 2600 loss_mean: 2.680535526275635
step: 2650 loss_mean: 2.716640214920044
step: 2700 loss_mean: 2.6863034629821776
step: 2750 loss_mean: 2.617220118045807
step: 2800 loss_mean: 2.707685387134552
step: 2850 loss_mean: 2.6411383295059205
step: 2900 loss_mean: 2.659612340927124
step: 2950 loss_mean: 2.6578260231018067
step: 3000 loss_mean: 2.6708972883224487
step: 3050 loss_mean: 2.732098994255066
step: 3100 loss_mean: 2.6732383966445923
step: 50 loss_mean: 2.770466890335083
step: 100 loss_mean: 2.8451381587982176
step: 150 loss_mean: 2.718997459411621
step: 200 loss_mean: 2.8359987783432006
step: 250 loss_mean: 2.7816733074188233
step: 300 loss_mean: 2.8873425912857056
step: 350 loss_mean: 2.751538953781128
step: 400 loss_mean: 2.781842575073242
step: 450 loss_mean: 2.749441132545471
step: 500 loss_mean: 2.8249142026901244
step: 550 loss_mean: 2.755760073661804
step: 600 loss_mean: 2.718642678260803
Epoch: 136 | Run time: 626.0 s | Train loss: 2.67 | Valid loss: 2.79
Saved checkpoint 136 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1949 s.
run_epoch
step: 50 loss_mean: 2.677322268486023
step: 100 loss_mean: 2.6677215242385866
step: 150 loss_mean: 2.7052058029174804
step: 200 loss_mean: 2.696891760826111
step: 250 loss_mean: 2.702848405838013
step: 300 loss_mean: 2.7057489490509035
step: 350 loss_mean: 2.657520670890808
step: 400 loss_mean: 2.6327199506759644
step: 450 loss_mean: 2.679377760887146
step: 500 loss_mean: 2.696803069114685
step: 550 loss_mean: 2.674559597969055
step: 600 loss_mean: 2.610690379142761
step: 650 loss_mean: 2.5921833229064943
step: 700 loss_mean: 2.7265014123916624
step: 750 loss_mean: 2.6747865056991578
step: 800 loss_mean: 2.6571881008148193
step: 850 loss_mean: 2.6780288314819334
step: 900 loss_mean: 2.672522253990173
step: 950 loss_mean: 2.6789304637908935
step: 1000 loss_mean: 2.6194233989715574
step: 1050 loss_mean: 2.620784544944763
step: 1100 loss_mean: 2.677867684364319
step: 1150 loss_mean: 2.697921104431152
step: 1200 loss_mean: 2.5925866889953615
step: 1250 loss_mean: 2.6449057960510256
step: 1300 loss_mean: 2.65056272983551
step: 1350 loss_mean: 2.6623029851913453
step: 1400 loss_mean: 2.6610626220703124
step: 1450 loss_mean: 2.6872253608703613
step: 1500 loss_mean: 2.702787256240845
step: 1550 loss_mean: 2.675039892196655
step: 1600 loss_mean: 2.6522006750106812
step: 1650 loss_mean: 2.7022651195526124
step: 1700 loss_mean: 2.663790059089661
step: 1750 loss_mean: 2.684752902984619
step: 1800 loss_mean: 2.6794875335693358
step: 1850 loss_mean: 2.7425046491622926
step: 1900 loss_mean: 2.678775420188904
step: 1950 loss_mean: 2.6738925170898438
step: 2000 loss_mean: 2.6974581384658816
step: 2050 loss_mean: 2.643494348526001
step: 2100 loss_mean: 2.6573637104034424
step: 2150 loss_mean: 2.695257601737976
step: 2200 loss_mean: 2.666784572601318
step: 2250 loss_mean: 2.65719286441803
step: 2300 loss_mean: 2.667699308395386
step: 2350 loss_mean: 2.639101309776306
step: 2400 loss_mean: 2.6125083494186403
step: 2450 loss_mean: 2.6005944538116457
step: 2500 loss_mean: 2.661685137748718
step: 2550 loss_mean: 2.667131767272949
step: 2600 loss_mean: 2.6948554706573487
step: 2650 loss_mean: 2.6597994470596316
step: 2700 loss_mean: 2.7108004570007322
step: 2750 loss_mean: 2.689516224861145
step: 2800 loss_mean: 2.6880244827270507
step: 2850 loss_mean: 2.6597498846054077
step: 2900 loss_mean: 2.6806623411178587
step: 2950 loss_mean: 2.7106795358657836
step: 3000 loss_mean: 2.637886838912964
step: 3050 loss_mean: 2.702962956428528
step: 3100 loss_mean: 2.669822325706482
step: 50 loss_mean: 2.733884267807007
step: 100 loss_mean: 2.78099045753479
step: 150 loss_mean: 2.704477672576904
step: 200 loss_mean: 2.800280394554138
step: 250 loss_mean: 2.7680109167099
step: 300 loss_mean: 2.8492322063446043
step: 350 loss_mean: 2.7194685530662537
step: 400 loss_mean: 2.7613653802871703
step: 450 loss_mean: 2.7076268148422242
step: 500 loss_mean: 2.798531761169434
step: 550 loss_mean: 2.7250957775115965
step: 600 loss_mean: 2.6737091064453127
Epoch: 137 | Run time: 617.0 s | Train loss: 2.67 | Valid loss: 2.75
Saved checkpoint 137 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.1974 s.
run_epoch
step: 50 loss_mean: 2.7088668823242186
step: 100 loss_mean: 2.662037477493286
step: 150 loss_mean: 2.7112008857727052
step: 200 loss_mean: 2.704089059829712
step: 250 loss_mean: 2.7223296976089477
step: 300 loss_mean: 2.664008240699768
step: 350 loss_mean: 2.7193298530578613
step: 400 loss_mean: 2.6725958299636843
step: 450 loss_mean: 2.621105875968933
step: 500 loss_mean: 2.6958287954330444
step: 550 loss_mean: 2.646588234901428
step: 600 loss_mean: 2.6636766815185546
step: 650 loss_mean: 2.6738729810714723
step: 700 loss_mean: 2.6120075464248655
step: 750 loss_mean: 2.6648252487182615
step: 800 loss_mean: 2.6775854635238647
step: 850 loss_mean: 2.6368574714660644
step: 900 loss_mean: 2.6459918928146364
step: 950 loss_mean: 2.677449951171875
step: 1000 loss_mean: 2.674950704574585
step: 1050 loss_mean: 2.7010069274902344
step: 1100 loss_mean: 2.677907724380493
step: 1150 loss_mean: 2.6442139768600463
step: 1200 loss_mean: 2.670599465370178
step: 1250 loss_mean: 2.698331847190857
step: 1300 loss_mean: 2.6613963556289675
step: 1350 loss_mean: 2.6625300884246825
step: 1400 loss_mean: 2.6884573125839233
step: 1450 loss_mean: 2.70503436088562
step: 1500 loss_mean: 2.656631989479065
step: 1550 loss_mean: 2.635099411010742
step: 1600 loss_mean: 2.6520964384078978
step: 1650 loss_mean: 2.659474883079529
step: 1700 loss_mean: 2.6704879331588747
step: 1750 loss_mean: 2.6955790424346926
step: 1800 loss_mean: 2.639713616371155
step: 1850 loss_mean: 2.658677272796631
step: 1900 loss_mean: 2.693740553855896
step: 1950 loss_mean: 2.6384193754196166
step: 2000 loss_mean: 2.679575262069702
step: 2050 loss_mean: 2.6669599866867064
step: 2100 loss_mean: 2.667556004524231
step: 2150 loss_mean: 2.6799202394485473
step: 2200 loss_mean: 2.680597949028015
step: 2250 loss_mean: 2.6984548234939574
step: 2300 loss_mean: 2.6347360897064207
step: 2350 loss_mean: 2.7219273614883424
step: 2400 loss_mean: 2.637721676826477
step: 2450 loss_mean: 2.6654834270477297
step: 2500 loss_mean: 2.6325323486328127
step: 2550 loss_mean: 2.667928171157837
step: 2600 loss_mean: 2.6766655588150026
step: 2650 loss_mean: 2.6848492574691774
step: 2700 loss_mean: 2.698597435951233
step: 2750 loss_mean: 2.642557544708252
step: 2800 loss_mean: 2.6487879037857054
step: 2850 loss_mean: 2.6773176097869875
step: 2900 loss_mean: 2.6921693515777587
step: 2950 loss_mean: 2.6236156606674195
step: 3000 loss_mean: 2.624853630065918
step: 3050 loss_mean: 2.687686381340027
step: 3100 loss_mean: 2.6266387128829956
step: 50 loss_mean: 2.7541145610809328
step: 100 loss_mean: 2.7710120391845705
step: 150 loss_mean: 2.727614951133728
step: 200 loss_mean: 2.8276120233535766
step: 250 loss_mean: 2.7760716247558594
step: 300 loss_mean: 2.874357876777649
step: 350 loss_mean: 2.7088881015777586
step: 400 loss_mean: 2.78991644859314
step: 450 loss_mean: 2.7207779550552367
step: 500 loss_mean: 2.8072983264923095
step: 550 loss_mean: 2.758529019355774
step: 600 loss_mean: 2.6811965894699097
Epoch: 138 | Run time: 659.0 s | Train loss: 2.67 | Valid loss: 2.77
Saved checkpoint 138 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.3913 s.
run_epoch
step: 50 loss_mean: 2.653946347236633
step: 100 loss_mean: 2.612903161048889
step: 150 loss_mean: 2.6671625185012817
step: 200 loss_mean: 2.6805481052398683
step: 250 loss_mean: 2.6417540240287782
step: 300 loss_mean: 2.68041729927063
step: 350 loss_mean: 2.697459921836853
step: 400 loss_mean: 2.6259729290008544
step: 450 loss_mean: 2.713941798210144
step: 500 loss_mean: 2.625997257232666
step: 550 loss_mean: 2.6241041469573974
step: 600 loss_mean: 2.6552498149871826
step: 650 loss_mean: 2.6687330389022828
step: 700 loss_mean: 2.710295901298523
step: 750 loss_mean: 2.6553528928756713
step: 800 loss_mean: 2.733192582130432
step: 850 loss_mean: 2.7199467515945432
step: 900 loss_mean: 2.6983264207839968
step: 950 loss_mean: 2.6619924974441527
step: 1000 loss_mean: 2.635725903511047
step: 1050 loss_mean: 2.693222794532776
step: 1100 loss_mean: 2.6350295448303225
step: 1150 loss_mean: 2.7499865961074828
step: 1200 loss_mean: 2.7026918935775757
step: 1250 loss_mean: 2.7205706071853637
step: 1300 loss_mean: 2.675445456504822
step: 1350 loss_mean: 2.664089865684509
step: 1400 loss_mean: 2.639000859260559
step: 1450 loss_mean: 2.6471283411979676
step: 1500 loss_mean: 2.6671275758743285
step: 1550 loss_mean: 2.7373787212371825
step: 1600 loss_mean: 2.680017600059509
step: 1650 loss_mean: 2.616188688278198
step: 1700 loss_mean: 2.6859919881820677
step: 1750 loss_mean: 2.6700867938995363
step: 1800 loss_mean: 2.6330180025100707
step: 1850 loss_mean: 2.6935697031021117
step: 1900 loss_mean: 2.672122459411621
step: 1950 loss_mean: 2.701557478904724
step: 2000 loss_mean: 2.610549602508545
step: 2050 loss_mean: 2.663493618965149
step: 2100 loss_mean: 2.5972110748291017
step: 2150 loss_mean: 2.6720577383041384
step: 2200 loss_mean: 2.650158281326294
step: 2250 loss_mean: 2.6730140256881714
step: 2300 loss_mean: 2.6582558584213256
step: 2350 loss_mean: 2.652762560844421
step: 2400 loss_mean: 2.633398480415344
step: 2450 loss_mean: 2.653434057235718
step: 2500 loss_mean: 2.6541013383865355
step: 2550 loss_mean: 2.6862699699401857
step: 2600 loss_mean: 2.701107702255249
step: 2650 loss_mean: 2.7067525959014893
step: 2700 loss_mean: 2.6893192672729493
step: 2750 loss_mean: 2.6635467004776
step: 2800 loss_mean: 2.6542074251174927
step: 2850 loss_mean: 2.646762466430664
step: 2900 loss_mean: 2.6683795499801635
step: 2950 loss_mean: 2.692699146270752
step: 3000 loss_mean: 2.665906252861023
step: 3050 loss_mean: 2.675785083770752
step: 3100 loss_mean: 2.669986915588379
step: 50 loss_mean: 2.7334509181976316
step: 100 loss_mean: 2.789512248039246
step: 150 loss_mean: 2.688492136001587
step: 200 loss_mean: 2.772962055206299
step: 250 loss_mean: 2.7472336435317994
step: 300 loss_mean: 2.8437288093566893
step: 350 loss_mean: 2.699409942626953
step: 400 loss_mean: 2.7199587392807008
step: 450 loss_mean: 2.7078521633148194
step: 500 loss_mean: 2.7669617128372193
step: 550 loss_mean: 2.7133727931976317
step: 600 loss_mean: 2.6636776065826417
Epoch: 139 | Run time: 680.0 s | Train loss: 2.67 | Valid loss: 2.74
Saved checkpoint 139 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.2009 s.
run_epoch
step: 50 loss_mean: 2.6144827795028687
step: 100 loss_mean: 2.625831699371338
step: 150 loss_mean: 2.674008927345276
step: 200 loss_mean: 2.652965297698975
step: 250 loss_mean: 2.6243701219558715
step: 300 loss_mean: 2.657906255722046
step: 350 loss_mean: 2.70343599319458
step: 400 loss_mean: 2.6403175306320192
step: 450 loss_mean: 2.7197448015213013
step: 500 loss_mean: 2.63571542263031
step: 550 loss_mean: 2.6517244243621825
step: 600 loss_mean: 2.668231256008148
step: 650 loss_mean: 2.693844909667969
step: 700 loss_mean: 2.7518351602554323
step: 750 loss_mean: 2.6003732681274414
step: 800 loss_mean: 2.6389208316802977
step: 850 loss_mean: 2.732274923324585
step: 900 loss_mean: 2.6159655952453615
step: 950 loss_mean: 2.651430950164795
step: 1000 loss_mean: 2.622034969329834
step: 1050 loss_mean: 2.7339322900772096
step: 1100 loss_mean: 2.6766541719436647
step: 1150 loss_mean: 2.7001963233947754
step: 1200 loss_mean: 2.6940271282196044
step: 1250 loss_mean: 2.6654760217666627
step: 1300 loss_mean: 2.717653431892395
step: 1350 loss_mean: 2.6776241159439085
step: 1400 loss_mean: 2.660562343597412
step: 1450 loss_mean: 2.691600408554077
step: 1500 loss_mean: 2.666288585662842
step: 1550 loss_mean: 2.7022057366371155
step: 1600 loss_mean: 2.685764503479004
step: 1650 loss_mean: 2.6285049390792845
step: 1700 loss_mean: 2.6937106323242186
step: 1750 loss_mean: 2.6337158203125
step: 1800 loss_mean: 2.6802680397033694
step: 1850 loss_mean: 2.662210965156555
step: 1900 loss_mean: 2.6797729253768923
step: 1950 loss_mean: 2.693343605995178
step: 2000 loss_mean: 2.620068917274475
step: 2050 loss_mean: 2.7058723068237303
step: 2100 loss_mean: 2.6634832429885864
step: 2150 loss_mean: 2.6736851263046266
step: 2200 loss_mean: 2.6472843980789182
step: 2250 loss_mean: 2.613273229598999
step: 2300 loss_mean: 2.718895058631897
step: 2350 loss_mean: 2.6859687089920046
step: 2400 loss_mean: 2.6726025676727296
step: 2450 loss_mean: 2.6474435806274412
step: 2500 loss_mean: 2.626199746131897
step: 2550 loss_mean: 2.6798323726654054
step: 2600 loss_mean: 2.6557356548309325
step: 2650 loss_mean: 2.7136825799942015
step: 2700 loss_mean: 2.6319499397277832
step: 2750 loss_mean: 2.654836668968201
step: 2800 loss_mean: 2.6833752727508546
step: 2850 loss_mean: 2.714313144683838
step: 2900 loss_mean: 2.691694130897522
step: 2950 loss_mean: 2.729864492416382
step: 3000 loss_mean: 2.671929984092712
step: 3050 loss_mean: 2.6398825454711914
step: 3100 loss_mean: 2.634026198387146
step: 50 loss_mean: 2.743197865486145
step: 100 loss_mean: 2.7655949544906617
step: 150 loss_mean: 2.7065310955047606
step: 200 loss_mean: 2.7862242126464842
step: 250 loss_mean: 2.7505381393432615
step: 300 loss_mean: 2.843305063247681
step: 350 loss_mean: 2.7004849910736084
step: 400 loss_mean: 2.7468456125259397
step: 450 loss_mean: 2.6898759460449218
step: 500 loss_mean: 2.766717596054077
step: 550 loss_mean: 2.7250544881820677
step: 600 loss_mean: 2.645898976325989
Epoch: 140 | Run time: 627.0 s | Train loss: 2.67 | Valid loss: 2.74
Saved checkpoint 140 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.2015 s.
run_epoch
step: 50 loss_mean: 2.6253529596328735
step: 100 loss_mean: 2.6968654775619507
step: 150 loss_mean: 2.691490707397461
step: 200 loss_mean: 2.741846704483032
step: 250 loss_mean: 2.7186657667160032
step: 300 loss_mean: 2.6935811185836793
step: 350 loss_mean: 2.6346795463562014
step: 400 loss_mean: 2.6581803941726685
step: 450 loss_mean: 2.6121281385421753
step: 500 loss_mean: 2.6491303300857543
step: 550 loss_mean: 2.6938575077056885
step: 600 loss_mean: 2.6396242141723634
step: 650 loss_mean: 2.736544089317322
step: 700 loss_mean: 2.6724816608428954
step: 750 loss_mean: 2.6452952575683595
step: 800 loss_mean: 2.676970157623291
step: 850 loss_mean: 2.652812042236328
step: 900 loss_mean: 2.6565792417526244
step: 950 loss_mean: 2.6920569372177123
step: 1000 loss_mean: 2.6258987283706663
step: 1050 loss_mean: 2.6385444688796995
step: 1100 loss_mean: 2.6937629079818723
step: 1150 loss_mean: 2.656201415061951
step: 1200 loss_mean: 2.7179856204986574
step: 1250 loss_mean: 2.667897562980652
step: 1300 loss_mean: 2.714536247253418
step: 1350 loss_mean: 2.5913004207611086
step: 1400 loss_mean: 2.644140362739563
step: 1450 loss_mean: 2.610402774810791
step: 1500 loss_mean: 2.747852911949158
step: 1550 loss_mean: 2.6540879583358765
step: 1600 loss_mean: 2.671547465324402
step: 1650 loss_mean: 2.731793694496155
step: 1700 loss_mean: 2.6517954158782957
step: 1750 loss_mean: 2.6858424234390257
step: 1800 loss_mean: 2.5970234441757203
step: 1850 loss_mean: 2.6292504692077636
step: 1900 loss_mean: 2.6023519134521482
step: 1950 loss_mean: 2.6845618677139282
step: 2000 loss_mean: 2.648805093765259
step: 2050 loss_mean: 2.702636008262634
step: 2100 loss_mean: 2.644206418991089
step: 2150 loss_mean: 2.6765888357162475
step: 2200 loss_mean: 2.66534218788147
step: 2250 loss_mean: 2.6983238315582274
step: 2300 loss_mean: 2.634109129905701
step: 2350 loss_mean: 2.6653514909744263
step: 2400 loss_mean: 2.6564320802688597
step: 2450 loss_mean: 2.6793311166763307
step: 2500 loss_mean: 2.6390182828903197
step: 2550 loss_mean: 2.662904767990112
step: 2600 loss_mean: 2.676187262535095
step: 2650 loss_mean: 2.622852573394775
step: 2700 loss_mean: 2.7131364583969115
step: 2750 loss_mean: 2.6583276414871215
step: 2800 loss_mean: 2.7272008800506593
step: 2850 loss_mean: 2.662949314117432
step: 2900 loss_mean: 2.7096391820907595
step: 2950 loss_mean: 2.628300747871399
step: 3000 loss_mean: 2.6810171747207643
step: 3050 loss_mean: 2.6612569665908814
step: 3100 loss_mean: 2.651322293281555
step: 50 loss_mean: 2.719574041366577
step: 100 loss_mean: 2.7647366952896117
step: 150 loss_mean: 2.709642825126648
step: 200 loss_mean: 2.783494167327881
step: 250 loss_mean: 2.7571563386917113
step: 300 loss_mean: 2.8403473472595215
step: 350 loss_mean: 2.696344437599182
step: 400 loss_mean: 2.7352923345565796
step: 450 loss_mean: 2.7088033533096314
step: 500 loss_mean: 2.772140312194824
step: 550 loss_mean: 2.732484016418457
step: 600 loss_mean: 2.6611280632019043
Epoch: 141 | Run time: 626.0 s | Train loss: 2.67 | Valid loss: 2.74
Saved checkpoint 141 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.2022 s.
run_epoch
step: 50 loss_mean: 2.659279475212097
step: 100 loss_mean: 2.64613516330719
step: 150 loss_mean: 2.654614472389221
step: 200 loss_mean: 2.6773684930801394
step: 250 loss_mean: 2.6535676193237303
step: 300 loss_mean: 2.632322964668274
step: 350 loss_mean: 2.708505325317383
step: 400 loss_mean: 2.6395869731903074
step: 450 loss_mean: 2.736517515182495
step: 500 loss_mean: 2.6314215207099916
step: 550 loss_mean: 2.646704339981079
step: 600 loss_mean: 2.6663931798934937
step: 650 loss_mean: 2.6085564422607423
step: 700 loss_mean: 2.621877517700195
step: 750 loss_mean: 2.71769718170166
step: 800 loss_mean: 2.67293007850647
step: 850 loss_mean: 2.6069194221496583
step: 900 loss_mean: 2.6933518505096434
step: 950 loss_mean: 2.6293659687042235
step: 1000 loss_mean: 2.6699390745162965
step: 1050 loss_mean: 2.6532658910751343
step: 1100 loss_mean: 2.7488054895401
step: 1150 loss_mean: 2.704894948005676
step: 1200 loss_mean: 2.612779049873352
step: 1250 loss_mean: 2.7241222715377806
step: 1300 loss_mean: 2.635774030685425
step: 1350 loss_mean: 2.6243039321899415
step: 1400 loss_mean: 2.685674867630005
step: 1450 loss_mean: 2.6619593000411985
step: 1500 loss_mean: 2.6564456748962404
step: 1550 loss_mean: 2.6919962120056153
step: 1600 loss_mean: 2.6610560750961305
step: 1650 loss_mean: 2.6399610900878905
step: 1700 loss_mean: 2.7180268001556396
step: 1750 loss_mean: 2.6362812089920045
step: 1800 loss_mean: 2.672179765701294
step: 1850 loss_mean: 2.661122803688049
step: 1900 loss_mean: 2.7041073131561277
step: 1950 loss_mean: 2.6347635698318483
step: 2000 loss_mean: 2.653152642250061
step: 2050 loss_mean: 2.674738116264343
step: 2100 loss_mean: 2.7202255630493166
step: 2150 loss_mean: 2.6277673697471617
step: 2200 loss_mean: 2.6410944843292237
step: 2250 loss_mean: 2.681547908782959
step: 2300 loss_mean: 2.7049031352996824
step: 2350 loss_mean: 2.691542110443115
step: 2400 loss_mean: 2.677317204475403
step: 2450 loss_mean: 2.6176796770095825
step: 2500 loss_mean: 2.6610391569137573
step: 2550 loss_mean: 2.619230380058289
step: 2600 loss_mean: 2.7002998971939087
step: 2650 loss_mean: 2.6765685939788817
step: 2700 loss_mean: 2.642926435470581
step: 2750 loss_mean: 2.7017485761642455
step: 2800 loss_mean: 2.65928786277771
step: 2850 loss_mean: 2.687366399765015
step: 2900 loss_mean: 2.7112919998168947
step: 2950 loss_mean: 2.6653319597244263
step: 3000 loss_mean: 2.6646086502075197
step: 3050 loss_mean: 2.7001346254348757
step: 3100 loss_mean: 2.6291712427139284
step: 50 loss_mean: 2.729687614440918
step: 100 loss_mean: 2.7823755407333373
step: 150 loss_mean: 2.699605541229248
step: 200 loss_mean: 2.7958376884460447
step: 250 loss_mean: 2.749374089241028
step: 300 loss_mean: 2.8612609577178953
step: 350 loss_mean: 2.6981402683258056
step: 400 loss_mean: 2.7425401401519776
step: 450 loss_mean: 2.7149605083465578
step: 500 loss_mean: 2.8025354242324827
step: 550 loss_mean: 2.729004182815552
step: 600 loss_mean: 2.6711564016342164
Epoch: 142 | Run time: 623.0 s | Train loss: 2.67 | Valid loss: 2.75
Saved checkpoint 142 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.2034 s.
run_epoch
step: 50 loss_mean: 2.650984592437744
step: 100 loss_mean: 2.6639777994155884
step: 150 loss_mean: 2.6555241203308104
step: 200 loss_mean: 2.6313706922531126
step: 250 loss_mean: 2.669036030769348
step: 300 loss_mean: 2.69044846534729
step: 350 loss_mean: 2.651944761276245
step: 400 loss_mean: 2.659631609916687
step: 450 loss_mean: 2.6508829736709596
step: 500 loss_mean: 2.6396156120300294
step: 550 loss_mean: 2.6881785202026367
step: 600 loss_mean: 2.659119577407837
step: 650 loss_mean: 2.6536729383468627
step: 700 loss_mean: 2.662400598526001
step: 750 loss_mean: 2.630465703010559
step: 800 loss_mean: 2.691257290840149
step: 850 loss_mean: 2.671589832305908
step: 900 loss_mean: 2.7111501693725586
step: 950 loss_mean: 2.711703062057495
step: 1000 loss_mean: 2.639632811546326
step: 1050 loss_mean: 2.6851938486099245
step: 1100 loss_mean: 2.7036279535293577
step: 1150 loss_mean: 2.6239001750946045
step: 1200 loss_mean: 2.6619082736968993
step: 1250 loss_mean: 2.6623629093170167
step: 1300 loss_mean: 2.671973481178284
step: 1350 loss_mean: 2.6851780939102174
step: 1400 loss_mean: 2.6523538541793825
step: 1450 loss_mean: 2.6973905467987063
step: 1500 loss_mean: 2.617663640975952
step: 1550 loss_mean: 2.7175422859191896
step: 1600 loss_mean: 2.660871057510376
step: 1650 loss_mean: 2.621725559234619
step: 1700 loss_mean: 2.6936358499526976
step: 1750 loss_mean: 2.6943897914886477
step: 1800 loss_mean: 2.6105239725112916
step: 1850 loss_mean: 2.7371141576766966
step: 1900 loss_mean: 2.6342695808410643
step: 1950 loss_mean: 2.736090426445007
step: 2000 loss_mean: 2.6975907564163206
step: 2050 loss_mean: 2.6402075242996217
step: 2100 loss_mean: 2.6423143100738526
step: 2150 loss_mean: 2.6516914653778074
step: 2200 loss_mean: 2.6614755296707155
step: 2250 loss_mean: 2.703286814689636
step: 2300 loss_mean: 2.643655996322632
step: 2350 loss_mean: 2.597224702835083
step: 2400 loss_mean: 2.6889317178726198
step: 2450 loss_mean: 2.673693928718567
step: 2500 loss_mean: 2.662951922416687
step: 2550 loss_mean: 2.6286235237121582
step: 2600 loss_mean: 2.6367501544952394
step: 2650 loss_mean: 2.6345306158065798
step: 2700 loss_mean: 2.724510040283203
step: 2750 loss_mean: 2.7087034225463866
step: 2800 loss_mean: 2.6798380756378175
step: 2850 loss_mean: 2.6447557067871093
step: 2900 loss_mean: 2.69297248840332
step: 2950 loss_mean: 2.694566774368286
step: 3000 loss_mean: 2.6097614002227782
step: 3050 loss_mean: 2.6576225757598877
step: 3100 loss_mean: 2.677337508201599
step: 50 loss_mean: 2.7584230136871337
step: 100 loss_mean: 2.803320636749268
step: 150 loss_mean: 2.728555455207825
step: 200 loss_mean: 2.8261427211761476
step: 250 loss_mean: 2.78185293674469
step: 300 loss_mean: 2.8791641664505003
step: 350 loss_mean: 2.7169608592987062
step: 400 loss_mean: 2.773717942237854
step: 450 loss_mean: 2.740432047843933
step: 500 loss_mean: 2.789410171508789
step: 550 loss_mean: 2.756448450088501
step: 600 loss_mean: 2.6872981643676757
Epoch: 143 | Run time: 625.0 s | Train loss: 2.67 | Valid loss: 2.77
Saved checkpoint 143 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.2049 s.
run_epoch
step: 50 loss_mean: 2.6159150981903077
step: 100 loss_mean: 2.6215674686431885
step: 150 loss_mean: 2.690342469215393
step: 200 loss_mean: 2.6525424671173097
step: 250 loss_mean: 2.659199957847595
step: 300 loss_mean: 2.6634160566329954
step: 350 loss_mean: 2.6704790687561033
step: 400 loss_mean: 2.670291323661804
step: 450 loss_mean: 2.7085745906829835
step: 500 loss_mean: 2.6446142101287844
step: 550 loss_mean: 2.6795756721496584
step: 600 loss_mean: 2.617157049179077
step: 650 loss_mean: 2.606272826194763
step: 700 loss_mean: 2.6513672494888305
step: 750 loss_mean: 2.6579359912872316
step: 800 loss_mean: 2.65688485622406
step: 850 loss_mean: 2.653077311515808
step: 900 loss_mean: 2.6572073793411253
step: 950 loss_mean: 2.6439599800109863
step: 1000 loss_mean: 2.636915979385376
step: 1050 loss_mean: 2.640186238288879
step: 1100 loss_mean: 2.664547173976898
step: 1150 loss_mean: 2.7155492639541627
step: 1200 loss_mean: 2.721634473800659
step: 1250 loss_mean: 2.6786534357070924
step: 1300 loss_mean: 2.657496099472046
step: 1350 loss_mean: 2.7516299867630005
step: 1400 loss_mean: 2.7538992977142334
step: 1450 loss_mean: 2.6629991149902343
step: 1500 loss_mean: 2.6311105394363405
step: 1550 loss_mean: 2.6627246236801145
step: 1600 loss_mean: 2.6750124883651734
step: 1650 loss_mean: 2.671033811569214
step: 1700 loss_mean: 2.6761222076416016
step: 1750 loss_mean: 2.6492850303649904
step: 1800 loss_mean: 2.623795819282532
step: 1850 loss_mean: 2.6780665206909178
step: 1900 loss_mean: 2.7289490556716918
step: 1950 loss_mean: 2.6542069053649904
step: 2000 loss_mean: 2.6736077213287355
step: 2050 loss_mean: 2.632600631713867
step: 2100 loss_mean: 2.687500853538513
step: 2150 loss_mean: 2.6605477046966555
step: 2200 loss_mean: 2.640830674171448
step: 2250 loss_mean: 2.648397750854492
step: 2300 loss_mean: 2.68543710231781
step: 2350 loss_mean: 2.680904998779297
step: 2400 loss_mean: 2.685135593414307
step: 2450 loss_mean: 2.679539804458618
step: 2500 loss_mean: 2.571387405395508
step: 2550 loss_mean: 2.6636378002166747
step: 2600 loss_mean: 2.6808154344558717
step: 2650 loss_mean: 2.6443008470535276
step: 2700 loss_mean: 2.704874286651611
step: 2750 loss_mean: 2.658667969703674
step: 2800 loss_mean: 2.7052003288269044
step: 2850 loss_mean: 2.639697594642639
step: 2900 loss_mean: 2.71477587223053
step: 2950 loss_mean: 2.6962016868591308
step: 3000 loss_mean: 2.6901642179489134
step: 3050 loss_mean: 2.6835850954055784
step: 3100 loss_mean: 2.6668982887268067
step: 50 loss_mean: 2.7290435886383055
step: 100 loss_mean: 2.803358316421509
step: 150 loss_mean: 2.7081676864624025
step: 200 loss_mean: 2.79018177986145
step: 250 loss_mean: 2.764147925376892
step: 300 loss_mean: 2.856653022766113
step: 350 loss_mean: 2.7211501479148863
step: 400 loss_mean: 2.7679464864730834
step: 450 loss_mean: 2.7057049226760865
step: 500 loss_mean: 2.798089714050293
step: 550 loss_mean: 2.737355728149414
step: 600 loss_mean: 2.67757363319397
Epoch: 144 | Run time: 626.0 s | Train loss: 2.67 | Valid loss: 2.75
Saved checkpoint 144 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.2085 s.
run_epoch
step: 50 loss_mean: 2.6052514266967775
step: 100 loss_mean: 2.644981904029846
step: 150 loss_mean: 2.699981551170349
step: 200 loss_mean: 2.668888502120972
step: 250 loss_mean: 2.669693322181702
step: 300 loss_mean: 2.66996376991272
step: 350 loss_mean: 2.644155511856079
step: 400 loss_mean: 2.6315115213394167
step: 450 loss_mean: 2.6524949502944946
step: 500 loss_mean: 2.6361300802230834
step: 550 loss_mean: 2.6743162536621092
step: 600 loss_mean: 2.6528258991241453
step: 650 loss_mean: 2.664226884841919
step: 700 loss_mean: 2.6975533723831178
step: 750 loss_mean: 2.722225184440613
step: 800 loss_mean: 2.6887601947784425
step: 850 loss_mean: 2.661906108856201
step: 900 loss_mean: 2.7059924030303955
step: 950 loss_mean: 2.677295236587524
step: 1000 loss_mean: 2.704970607757568
step: 1050 loss_mean: 2.669385161399841
step: 1100 loss_mean: 2.6992383718490602
step: 1150 loss_mean: 2.6143556022644043
step: 1200 loss_mean: 2.6618415546417236
step: 1250 loss_mean: 2.6698127603530883
step: 1300 loss_mean: 2.6862940788269043
step: 1350 loss_mean: 2.6403212690353395
step: 1400 loss_mean: 2.61877094745636
step: 1450 loss_mean: 2.6404748010635375
step: 1500 loss_mean: 2.641593828201294
step: 1550 loss_mean: 2.688393921852112
step: 1600 loss_mean: 2.6681039428710935
step: 1650 loss_mean: 2.677226266860962
step: 1700 loss_mean: 2.6061441707611084
step: 1750 loss_mean: 2.6495072650909424
step: 1800 loss_mean: 2.6528660106658934
step: 1850 loss_mean: 2.6837212514877318
step: 1900 loss_mean: 2.629918828010559
step: 1950 loss_mean: 2.6686455821990966
step: 2000 loss_mean: 2.73721097946167
step: 2050 loss_mean: 2.6862744092941284
step: 2100 loss_mean: 2.6944409132003786
step: 2150 loss_mean: 2.649679489135742
step: 2200 loss_mean: 2.6254876041412354
step: 2250 loss_mean: 2.7030431175231935
step: 2300 loss_mean: 2.6686488437652587
step: 2350 loss_mean: 2.6726412105560304
step: 2400 loss_mean: 2.678082947731018
step: 2450 loss_mean: 2.710466012954712
step: 2500 loss_mean: 2.718995280265808
step: 2550 loss_mean: 2.5904844284057615
step: 2600 loss_mean: 2.6146785736083986
step: 2650 loss_mean: 2.6900449991226196
step: 2700 loss_mean: 2.6408726835250853
step: 2750 loss_mean: 2.655353708267212
step: 2800 loss_mean: 2.6969268465042116
step: 2850 loss_mean: 2.6399957132339478
step: 2900 loss_mean: 2.6442467832565306
step: 2950 loss_mean: 2.7117072057724
step: 3000 loss_mean: 2.6453336906433105
step: 3050 loss_mean: 2.6227183294296266
step: 3100 loss_mean: 2.6486966896057127
step: 50 loss_mean: 2.7189112377166746
step: 100 loss_mean: 2.765208697319031
step: 150 loss_mean: 2.708686099052429
step: 200 loss_mean: 2.7843819665908813
step: 250 loss_mean: 2.744108214378357
step: 300 loss_mean: 2.85115074634552
step: 350 loss_mean: 2.71477991104126
step: 400 loss_mean: 2.738320760726929
step: 450 loss_mean: 2.6950824642181397
step: 500 loss_mean: 2.779186935424805
step: 550 loss_mean: 2.7204833793640137
step: 600 loss_mean: 2.6515424489974975
Epoch: 145 | Run time: 620.0 s | Train loss: 2.66 | Valid loss: 2.74
Saved checkpoint 145 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.2082 s.
run_epoch
step: 50 loss_mean: 2.604035792350769
step: 100 loss_mean: 2.6532238578796385
step: 150 loss_mean: 2.631190161705017
step: 200 loss_mean: 2.65554470539093
step: 250 loss_mean: 2.6216002535820007
step: 300 loss_mean: 2.690728521347046
step: 350 loss_mean: 2.660360727310181
step: 400 loss_mean: 2.6444370269775392
step: 450 loss_mean: 2.6305480885505674
step: 500 loss_mean: 2.65040235042572
step: 550 loss_mean: 2.6635772371292115
step: 600 loss_mean: 2.6673608160018922
step: 650 loss_mean: 2.6774577951431273
step: 700 loss_mean: 2.710655755996704
step: 750 loss_mean: 2.666244192123413
step: 800 loss_mean: 2.6141544246673583
step: 850 loss_mean: 2.65839768409729
step: 900 loss_mean: 2.6778356552124025
step: 950 loss_mean: 2.617612180709839
step: 1000 loss_mean: 2.681008024215698
step: 1050 loss_mean: 2.707443075180054
step: 1100 loss_mean: 2.6691281747817994
step: 1150 loss_mean: 2.6484478521347046
step: 1200 loss_mean: 2.6825384998321535
step: 1250 loss_mean: 2.620178847312927
step: 1300 loss_mean: 2.633000888824463
step: 1350 loss_mean: 2.7160026025772095
step: 1400 loss_mean: 2.6963564205169677
step: 1450 loss_mean: 2.6760261631011963
step: 1500 loss_mean: 2.6729575538635255
step: 1550 loss_mean: 2.694787940979004
step: 1600 loss_mean: 2.6564157438278198
step: 1650 loss_mean: 2.710034132003784
step: 1700 loss_mean: 2.6614093923568727
step: 1750 loss_mean: 2.6573734855651856
step: 1800 loss_mean: 2.6658583784103396
step: 1850 loss_mean: 2.6855332803726197
step: 1900 loss_mean: 2.6701060485839845
step: 1950 loss_mean: 2.6809506750106813
step: 2000 loss_mean: 2.6266105461120604
step: 2050 loss_mean: 2.666309494972229
step: 2100 loss_mean: 2.625204396247864
step: 2150 loss_mean: 2.714277229309082
step: 2200 loss_mean: 2.694181008338928
step: 2250 loss_mean: 2.6603781938552857
step: 2300 loss_mean: 2.6823586940765383
step: 2350 loss_mean: 2.6893038940429688
step: 2400 loss_mean: 2.6465385866165163
step: 2450 loss_mean: 2.6213571643829345
step: 2500 loss_mean: 2.6891959428787233
step: 2550 loss_mean: 2.6471605587005613
step: 2600 loss_mean: 2.6725878286361695
step: 2650 loss_mean: 2.705487596988678
step: 2700 loss_mean: 2.6588497734069825
step: 2750 loss_mean: 2.6519761943817137
step: 2800 loss_mean: 2.6588302135467528
step: 2850 loss_mean: 2.69352997303009
step: 2900 loss_mean: 2.7045993328094484
step: 2950 loss_mean: 2.636740221977234
step: 3000 loss_mean: 2.6813758420944214
step: 3050 loss_mean: 2.641143789291382
step: 3100 loss_mean: 2.631569437980652
step: 50 loss_mean: 2.745924310684204
step: 100 loss_mean: 2.7950143957138063
step: 150 loss_mean: 2.7291231870651247
step: 200 loss_mean: 2.7938628339767457
step: 250 loss_mean: 2.758383207321167
step: 300 loss_mean: 2.8595494747161867
step: 350 loss_mean: 2.7422405815124513
step: 400 loss_mean: 2.741667294502258
step: 450 loss_mean: 2.710615816116333
step: 500 loss_mean: 2.82447603225708
step: 550 loss_mean: 2.7402397060394286
step: 600 loss_mean: 2.6811360788345335
Epoch: 146 | Run time: 622.0 s | Train loss: 2.66 | Valid loss: 2.76
Saved checkpoint 146 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.2103 s.
run_epoch
step: 50 loss_mean: 2.628451080322266
step: 100 loss_mean: 2.6120155096054076
step: 150 loss_mean: 2.6885205364227294
step: 200 loss_mean: 2.67293963432312
step: 250 loss_mean: 2.6542600107192995
step: 300 loss_mean: 2.6971775984764097
step: 350 loss_mean: 2.656848678588867
step: 400 loss_mean: 2.6689401340484618
step: 450 loss_mean: 2.7395185613632203
step: 500 loss_mean: 2.676090040206909
step: 550 loss_mean: 2.6885778284072877
step: 600 loss_mean: 2.658949542045593
step: 650 loss_mean: 2.6383322358131407
step: 700 loss_mean: 2.6715352058410646
step: 750 loss_mean: 2.6417523336410524
step: 800 loss_mean: 2.6635815477371216
step: 850 loss_mean: 2.6665042543411257
step: 900 loss_mean: 2.673631772994995
step: 950 loss_mean: 2.639505314826965
step: 1000 loss_mean: 2.6769077444076537
step: 1050 loss_mean: 2.641140112876892
step: 1100 loss_mean: 2.6909731245040893
step: 1150 loss_mean: 2.6397218322753906
step: 1200 loss_mean: 2.6718061542510987
step: 1250 loss_mean: 2.6496340799331666
step: 1300 loss_mean: 2.6368179941177368
step: 1350 loss_mean: 2.675507493019104
step: 1400 loss_mean: 2.616158990859985
step: 1450 loss_mean: 2.6187047147750855
step: 1500 loss_mean: 2.641831407546997
step: 1550 loss_mean: 2.6400548028945923
step: 1600 loss_mean: 2.654478139877319
step: 1650 loss_mean: 2.6718761348724365
step: 1700 loss_mean: 2.6022709035873413
step: 1750 loss_mean: 2.66495174407959
step: 1800 loss_mean: 2.6278672981262208
step: 1850 loss_mean: 2.6644194078445436
step: 1900 loss_mean: 2.638405194282532
step: 1950 loss_mean: 2.763046932220459
step: 2000 loss_mean: 2.694722542762756
step: 2050 loss_mean: 2.703211827278137
step: 2100 loss_mean: 2.6556066608428956
step: 2150 loss_mean: 2.68148579120636
step: 2200 loss_mean: 2.678807578086853
step: 2250 loss_mean: 2.662694897651672
step: 2300 loss_mean: 2.665778889656067
step: 2350 loss_mean: 2.683144335746765
step: 2400 loss_mean: 2.635188980102539
step: 2450 loss_mean: 2.653292908668518
step: 2500 loss_mean: 2.684125018119812
step: 2550 loss_mean: 2.7189428329467775
step: 2600 loss_mean: 2.646165342330933
step: 2650 loss_mean: 2.6679767560958862
step: 2700 loss_mean: 2.669952878952026
step: 2750 loss_mean: 2.6710688161849974
step: 2800 loss_mean: 2.6084292125701904
step: 2850 loss_mean: 2.68896279335022
step: 2900 loss_mean: 2.648189845085144
step: 2950 loss_mean: 2.66600989818573
step: 3000 loss_mean: 2.6644512891769407
step: 3050 loss_mean: 2.6635154724121093
step: 3100 loss_mean: 2.670103659629822
step: 50 loss_mean: 2.7209207344055177
step: 100 loss_mean: 2.7718107175827025
step: 150 loss_mean: 2.70292920589447
step: 200 loss_mean: 2.7881232261657716
step: 250 loss_mean: 2.7653627490997312
step: 300 loss_mean: 2.8555767488479615
step: 350 loss_mean: 2.712029800415039
step: 400 loss_mean: 2.7284939241409303
step: 450 loss_mean: 2.696320858001709
step: 500 loss_mean: 2.804312562942505
step: 550 loss_mean: 2.7317107248306276
step: 600 loss_mean: 2.6607679557800292
Epoch: 147 | Run time: 623.0 s | Train loss: 2.66 | Valid loss: 2.74
Saved checkpoint 147 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.2120 s.
run_epoch
step: 50 loss_mean: 2.644584393501282
step: 100 loss_mean: 2.615060257911682
step: 150 loss_mean: 2.6629961919784546
step: 200 loss_mean: 2.6660517406463624
step: 250 loss_mean: 2.692930598258972
step: 300 loss_mean: 2.6178083419799805
step: 350 loss_mean: 2.679747200012207
step: 400 loss_mean: 2.6403826093673706
step: 450 loss_mean: 2.670138874053955
step: 500 loss_mean: 2.6657547426223753
step: 550 loss_mean: 2.6582324647903444
step: 600 loss_mean: 2.6488850212097166
step: 650 loss_mean: 2.6308838367462157
step: 700 loss_mean: 2.6493197011947633
step: 750 loss_mean: 2.615543768405914
step: 800 loss_mean: 2.626758670806885
step: 850 loss_mean: 2.7063957500457763
step: 900 loss_mean: 2.584872784614563
step: 950 loss_mean: 2.686557059288025
step: 1000 loss_mean: 2.669234552383423
step: 1050 loss_mean: 2.6899587678909302
step: 1100 loss_mean: 2.64245662689209
step: 1150 loss_mean: 2.6922180700302123
step: 1200 loss_mean: 2.6788216972351075
step: 1250 loss_mean: 2.642316989898682
step: 1300 loss_mean: 2.609090976715088
step: 1350 loss_mean: 2.6780568885803224
step: 1400 loss_mean: 2.6623752403259275
step: 1450 loss_mean: 2.6764292097091675
step: 1500 loss_mean: 2.7021415615081787
step: 1550 loss_mean: 2.685746455192566
step: 1600 loss_mean: 2.6888661432266234
step: 1650 loss_mean: 2.6655833244323732
step: 1700 loss_mean: 2.6398309755325315
step: 1750 loss_mean: 2.5981918740272523
step: 1800 loss_mean: 2.715094151496887
step: 1850 loss_mean: 2.6399222946166994
step: 1900 loss_mean: 2.66560378074646
step: 1950 loss_mean: 2.728809185028076
step: 2000 loss_mean: 2.648940935134888
step: 2050 loss_mean: 2.630866594314575
step: 2100 loss_mean: 2.6680941009521484
step: 2150 loss_mean: 2.668098096847534
step: 2200 loss_mean: 2.6489157104492187
step: 2250 loss_mean: 2.720798511505127
step: 2300 loss_mean: 2.7003847932815552
step: 2350 loss_mean: 2.665964322090149
step: 2400 loss_mean: 2.627493085861206
step: 2450 loss_mean: 2.662154598236084
step: 2500 loss_mean: 2.6784340047836306
step: 2550 loss_mean: 2.6656202602386476
step: 2600 loss_mean: 2.629836983680725
step: 2650 loss_mean: 2.6858348512649535
step: 2700 loss_mean: 2.6616497373580934
step: 2750 loss_mean: 2.7098626947402953
step: 2800 loss_mean: 2.668469820022583
step: 2850 loss_mean: 2.675687212944031
step: 2900 loss_mean: 2.6919611263275147
step: 2950 loss_mean: 2.705070686340332
step: 3000 loss_mean: 2.6819273805618287
step: 3050 loss_mean: 2.673153867721558
step: 3100 loss_mean: 2.666423077583313
step: 50 loss_mean: 2.7141144466400147
step: 100 loss_mean: 2.773772134780884
step: 150 loss_mean: 2.69905704498291
step: 200 loss_mean: 2.7908293914794924
step: 250 loss_mean: 2.7604376554489134
step: 300 loss_mean: 2.8554282188415527
step: 350 loss_mean: 2.6925003695487977
step: 400 loss_mean: 2.7286930179595945
step: 450 loss_mean: 2.7045196390151975
step: 500 loss_mean: 2.781371898651123
step: 550 loss_mean: 2.7297416305541993
step: 600 loss_mean: 2.6518787574768066
Epoch: 148 | Run time: 620.0 s | Train loss: 2.66 | Valid loss: 2.74
Saved checkpoint 148 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.2127 s.
run_epoch
step: 50 loss_mean: 2.6303529644012453
step: 100 loss_mean: 2.618936619758606
step: 150 loss_mean: 2.645515651702881
step: 200 loss_mean: 2.6658658027648925
step: 250 loss_mean: 2.696444687843323
step: 300 loss_mean: 2.6775926971435546
step: 350 loss_mean: 2.7065482187271117
step: 400 loss_mean: 2.6878497123718263
step: 450 loss_mean: 2.669484806060791
step: 500 loss_mean: 2.7062935400009156
step: 550 loss_mean: 2.660643582344055
step: 600 loss_mean: 2.6698033142089845
step: 650 loss_mean: 2.634045820236206
step: 700 loss_mean: 2.680802412033081
step: 750 loss_mean: 2.624036917686462
step: 800 loss_mean: 2.612342233657837
step: 850 loss_mean: 2.6220719146728517
step: 900 loss_mean: 2.6794084167480468
step: 950 loss_mean: 2.7130555295944214
step: 1000 loss_mean: 2.671542205810547
step: 1050 loss_mean: 2.612704620361328
step: 1100 loss_mean: 2.69707359790802
step: 1150 loss_mean: 2.697603816986084
step: 1200 loss_mean: 2.6394153785705567
step: 1250 loss_mean: 2.6580958271026613
step: 1300 loss_mean: 2.6815156650543215
step: 1350 loss_mean: 2.6660230922698975
step: 1400 loss_mean: 2.6805613946914675
step: 1450 loss_mean: 2.673402638435364
step: 1500 loss_mean: 2.68136812210083
step: 1550 loss_mean: 2.702267761230469
step: 1600 loss_mean: 2.6564341259002684
step: 1650 loss_mean: 2.627514419555664
step: 1700 loss_mean: 2.6264065074920655
step: 1750 loss_mean: 2.656300563812256
step: 1800 loss_mean: 2.706314401626587
step: 1850 loss_mean: 2.669953603744507
step: 1900 loss_mean: 2.6414622592926027
step: 1950 loss_mean: 2.6612981843948362
step: 2000 loss_mean: 2.6805545234680177
step: 2050 loss_mean: 2.659067678451538
step: 2100 loss_mean: 2.67997447013855
step: 2150 loss_mean: 2.6393400764465333
step: 2200 loss_mean: 2.622974591255188
step: 2250 loss_mean: 2.692116813659668
step: 2300 loss_mean: 2.67796338558197
step: 2350 loss_mean: 2.6242515993118287
step: 2400 loss_mean: 2.6633021020889283
step: 2450 loss_mean: 2.63393844127655
step: 2500 loss_mean: 2.62452401638031
step: 2550 loss_mean: 2.6879376411437987
step: 2600 loss_mean: 2.6418761348724367
step: 2650 loss_mean: 2.6910265493392944
step: 2700 loss_mean: 2.647610831260681
step: 2750 loss_mean: 2.6550670528411864
step: 2800 loss_mean: 2.6473225069046022
step: 2850 loss_mean: 2.7142696952819825
step: 2900 loss_mean: 2.619439287185669
step: 2950 loss_mean: 2.6853530406951904
step: 3000 loss_mean: 2.580150547027588
step: 3050 loss_mean: 2.6836233854293825
step: 3100 loss_mean: 2.6482381200790406
step: 50 loss_mean: 2.737930407524109
step: 100 loss_mean: 2.7956587791442873
step: 150 loss_mean: 2.715262575149536
step: 200 loss_mean: 2.8061052322387696
step: 250 loss_mean: 2.7482901000976563
step: 300 loss_mean: 2.8796362257003785
step: 350 loss_mean: 2.722311010360718
step: 400 loss_mean: 2.7501440763473513
step: 450 loss_mean: 2.7098904514312743
step: 500 loss_mean: 2.8382089567184448
step: 550 loss_mean: 2.757215728759766
step: 600 loss_mean: 2.676511526107788
Epoch: 149 | Run time: 622.0 s | Train loss: 2.66 | Valid loss: 2.76
Saved checkpoint 149 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.2167 s.
run_epoch
step: 50 loss_mean: 2.6957402992248536
step: 100 loss_mean: 2.639764413833618
step: 150 loss_mean: 2.6726752090454102
step: 200 loss_mean: 2.709056739807129
step: 250 loss_mean: 2.627812378406525
step: 300 loss_mean: 2.65210599899292
step: 350 loss_mean: 2.670315022468567
step: 400 loss_mean: 2.647852368354797
step: 450 loss_mean: 2.62562557220459
step: 500 loss_mean: 2.607860598564148
step: 550 loss_mean: 2.602914423942566
step: 600 loss_mean: 2.669565563201904
step: 650 loss_mean: 2.6416953563690186
step: 700 loss_mean: 2.675151619911194
step: 750 loss_mean: 2.667958097457886
step: 800 loss_mean: 2.6417329502105713
step: 850 loss_mean: 2.6352666425704956
step: 900 loss_mean: 2.657197813987732
step: 950 loss_mean: 2.5741479063034056
step: 1000 loss_mean: 2.6288988971710205
step: 1050 loss_mean: 2.6759604024887085
step: 1100 loss_mean: 2.617509937286377
step: 1150 loss_mean: 2.6686426115036013
step: 1200 loss_mean: 2.675630416870117
step: 1250 loss_mean: 2.662778697013855
step: 1300 loss_mean: 2.669876799583435
step: 1350 loss_mean: 2.6737911033630373
step: 1400 loss_mean: 2.6388991117477416
step: 1450 loss_mean: 2.629796557426453
step: 1500 loss_mean: 2.67428120136261
step: 1550 loss_mean: 2.676216416358948
step: 1600 loss_mean: 2.716031622886658
step: 1650 loss_mean: 2.6162963485717774
step: 1700 loss_mean: 2.5854969930648806
step: 1750 loss_mean: 2.625296154022217
step: 1800 loss_mean: 2.64338996887207
step: 1850 loss_mean: 2.7690709972381593
step: 1900 loss_mean: 2.675696692466736
step: 1950 loss_mean: 2.687916107177734
step: 2000 loss_mean: 2.64488073348999
step: 2050 loss_mean: 2.6440923833847045
step: 2100 loss_mean: 2.687687578201294
step: 2150 loss_mean: 2.6304460048675535
step: 2200 loss_mean: 2.6831643915176393
step: 2250 loss_mean: 2.622874631881714
step: 2300 loss_mean: 2.6249609375
step: 2350 loss_mean: 2.6537264013290405
step: 2400 loss_mean: 2.6572037601470946
step: 2450 loss_mean: 2.723801441192627
step: 2500 loss_mean: 2.672205033302307
step: 2550 loss_mean: 2.6861384391784666
step: 2600 loss_mean: 2.6875830936431884
step: 2650 loss_mean: 2.6809184169769287
step: 2700 loss_mean: 2.711395468711853
step: 2750 loss_mean: 2.6996334743499757
step: 2800 loss_mean: 2.69850501537323
step: 2850 loss_mean: 2.655195240974426
step: 2900 loss_mean: 2.627197012901306
step: 2950 loss_mean: 2.697224488258362
step: 3000 loss_mean: 2.6776724004745485
step: 3050 loss_mean: 2.6474823570251464
step: 3100 loss_mean: 2.6917957735061644
step: 50 loss_mean: 2.7470397472381594
step: 100 loss_mean: 2.8068829345703126
step: 150 loss_mean: 2.707953243255615
step: 200 loss_mean: 2.8072243976593017
step: 250 loss_mean: 2.7733665990829466
step: 300 loss_mean: 2.8688667583465577
step: 350 loss_mean: 2.702543511390686
step: 400 loss_mean: 2.747300930023193
step: 450 loss_mean: 2.7106594467163085
step: 500 loss_mean: 2.8028838872909545
step: 550 loss_mean: 2.7291057014465334
step: 600 loss_mean: 2.6860154628753663
Epoch: 150 | Run time: 623.0 s | Train loss: 2.66 | Valid loss: 2.76
Saved checkpoint 150 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.2157 s.
run_epoch
step: 50 loss_mean: 2.617974514961243
step: 100 loss_mean: 2.647531757354736
step: 150 loss_mean: 2.661149697303772
step: 200 loss_mean: 2.6138487434387208
step: 250 loss_mean: 2.631569061279297
step: 300 loss_mean: 2.6582299995422365
step: 350 loss_mean: 2.650164532661438
step: 400 loss_mean: 2.7117331075668334
step: 450 loss_mean: 2.683671245574951
step: 500 loss_mean: 2.6935193967819213
step: 550 loss_mean: 2.661747465133667
step: 600 loss_mean: 2.6322880744934083
step: 650 loss_mean: 2.6880279684066775
step: 700 loss_mean: 2.6792580890655517
step: 750 loss_mean: 2.6420992755889894
step: 800 loss_mean: 2.679621920585632
step: 850 loss_mean: 2.6297722625732423
step: 900 loss_mean: 2.66161069393158
step: 950 loss_mean: 2.68331356048584
step: 1000 loss_mean: 2.6196936655044554
step: 1050 loss_mean: 2.6350398111343383
step: 1100 loss_mean: 2.6479438400268553
step: 1150 loss_mean: 2.618363938331604
step: 1200 loss_mean: 2.663776745796204
step: 1250 loss_mean: 2.647838461399078
step: 1300 loss_mean: 2.6603956699371336
step: 1350 loss_mean: 2.6523824548721313
step: 1400 loss_mean: 2.679584708213806
step: 1450 loss_mean: 2.701190161705017
step: 1500 loss_mean: 2.663530716896057
step: 1550 loss_mean: 2.685121388435364
step: 1600 loss_mean: 2.64234694480896
step: 1650 loss_mean: 2.6694752550125123
step: 1700 loss_mean: 2.6007827281951905
step: 1750 loss_mean: 2.6409962606430053
step: 1800 loss_mean: 2.677536368370056
step: 1850 loss_mean: 2.6571575355529786
step: 1900 loss_mean: 2.6777779150009153
step: 1950 loss_mean: 2.667713513374329
step: 2000 loss_mean: 2.676890482902527
step: 2050 loss_mean: 2.666186227798462
step: 2100 loss_mean: 2.67563853263855
step: 2150 loss_mean: 2.654294376373291
step: 2200 loss_mean: 2.6594008588790894
step: 2250 loss_mean: 2.7228819847106935
step: 2300 loss_mean: 2.680588812828064
step: 2350 loss_mean: 2.646292495727539
step: 2400 loss_mean: 2.7266153144836425
step: 2450 loss_mean: 2.6420767545700072
step: 2500 loss_mean: 2.671208610534668
step: 2550 loss_mean: 2.6203083419799804
step: 2600 loss_mean: 2.6857043170928954
step: 2650 loss_mean: 2.6068790674209597
step: 2700 loss_mean: 2.66454252243042
step: 2750 loss_mean: 2.681380066871643
step: 2800 loss_mean: 2.663525700569153
step: 2850 loss_mean: 2.680158805847168
step: 2900 loss_mean: 2.657822461128235
step: 2950 loss_mean: 2.6409744024276733
step: 3000 loss_mean: 2.668792791366577
step: 3050 loss_mean: 2.6357025861740113
step: 3100 loss_mean: 2.659317102432251
step: 50 loss_mean: 2.750940704345703
step: 100 loss_mean: 2.793922715187073
step: 150 loss_mean: 2.7305441761016844
step: 200 loss_mean: 2.8015746307373046
step: 250 loss_mean: 2.7786119270324705
step: 300 loss_mean: 2.8527322244644164
step: 350 loss_mean: 2.7353622674942017
step: 400 loss_mean: 2.7601355695724488
step: 450 loss_mean: 2.7244853353500367
step: 500 loss_mean: 2.8375779247283934
step: 550 loss_mean: 2.7429603433609007
step: 600 loss_mean: 2.6814341878890993
Epoch: 151 | Run time: 626.0 s | Train loss: 2.66 | Valid loss: 2.77
Saved checkpoint 151 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.2617 s.
run_epoch
step: 50 loss_mean: 2.7297993230819704
step: 100 loss_mean: 2.6281979370117186
step: 150 loss_mean: 2.660653281211853
step: 200 loss_mean: 2.667288274765015
step: 250 loss_mean: 2.6841529321670534
step: 300 loss_mean: 2.6510428762435914
step: 350 loss_mean: 2.6833899068832396
step: 400 loss_mean: 2.622842311859131
step: 450 loss_mean: 2.6452337980270384
step: 500 loss_mean: 2.6585147047042845
step: 550 loss_mean: 2.635503339767456
step: 600 loss_mean: 2.688530488014221
step: 650 loss_mean: 2.69450355052948
step: 700 loss_mean: 2.6454993724822997
step: 750 loss_mean: 2.692316966056824
step: 800 loss_mean: 2.6554573249816893
step: 850 loss_mean: 2.626274881362915
step: 900 loss_mean: 2.6630392360687254
step: 950 loss_mean: 2.6441853523254393
step: 1000 loss_mean: 2.6931106090545653
step: 1050 loss_mean: 2.673453688621521
step: 1100 loss_mean: 2.6313021326065065
step: 1150 loss_mean: 2.6347539806365967
step: 1200 loss_mean: 2.6804661703109742
step: 1250 loss_mean: 2.6495741271972655
step: 1300 loss_mean: 2.694543995857239
step: 1350 loss_mean: 2.6575043201446533
step: 1400 loss_mean: 2.714189820289612
step: 1450 loss_mean: 2.645004539489746
step: 1500 loss_mean: 2.6364947891235353
step: 1550 loss_mean: 2.6534960985183718
step: 1600 loss_mean: 2.655913133621216
step: 1650 loss_mean: 2.6926391029357912
step: 1700 loss_mean: 2.612702751159668
step: 1750 loss_mean: 2.6388579654693602
step: 1800 loss_mean: 2.697877130508423
step: 1850 loss_mean: 2.5901427841186524
step: 1900 loss_mean: 2.6663841676712035
step: 1950 loss_mean: 2.655943446159363
step: 2000 loss_mean: 2.7029767751693727
step: 2050 loss_mean: 2.6891407489776613
step: 2100 loss_mean: 2.684003248214722
step: 2150 loss_mean: 2.690832734107971
step: 2200 loss_mean: 2.666545100212097
step: 2250 loss_mean: 2.6409958600997925
step: 2300 loss_mean: 2.619524917602539
step: 2350 loss_mean: 2.660635552406311
step: 2400 loss_mean: 2.690264678001404
step: 2450 loss_mean: 2.6712911319732666
step: 2500 loss_mean: 2.644693145751953
step: 2550 loss_mean: 2.654882297515869
step: 2600 loss_mean: 2.632709264755249
step: 2650 loss_mean: 2.682402391433716
step: 2700 loss_mean: 2.6240890216827393
step: 2750 loss_mean: 2.6512457180023192
step: 2800 loss_mean: 2.6953434324264527
step: 2850 loss_mean: 2.641544737815857
step: 2900 loss_mean: 2.67970091342926
step: 2950 loss_mean: 2.680426249504089
step: 3000 loss_mean: 2.621055064201355
step: 3050 loss_mean: 2.6743972396850584
step: 3100 loss_mean: 2.6787151098251343
step: 50 loss_mean: 2.7340825748443605
step: 100 loss_mean: 2.7701644611358645
step: 150 loss_mean: 2.7042627239227297
step: 200 loss_mean: 2.79347354888916
step: 250 loss_mean: 2.761027984619141
step: 300 loss_mean: 2.8526315879821778
step: 350 loss_mean: 2.708134732246399
step: 400 loss_mean: 2.7515281724929808
step: 450 loss_mean: 2.7065186214447023
step: 500 loss_mean: 2.7962773752212526
step: 550 loss_mean: 2.731501369476318
step: 600 loss_mean: 2.6818381214141844
Epoch: 152 | Run time: 627.0 s | Train loss: 2.66 | Valid loss: 2.75
Saved checkpoint 152 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.2183 s.
run_epoch
step: 50 loss_mean: 2.674752173423767
step: 100 loss_mean: 2.678032546043396
step: 150 loss_mean: 2.649747929573059
step: 200 loss_mean: 2.6952870893478393
step: 250 loss_mean: 2.6171264123916624
step: 300 loss_mean: 2.6370761346817018
step: 350 loss_mean: 2.580502405166626
step: 400 loss_mean: 2.642735595703125
step: 450 loss_mean: 2.602607922554016
step: 500 loss_mean: 2.6557164573669434
step: 550 loss_mean: 2.6677632093429566
step: 600 loss_mean: 2.676102571487427
step: 650 loss_mean: 2.6296999216079713
step: 700 loss_mean: 2.6310964703559874
step: 750 loss_mean: 2.647041711807251
step: 800 loss_mean: 2.6469562458992004
step: 850 loss_mean: 2.645543360710144
step: 900 loss_mean: 2.644701781272888
step: 950 loss_mean: 2.678555006980896
step: 1000 loss_mean: 2.6635088181495665
step: 1050 loss_mean: 2.6954143142700193
step: 1100 loss_mean: 2.6430918312072755
step: 1150 loss_mean: 2.667894415855408
step: 1200 loss_mean: 2.686575560569763
step: 1250 loss_mean: 2.6339866876602174
step: 1300 loss_mean: 2.7126824426651
step: 1350 loss_mean: 2.659059553146362
step: 1400 loss_mean: 2.7169339752197263
step: 1450 loss_mean: 2.630210666656494
step: 1500 loss_mean: 2.619131479263306
step: 1550 loss_mean: 2.657716784477234
step: 1600 loss_mean: 2.696868438720703
step: 1650 loss_mean: 2.6912031888961794
step: 1700 loss_mean: 2.6395963621139527
step: 1750 loss_mean: 2.6815573024749755
step: 1800 loss_mean: 2.7059255504608153
step: 1850 loss_mean: 2.631026129722595
step: 1900 loss_mean: 2.6960934686660765
step: 1950 loss_mean: 2.7037593936920166
step: 2000 loss_mean: 2.6792828702926634
step: 2050 loss_mean: 2.669733419418335
step: 2100 loss_mean: 2.6826403379440307
step: 2150 loss_mean: 2.6835587453842162
step: 2200 loss_mean: 2.664870066642761
step: 2250 loss_mean: 2.656887094974518
step: 2300 loss_mean: 2.648571105003357
step: 2350 loss_mean: 2.7320819234848024
step: 2400 loss_mean: 2.6994560050964354
step: 2450 loss_mean: 2.6685554456710814
step: 2500 loss_mean: 2.639150371551514
step: 2550 loss_mean: 2.5979436588287355
step: 2600 loss_mean: 2.6724715709686278
step: 2650 loss_mean: 2.659854669570923
step: 2700 loss_mean: 2.7072518920898436
step: 2750 loss_mean: 2.631826934814453
step: 2800 loss_mean: 2.623835115432739
step: 2850 loss_mean: 2.6315983963012695
step: 2900 loss_mean: 2.618135588169098
step: 2950 loss_mean: 2.6773180866241457
step: 3000 loss_mean: 2.62423056602478
step: 3050 loss_mean: 2.6558827495574953
step: 3100 loss_mean: 2.6680390357971193
step: 50 loss_mean: 2.718589425086975
step: 100 loss_mean: 2.767581706047058
step: 150 loss_mean: 2.7181405639648437
step: 200 loss_mean: 2.799432120323181
step: 250 loss_mean: 2.7646013879776
step: 300 loss_mean: 2.86188289642334
step: 350 loss_mean: 2.7289252996444704
step: 400 loss_mean: 2.7702651596069336
step: 450 loss_mean: 2.691868906021118
step: 500 loss_mean: 2.8085896015167235
step: 550 loss_mean: 2.7473992109298706
step: 600 loss_mean: 2.659596266746521
Epoch: 153 | Run time: 625.0 s | Train loss: 2.66 | Valid loss: 2.75
Saved checkpoint 153 to ../scratch/datasets/retro_branching/supervised_learner/gnn/gnn_0/ in 0.2203 s.
run_epoch
step: 50 loss_mean: 2.658902153968811
step: 100 loss_mean: 2.6055054020881654
step: 150 loss_mean: 2.6383475923538207
step: 200 loss_mean: 2.68868926525116
step: 250 loss_mean: 2.671170711517334
step: 300 loss_mean: 2.6802926301956176
step: 350 loss_mean: 2.6590300226211547
step: 400 loss_mean: 2.6444026947021486
step: 450 loss_mean: 2.7144680881500243
step: 500 loss_mean: 2.6810775423049926
step: 550 loss_mean: 2.6511393785476685
step: 600 loss_mean: 2.630663957595825
step: 650 loss_mean: 2.664307689666748
step: 700 loss_mean: 2.655904688835144
step: 750 loss_mean: 2.6941263341903685
step: 800 loss_mean: 2.6879440546035767
step: 850 loss_mean: 2.693130068778992
step: 900 loss_mean: 2.657150330543518
step: 950 loss_mean: 2.6249206471443176
step: 1000 loss_mean: 2.6366717863082885
step: 1050 loss_mean: 2.661822862625122
step: 1100 loss_mean: 2.6817649269104002
step: 1150 loss_mean: 2.7095957946777345
step: 1200 loss_mean: 2.6776408100128175
step: 1250 loss_mean: 2.6724468231201173
step: 1300 loss_mean: 2.6546591567993163
step: 1350 loss_mean: 2.632524094581604
step: 1400 loss_mean: 2.6774438285827635
step: 1450 loss_mean: 2.6238333892822268
step: 1500 loss_mean: 2.6328555297851564
step: 1550 loss_mean: 2.616025061607361
step: 1600 loss_mean: 2.63919771194458
step: 1650 loss_mean: 2.665501184463501
step: 1700 loss_mean: 2.6064048528671266
step: 1750 loss_mean: 2.654403443336487
step: 1800 loss_mean: 2.608725872039795
step: 1850 loss_mean: 2.6569084882736207
step: 1900 loss_mean: 2.682434206008911
step: 1950 loss_mean: 2.705499892234802
step: 2000 loss_mean: 2.7063803005218507
step: 2050 loss_mean: 2.6708205771446227
step: 2100 loss_mean: 2.662851138114929
step: 2150 loss_mean: 2.674765944480896
step: 2200 loss_mean: 2.639526753425598
step: 2250 loss_mean: 2.6800606632232666
step: 2300 loss_mean: 2.6102415132522583
step: 2350 loss_mean: 2.6465996503829956
step: 2400 loss_mean: 2.633732628822327
step: 2450 loss_mean: 2.652107093334198
step: 2500 loss_mean: 2.713592414855957